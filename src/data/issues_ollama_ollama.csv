url,repository_url,labels_url,comments_url,events_url,html_url,id,node_id,number,title,user,labels,state,locked,assignee,assignees,milestone,comments,created_at,updated_at,closed_at,author_association,type,active_lock_reason,sub_issues_summary,issue_dependencies_summary,body,closed_by,reactions,timeline_url,performed_via_github_app,state_reason,draft,pull_request
https://api.github.com/repos/ollama/ollama/issues/13230,https://api.github.com/repos/ollama/ollama,https://api.github.com/repos/ollama/ollama/issues/13230/labels{/name},https://api.github.com/repos/ollama/ollama/issues/13230/comments,https://api.github.com/repos/ollama/ollama/issues/13230/events,https://github.com/ollama/ollama/issues/13230,3659723177,I_kwDOJ0Z1Ps7aIvGp,13230,`ollama serve` causes `ollama ps` to always return empty results,"{'login': 'anderscui', 'id': 3828167, 'node_id': 'MDQ6VXNlcjM4MjgxNjc=', 'avatar_url': 'https://avatars.githubusercontent.com/u/3828167?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/anderscui', 'html_url': 'https://github.com/anderscui', 'followers_url': 'https://api.github.com/users/anderscui/followers', 'following_url': 'https://api.github.com/users/anderscui/following{/other_user}', 'gists_url': 'https://api.github.com/users/anderscui/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/anderscui/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/anderscui/subscriptions', 'organizations_url': 'https://api.github.com/users/anderscui/orgs', 'repos_url': 'https://api.github.com/users/anderscui/repos', 'events_url': 'https://api.github.com/users/anderscui/events{/privacy}', 'received_events_url': 'https://api.github.com/users/anderscui/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}","[{'id': 5667396184, 'node_id': 'LA_kwDOJ0Z1Ps8AAAABUc2aWA', 'url': 'https://api.github.com/repos/ollama/ollama/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': ""Something isn't working""}]",open,False,,[],,2,2025-11-24T16:53:52Z,2025-11-24T17:44:21Z,,NONE,,,"{'total': 0, 'completed': 0, 'percent_completed': 0}","{'blocked_by': 0, 'total_blocked_by': 0, 'blocking': 0, 'total_blocking': 0}","### What is the issue?

When `ollama serve` is running, the command `ollama ps` always returns an empty list, regardless of whether `OLLAMA_KEEP_ALIVE` is set. After stopping `ollama serve`, `ollama ps` immediately shows the expected running model instances again.

### Issue 1: Without serve → model appears; With serve → model disappears

**Steps to reproduce**

1. Do NOT run `ollama serve`.

2. Run a generation request:

```bash
curl http://localhost:11434/api/generate -d '{
  ""model"": ""deepseek-r1:1.5b"",
  ""prompt"": ""hello world""
}'
```

3. `ollama ps` shows:

```bash
NAME                ID              SIZE      PROCESSOR    CONTEXT    UNTIL
deepseek-r1:1.5b    e0979632db5a    2.8 GB    100% GPU     32768      4 minutes from now
```

4. Then run:

```bash
export OLLAMA_KEEP_ALIVE=30m
ollama serve
```

5. Run ollama ps again → returns empty result

6. Stop ollama serve → run ollama ps → the record reappears

### Issue 2: With serve running, requests produce no visible instances.

**Steps to reproduce**

Similar to Issue 1: first run `ollama serve`, then run the above curl command, `ollama ps` shows empty result. Stop ollama serve → run ollama ps → the record appears.

### Relevant log output

```shell

```

### OS

macOS

### GPU

Intel

### CPU

Apple

### Ollama version

0.13.0",,"{'url': 'https://api.github.com/repos/ollama/ollama/issues/13230/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}",https://api.github.com/repos/ollama/ollama/issues/13230/timeline,,,,
https://api.github.com/repos/ollama/ollama/issues/13229,https://api.github.com/repos/ollama/ollama,https://api.github.com/repos/ollama/ollama/issues/13229/labels{/name},https://api.github.com/repos/ollama/ollama/issues/13229/comments,https://api.github.com/repos/ollama/ollama/issues/13229/events,https://github.com/ollama/ollama/issues/13229,3659595447,I_kwDOJ0Z1Ps7aIP63,13229,Problem: Inconsistent Responses and `done:false` with mistral-small3.1:24b,"{'login': 'ZeyBal', 'id': 187287471, 'node_id': 'U_kgDOCynHrw', 'avatar_url': 'https://avatars.githubusercontent.com/u/187287471?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/ZeyBal', 'html_url': 'https://github.com/ZeyBal', 'followers_url': 'https://api.github.com/users/ZeyBal/followers', 'following_url': 'https://api.github.com/users/ZeyBal/following{/other_user}', 'gists_url': 'https://api.github.com/users/ZeyBal/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/ZeyBal/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/ZeyBal/subscriptions', 'organizations_url': 'https://api.github.com/users/ZeyBal/orgs', 'repos_url': 'https://api.github.com/users/ZeyBal/repos', 'events_url': 'https://api.github.com/users/ZeyBal/events{/privacy}', 'received_events_url': 'https://api.github.com/users/ZeyBal/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}","[{'id': 5667396184, 'node_id': 'LA_kwDOJ0Z1Ps8AAAABUc2aWA', 'url': 'https://api.github.com/repos/ollama/ollama/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': ""Something isn't working""}]",open,False,,[],,2,2025-11-24T16:17:55Z,2025-11-24T16:57:17Z,,NONE,,,"{'total': 0, 'completed': 0, 'percent_completed': 0}","{'blocked_by': 0, 'total_blocked_by': 0, 'blocking': 0, 'total_blocking': 0}","### What is the issue?

## Problem: Inconsistent Responses and `done:false` When Analyzing Multiple Files

I want to analyze a list of files using a specific prompt, where I ask the model to return a table with the following columns:

* The first column corresponds to the criterion.
* The second column contains the model’s analysis based on the documents. If no information is available, it should return `-`.
* The third column contains the associated quotation extracted from the document.

After extracting the text from all files, I send the files' content (in Markdown) along with my prompt to the Ollama endpoint `/api/chat`.

However, I encounter inconsistent behavior, sometimes I get the full table, but sometimes I don’t.
Sometimes the response contains `""done"": false`, and the model output always stops at the **Analysis** column.
In the Docker logs of my Ollama container, the request still returns HTTP 200.

### What I tested

I tested using a Python script and also with Postman.
I tested both endpoints: `/api/chat` and `/api/generate`.

Example request:

```json
{
  ""model"": ""mistral-small3.1:24b"",
  ""messages"": [
    { ""role"": ""user"", ""content"": ""File names and file contents with the prompt "" }
  ],
  ""stream"": false,
  ""options"": {
    ""num_ctx"": 128000,
    ""temperature"": 0.2
  }
}
```

Example incomplete response:

```json
{
    ""model"": ""mistral-small3.1:24b"",
    ""created_at"": ""2025-11-24T15:43:38.33241347Z"",
    ""message"": {
        ""role"": ""assistant"",
        ""content"": ""Here is the analysis of the provided documents:\n\n| Criterion                                                                 | Analysis                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                ""
    },
    ""done"": false
}
```
I also tested multiple `temperature` values: `0.1`, `0.2`, `0.3`, `0.5`, `0.8`.

When using `""stream"": true`, I waited more than 30 minutes but I did not receive a complete response. 

### Issue

I don’t understand why the model gets stuck for this specific prompt, or why it sometimes stops generating output partway through the table. Any insight into why this happens or how to fix it would be appreciated.


### Relevant log output

```shell

```

### OS

Linux, Docker

### GPU

Nvidia

### CPU

_No response_

### Ollama version

0.9.0",,"{'url': 'https://api.github.com/repos/ollama/ollama/issues/13229/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}",https://api.github.com/repos/ollama/ollama/issues/13229/timeline,,,,
https://api.github.com/repos/ollama/ollama/issues/13228,https://api.github.com/repos/ollama/ollama,https://api.github.com/repos/ollama/ollama/issues/13228/labels{/name},https://api.github.com/repos/ollama/ollama/issues/13228/comments,https://api.github.com/repos/ollama/ollama/issues/13228/events,https://github.com/ollama/ollama/pull/13228,3659369415,PR_kwDOJ0Z1Ps61OWQu,13228,docs: add AGENTS.md for AI coding assistants,"{'login': 'cd-Ishita', 'id': 61752983, 'node_id': 'MDQ6VXNlcjYxNzUyOTgz', 'avatar_url': 'https://avatars.githubusercontent.com/u/61752983?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/cd-Ishita', 'html_url': 'https://github.com/cd-Ishita', 'followers_url': 'https://api.github.com/users/cd-Ishita/followers', 'following_url': 'https://api.github.com/users/cd-Ishita/following{/other_user}', 'gists_url': 'https://api.github.com/users/cd-Ishita/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/cd-Ishita/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/cd-Ishita/subscriptions', 'organizations_url': 'https://api.github.com/users/cd-Ishita/orgs', 'repos_url': 'https://api.github.com/users/cd-Ishita/repos', 'events_url': 'https://api.github.com/users/cd-Ishita/events{/privacy}', 'received_events_url': 'https://api.github.com/users/cd-Ishita/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}",[],open,False,,[],,0,2025-11-24T15:19:55Z,2025-11-24T15:19:55Z,,NONE,,,,,"## Description

This PR adds an `AGENTS.md` file to help AI coding assistants (like Claude, GitHub Copilot, Cursor) better understand the project's structure and conventions.

## What's Included

- ✅ Project structure and tech stack
- ✅ Build and test commands
- ✅ Coding standards with examples
- ✅ Git workflow and commit conventions
- ✅ AI-specific code generation guidelines

## Benefits

- Better AI-generated code following project conventions
- Faster contributor onboarding
- Codified best practices

## Testing

All commands tested and verified to work correctly.

Happy to make adjustments based on feedback!",,"{'url': 'https://api.github.com/repos/ollama/ollama/issues/13228/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}",https://api.github.com/repos/ollama/ollama/issues/13228/timeline,,,False,"{'url': 'https://api.github.com/repos/ollama/ollama/pulls/13228', 'html_url': 'https://github.com/ollama/ollama/pull/13228', 'diff_url': 'https://github.com/ollama/ollama/pull/13228.diff', 'patch_url': 'https://github.com/ollama/ollama/pull/13228.patch', 'merged_at': None}"
https://api.github.com/repos/ollama/ollama/issues/13227,https://api.github.com/repos/ollama/ollama,https://api.github.com/repos/ollama/ollama/issues/13227/labels{/name},https://api.github.com/repos/ollama/ollama/issues/13227/comments,https://api.github.com/repos/ollama/ollama/issues/13227/events,https://github.com/ollama/ollama/issues/13227,3659132005,I_kwDOJ0Z1Ps7aGexl,13227,"Ollama evicts previously loaded model, although system memory is suficient","{'login': 'ioannis-soukas', 'id': 57636451, 'node_id': 'MDQ6VXNlcjU3NjM2NDUx', 'avatar_url': 'https://avatars.githubusercontent.com/u/57636451?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/ioannis-soukas', 'html_url': 'https://github.com/ioannis-soukas', 'followers_url': 'https://api.github.com/users/ioannis-soukas/followers', 'following_url': 'https://api.github.com/users/ioannis-soukas/following{/other_user}', 'gists_url': 'https://api.github.com/users/ioannis-soukas/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/ioannis-soukas/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/ioannis-soukas/subscriptions', 'organizations_url': 'https://api.github.com/users/ioannis-soukas/orgs', 'repos_url': 'https://api.github.com/users/ioannis-soukas/repos', 'events_url': 'https://api.github.com/users/ioannis-soukas/events{/privacy}', 'received_events_url': 'https://api.github.com/users/ioannis-soukas/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}","[{'id': 5667396184, 'node_id': 'LA_kwDOJ0Z1Ps8AAAABUc2aWA', 'url': 'https://api.github.com/repos/ollama/ollama/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': ""Something isn't working""}]",open,False,,[],,4,2025-11-24T14:20:41Z,2025-11-24T15:14:20Z,,NONE,,,"{'total': 0, 'completed': 0, 'percent_completed': 0}","{'blocked_by': 0, 'total_blocked_by': 0, 'blocking': 0, 'total_blocking': 0}","### What is the issue?

Hello,

I am running Ollama v0.13.0 CPU only, on a dedicated ubuntu server with 512 GB RAM. 

Although in previous versions I could keep multiple models loaded in memory (Environment=""OLLAMA_KEEP_ALIVE=-1""), after the last update it only keeps one model in memory, even when using only small models. Model eviction happens as soon as I load another model.

For example, it can not have gemma3:1b & gemma3:27b loaded at the same time, although the total ram they consume is small, compared to my free RAM. The server is only used for ollama & open-webui, there is no other software running or consuming RAM. 

Thank you!

### Relevant log output

```shell
Nov 24 13:55:06 llm ollama[7189]: time=2025-11-24T13:55:06.999Z level=INFO source=sched.go:443 msg=""system memory"" total=""503.6 GiB"" free=""498.1 GiB"" free_swap=""8.0 GiB""
Nov 24 13:55:06 llm ollama[7189]: time=2025-11-24T13:55:06.999Z level=INFO source=server.go:702 msg=""loading model"" ""model layers""=63 requested=-1
Nov 24 13:55:06 llm ollama[7189]: time=2025-11-24T13:55:06.999Z level=INFO source=server.go:974 msg=""model requires more memory than is currently available, evicting a model to make space"" ""loaded layers""=0
Nov 24 13:55:07 llm ollama[7189]: time=2025-11-24T13:55:07.022Z level=INFO source=runner.go:1398 msg=""starting ollama engine""
Nov 24 13:55:07 llm ollama[7189]: time=2025-11-24T13:55:07.022Z level=INFO source=runner.go:1433 msg=""Server listening on 127.0.0.1:41401""
Nov 24 13:55:07 llm ollama[7189]: time=2025-11-24T13:55:07.352Z level=INFO source=sched.go:443 msg=""system memory"" total=""503.6 GiB"" free=""499.3 GiB"" free_swap=""8.0 GiB""
Nov 24 13:55:07 llm ollama[7189]: time=2025-11-24T13:55:07.352Z level=INFO source=server.go:702 msg=""loading model"" ""model layers""=63 requested=-1
Nov 24 13:55:07 llm ollama[7189]: time=2025-11-24T13:55:07.353Z level=INFO source=runner.go:1271 msg=load request=""{Operation:fit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:true KvSize:4096 KvCacheType: NumThreads:28 GPULayers:[] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}""
Nov 24 13:55:07 llm ollama[7189]: time=2025-11-24T13:55:07.454Z level=INFO source=ggml.go:136 msg="""" architecture=gemma3 file_type=Q4_0 name="""" description="""" num_tensors=1247 num_key_values=40
```

### OS

Ubuntu 24.04.3 LTS
### GPU

None
### CPU

2xIntel E5-2680v4

### Ollama version

0.13.0",,"{'url': 'https://api.github.com/repos/ollama/ollama/issues/13227/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}",https://api.github.com/repos/ollama/ollama/issues/13227/timeline,,,,
https://api.github.com/repos/ollama/ollama/issues/13226,https://api.github.com/repos/ollama/ollama,https://api.github.com/repos/ollama/ollama/issues/13226/labels{/name},https://api.github.com/repos/ollama/ollama/issues/13226/comments,https://api.github.com/repos/ollama/ollama/issues/13226/events,https://github.com/ollama/ollama/issues/13226,3658968405,I_kwDOJ0Z1Ps7aF21V,13226,Cannot change model location,"{'login': 'Playitcooool', 'id': 129354153, 'node_id': 'U_kgDOB7XJqQ', 'avatar_url': 'https://avatars.githubusercontent.com/u/129354153?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/Playitcooool', 'html_url': 'https://github.com/Playitcooool', 'followers_url': 'https://api.github.com/users/Playitcooool/followers', 'following_url': 'https://api.github.com/users/Playitcooool/following{/other_user}', 'gists_url': 'https://api.github.com/users/Playitcooool/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/Playitcooool/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/Playitcooool/subscriptions', 'organizations_url': 'https://api.github.com/users/Playitcooool/orgs', 'repos_url': 'https://api.github.com/users/Playitcooool/repos', 'events_url': 'https://api.github.com/users/Playitcooool/events{/privacy}', 'received_events_url': 'https://api.github.com/users/Playitcooool/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}","[{'id': 5667396184, 'node_id': 'LA_kwDOJ0Z1Ps8AAAABUc2aWA', 'url': 'https://api.github.com/repos/ollama/ollama/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': ""Something isn't working""}]",open,False,,[],,1,2025-11-24T13:45:10Z,2025-11-24T14:05:23Z,,NONE,,,"{'total': 0, 'completed': 0, 'percent_completed': 0}","{'blocked_by': 0, 'total_blocked_by': 0, 'blocking': 0, 'total_blocking': 0}","### What is the issue?

Cannot change model location by anyway suddenly.It remains default path after browsing

<img width=""701"" height=""121"" alt=""Image"" src=""https://github.com/user-attachments/assets/c43b0fc4-8975-4d21-b5e0-d6b72ef62dfe"" />

### Relevant log output

```shell

```

### OS

macOS

### GPU

_No response_

### CPU

_No response_

### Ollama version

_No response_",,"{'url': 'https://api.github.com/repos/ollama/ollama/issues/13226/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}",https://api.github.com/repos/ollama/ollama/issues/13226/timeline,,,,
https://api.github.com/repos/ollama/ollama/issues/13225,https://api.github.com/repos/ollama/ollama,https://api.github.com/repos/ollama/ollama/issues/13225/labels{/name},https://api.github.com/repos/ollama/ollama/issues/13225/comments,https://api.github.com/repos/ollama/ollama/issues/13225/events,https://github.com/ollama/ollama/issues/13225,3658060553,I_kwDOJ0Z1Ps7aCZMJ,13225,GPU Offline Issues During Extended Operation,"{'login': 'pureGavin', 'id': 45478229, 'node_id': 'MDQ6VXNlcjQ1NDc4MjI5', 'avatar_url': 'https://avatars.githubusercontent.com/u/45478229?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/pureGavin', 'html_url': 'https://github.com/pureGavin', 'followers_url': 'https://api.github.com/users/pureGavin/followers', 'following_url': 'https://api.github.com/users/pureGavin/following{/other_user}', 'gists_url': 'https://api.github.com/users/pureGavin/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/pureGavin/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/pureGavin/subscriptions', 'organizations_url': 'https://api.github.com/users/pureGavin/orgs', 'repos_url': 'https://api.github.com/users/pureGavin/repos', 'events_url': 'https://api.github.com/users/pureGavin/events{/privacy}', 'received_events_url': 'https://api.github.com/users/pureGavin/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}","[{'id': 5667396200, 'node_id': 'LA_kwDOJ0Z1Ps8AAAABUc2aaA', 'url': 'https://api.github.com/repos/ollama/ollama/labels/feature%20request', 'name': 'feature request', 'color': 'a2eeef', 'default': False, 'description': 'New feature or request'}]",open,False,,[],,1,2025-11-24T09:58:37Z,2025-11-24T14:16:03Z,,NONE,,,"{'total': 0, 'completed': 0, 'percent_completed': 0}","{'blocked_by': 0, 'total_blocked_by': 0, 'blocking': 0, 'total_blocking': 0}","I encountered the issue described in this link: https://github.com/ollama/ollama/issues/10257. I also have this problem. I saw the official documentation mention a fix involving adding configuration to the daemon.json file: https://docs.ollama.com/troubleshooting#linux-docker. However, I'm using Ubuntu 24.04, which uses the latest cgroup V2. This means the official fix provided in the documentation does not work on my system. Does Ollama have any plans to address this issue?",,"{'url': 'https://api.github.com/repos/ollama/ollama/issues/13225/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}",https://api.github.com/repos/ollama/ollama/issues/13225/timeline,,,,
https://api.github.com/repos/ollama/ollama/issues/13224,https://api.github.com/repos/ollama/ollama,https://api.github.com/repos/ollama/ollama/issues/13224/labels{/name},https://api.github.com/repos/ollama/ollama/issues/13224/comments,https://api.github.com/repos/ollama/ollama/issues/13224/events,https://github.com/ollama/ollama/issues/13224,3657628930,I_kwDOJ0Z1Ps7aAv0C,13224,Error loading mistral-small3.2:24b on AMD GPU,"{'login': 'johanatandromeda', 'id': 110618581, 'node_id': 'U_kgDOBpfn1Q', 'avatar_url': 'https://avatars.githubusercontent.com/u/110618581?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/johanatandromeda', 'html_url': 'https://github.com/johanatandromeda', 'followers_url': 'https://api.github.com/users/johanatandromeda/followers', 'following_url': 'https://api.github.com/users/johanatandromeda/following{/other_user}', 'gists_url': 'https://api.github.com/users/johanatandromeda/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/johanatandromeda/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/johanatandromeda/subscriptions', 'organizations_url': 'https://api.github.com/users/johanatandromeda/orgs', 'repos_url': 'https://api.github.com/users/johanatandromeda/repos', 'events_url': 'https://api.github.com/users/johanatandromeda/events{/privacy}', 'received_events_url': 'https://api.github.com/users/johanatandromeda/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}","[{'id': 5667396184, 'node_id': 'LA_kwDOJ0Z1Ps8AAAABUc2aWA', 'url': 'https://api.github.com/repos/ollama/ollama/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': ""Something isn't working""}]",open,False,,[],,0,2025-11-24T08:14:49Z,2025-11-24T08:14:49Z,,NONE,,,"{'total': 0, 'completed': 0, 'percent_completed': 0}","{'blocked_by': 0, 'total_blocked_by': 0, 'blocking': 0, 'total_blocking': 0}","### What is the issue?

I get an error loading mistral-small3.2:24b on an AMD RX 7600 XT. The setup is

Proxmox 9 on  AMD Ryzen 7 5800X -> VM with CPU x86-64-v3 and GPU with PCI passthrough -> Docker -> Ollama 0.13.0. The VM has 32GB RAM and the GPU is 16GB VRAM

I also get errors with devstral:24b and deepseek-r1:14b. qwen3:30b-a3b-instruct-2507-q4_k_M works.

### Relevant log output

```shell
ollama             | time=2025-11-24T06:34:43.466Z level=INFO source=server.go:392 msg=""starting runner"" cmd=""/usr/bin/ollama runner --model /root/.ollama/models/blobs/sha256-b3a2c9a8fef9be8d2ef951aecca36a36b9ea0b70abe9359eab4315bf4cd9be01 --port 32811""
ollama             | time=2025-11-24T06:34:43.466Z level=INFO source=sched.go:443 msg=""system memory"" total=""31.4 GiB"" free=""7.9 GiB"" free_swap=""6.7 GiB""
ollama             | time=2025-11-24T06:34:43.466Z level=INFO source=sched.go:450 msg=""gpu memory"" id=0 library=ROCm available=""15.5 GiB"" free=""16.0 GiB"" minimum=""457.0 MiB"" overhead=""0 B""
ollama             | time=2025-11-24T06:34:43.466Z level=INFO source=server.go:459 msg=""loading model"" ""model layers""=41 requested=-1
ollama             | time=2025-11-24T06:34:43.466Z level=INFO source=device.go:240 msg=""model weights"" device=ROCm0 size=""9.0 GiB""
ollama             | time=2025-11-24T06:34:43.466Z level=INFO source=device.go:245 msg=""model weights"" device=CPU size=""4.0 GiB""
ollama             | time=2025-11-24T06:34:43.466Z level=INFO source=device.go:251 msg=""kv cache"" device=ROCm0 size=""3.5 GiB""
ollama             | time=2025-11-24T06:34:43.466Z level=INFO source=device.go:256 msg=""kv cache"" device=CPU size=""1.3 GiB""
ollama             | time=2025-11-24T06:34:43.466Z level=INFO source=device.go:262 msg=""compute graph"" device=ROCm0 size=""2.2 GiB""
ollama             | time=2025-11-24T06:34:43.466Z level=INFO source=device.go:272 msg=""total memory"" size=""20.1 GiB""
ollama             | time=2025-11-24T06:34:43.473Z level=INFO source=runner.go:963 msg=""starting go runner""
ollama             | load_backend: loaded CPU backend from /usr/lib/ollama/libggml-cpu-haswell.so
ollama             | /opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory
ollama             | ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ollama             | ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ollama             | ggml_cuda_init: found 1 ROCm devices:
ollama             |   Device 0: AMD Radeon Graphics, gfx1102 (0x1102), VMM: no, Wave Size: 32, ID: 0
ollama             | load_backend: loaded ROCm backend from /usr/lib/ollama/rocm/libggml-hip.so
ollama             | time=2025-11-24T06:34:44.126Z level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 ROCm.0.NO_VMM=1 ROCm.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
ollama             | time=2025-11-24T06:34:44.126Z level=INFO source=runner.go:999 msg=""Server listening on 127.0.0.1:32811""
ollama             | time=2025-11-24T06:34:44.131Z level=INFO source=runner.go:893 msg=load request=""{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:false KvSize:32000 KvCacheType: NumThreads:6 GPULayers:29[ID:0 Layers:29(11..39)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}""
ollama             | ggml_hip_get_device_memory searching for device 0000:00:10.0
ollama             | ggml_backend_cuda_device_get_memory device 0000:00:10.0 utilizing AMD specific memory reporting free: 17135820800 total: 17163091968
ollama             | llama_model_load_from_file_impl: using device ROCm0 (AMD Radeon Graphics) (0000:00:10.0) - 16341 MiB free
ollama             | time=2025-11-24T06:34:44.134Z level=INFO source=server.go:1294 msg=""waiting for llama runner to start responding""
ollama             | time=2025-11-24T06:34:44.135Z level=INFO source=server.go:1328 msg=""waiting for server to become available"" status=""llm server loading model""
ollama             | llama_model_loader: loaded meta data with 41 key-value pairs and 363 tensors from /root/.ollama/models/blobs/sha256-b3a2c9a8fef9be8d2ef951aecca36a36b9ea0b70abe9359eab4315bf4cd9be01 (version GGUF V3 (latest))
ollama             | llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
ollama             | llama_model_loader: - kv   0:                       general.architecture str              = llama
ollama             | llama_model_loader: - kv   1:                  general.base_model.0.name str              = Devstrall Small 2505
ollama             | llama_model_loader: - kv   2:          general.base_model.0.organization str              = Mistralai
ollama             | llama_model_loader: - kv   3:              general.base_model.0.repo_url str              = https://huggingface.co/mistralai/Devs...
ollama             | llama_model_loader: - kv   4:               general.base_model.0.version str              = 2505
ollama             | llama_model_loader: - kv   5:                   general.base_model.count u32              = 1
ollama             | llama_model_loader: - kv   6:                           general.basename str              = Devstral
ollama             | llama_model_loader: - kv   7:                          general.file_type u32              = 15
ollama             | llama_model_loader: - kv   8:                          general.languages arr[str,24]      = [""en"", ""fr"", ""de"", ""es"", ""pt"", ""it"", ...
ollama             | llama_model_loader: - kv   9:                            general.license str              = apache-2.0
ollama             | llama_model_loader: - kv  10:                               general.name str              = Devstral Small 2505
ollama             | llama_model_loader: - kv  11:                    general.parameter_count u64              = 23572403200
ollama             | llama_model_loader: - kv  12:               general.quantization_version u32              = 2
ollama             | llama_model_loader: - kv  13:                         general.size_label str              = Small
ollama             | llama_model_loader: - kv  14:                               general.tags arr[str,1]       = [""text2text-generation""]
ollama             | llama_model_loader: - kv  15:                               general.type str              = model
ollama             | llama_model_loader: - kv  16:                            general.version str              = 2505
ollama             | llama_model_loader: - kv  17:                 llama.attention.head_count u32              = 32
ollama             | llama_model_loader: - kv  18:              llama.attention.head_count_kv u32              = 8
ollama             | llama_model_loader: - kv  19:                 llama.attention.key_length u32              = 128
ollama             | llama_model_loader: - kv  20:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
ollama             | llama_model_loader: - kv  21:               llama.attention.value_length u32              = 128
ollama             | llama_model_loader: - kv  22:                          llama.block_count u32              = 40
ollama             | llama_model_loader: - kv  23:                       llama.context_length u32              = 131072
ollama             | llama_model_loader: - kv  24:                     llama.embedding_length u32              = 5120
ollama             | llama_model_loader: - kv  25:                  llama.feed_forward_length u32              = 32768
ollama             | llama_model_loader: - kv  26:                 llama.rope.dimension_count u32              = 128
ollama             | llama_model_loader: - kv  27:                       llama.rope.freq_base f32              = 1000000000.000000
ollama             | llama_model_loader: - kv  28:                           llama.vocab_size u32              = 131072
ollama             | llama_model_loader: - kv  29:                    tokenizer.chat_template str              = {%- set today = strftime_now(""%Y-%m-%...
ollama             | llama_model_loader: - kv  30:               tokenizer.ggml.add_bos_token bool             = true
ollama             | llama_model_loader: - kv  31:               tokenizer.ggml.add_eos_token bool             = false
ollama             | llama_model_loader: - kv  32:            tokenizer.ggml.add_space_prefix bool             = false
ollama             | llama_model_loader: - kv  33:                tokenizer.ggml.bos_token_id u32              = 1
ollama             | llama_model_loader: - kv  34:                tokenizer.ggml.eos_token_id u32              = 2
ollama             | llama_model_loader: - kv  35:                      tokenizer.ggml.merges arr[str,269443]  = [""Ġ Ġ"", ""Ġ t"", ""e r"", ""i n"", ""Ġ �...
ollama             | llama_model_loader: - kv  36:                       tokenizer.ggml.model str              = gpt2
ollama             | llama_model_loader: - kv  37:                         tokenizer.ggml.pre str              = tekken
ollama             | llama_model_loader: - kv  38:                  tokenizer.ggml.token_type arr[i32,131072]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...
ollama             | llama_model_loader: - kv  39:                      tokenizer.ggml.tokens arr[str,131072]  = [""<unk>"", ""<s>"", ""</s>"", ""[INST]"", ""[...
ollama             | llama_model_loader: - kv  40:            tokenizer.ggml.unknown_token_id u32              = 0
ollama             | llama_model_loader: - type  f32:   81 tensors
ollama             | llama_model_loader: - type q4_K:  241 tensors
ollama             | llama_model_loader: - type q6_K:   41 tensors
ollama             | print_info: file format = GGUF V3 (latest)
ollama             | print_info: file type   = Q4_K - Medium
ollama             | print_info: file size   = 13.34 GiB (4.86 BPW) 
ollama             | load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
ollama             | load: printing all EOG tokens:
ollama             | load:   - 2 ('</s>')
ollama             | load: special tokens cache size = 1000
ollama             | load: token to piece cache size = 0.8498 MB
ollama             | print_info: arch             = llama
ollama             | print_info: vocab_only       = 0
ollama             | print_info: n_ctx_train      = 131072
ollama             | print_info: n_embd           = 5120
ollama             | print_info: n_layer          = 40
ollama             | print_info: n_head           = 32
ollama             | print_info: n_head_kv        = 8
ollama             | print_info: n_rot            = 128
ollama             | print_info: n_swa            = 0
ollama             | print_info: is_swa_any       = 0
ollama             | print_info: n_embd_head_k    = 128
ollama             | print_info: n_embd_head_v    = 128
ollama             | print_info: n_gqa            = 4
ollama             | print_info: n_embd_k_gqa     = 1024
ollama             | print_info: n_embd_v_gqa     = 1024
ollama             | print_info: f_norm_eps       = 0.0e+00
ollama             | print_info: f_norm_rms_eps   = 1.0e-05
ollama             | print_info: f_clamp_kqv      = 0.0e+00
ollama             | print_info: f_max_alibi_bias = 0.0e+00
ollama             | print_info: f_logit_scale    = 0.0e+00
ollama             | print_info: f_attn_scale     = 0.0e+00
ollama             | print_info: n_ff             = 32768
ollama             | print_info: n_expert         = 0
ollama             | print_info: n_expert_used    = 0
ollama             | print_info: causal attn      = 1
ollama             | print_info: pooling type     = 0
ollama             | print_info: rope type        = 0
ollama             | print_info: rope scaling     = linear
ollama             | print_info: freq_base_train  = 1000000000.0
ollama             | print_info: freq_scale_train = 1
ollama             | print_info: n_ctx_orig_yarn  = 131072
ollama             | print_info: rope_finetuned   = unknown
ollama             | print_info: model type       = 13B
ollama             | print_info: model params     = 23.57 B
ollama             | print_info: general.name     = Devstral Small 2505
ollama             | print_info: vocab type       = BPE
ollama             | print_info: n_vocab          = 131072
ollama             | print_info: n_merges         = 269443
ollama             | print_info: BOS token        = 1 '<s>'
ollama             | print_info: EOS token        = 2 '</s>'
ollama             | print_info: UNK token        = 0 '<unk>'
ollama             | print_info: LF token         = 1010 'Ċ'
ollama             | print_info: EOG token        = 2 '</s>'
ollama             | print_info: max token length = 150
ollama             | load_tensors: loading model tensors, this can take a while... (mmap = false)
ollama             | load_tensors: offloading 29 repeating layers to GPU
ollama             | load_tensors: offloaded 29/41 layers to GPU
ollama             | load_tensors:    ROCm_Host model buffer size =  4102.60 MiB
ollama             | load_tensors:        ROCm0 model buffer size =  9199.77 MiB
ollama             | load_tensors:          CPU model buffer size =   360.00 MiB
ollama             | llama_context: constructing llama_context
ollama             | llama_context: n_seq_max     = 1
ollama             | llama_context: n_ctx         = 32000
ollama             | llama_context: n_ctx_per_seq = 32000
ollama             | llama_context: n_batch       = 512
ollama             | llama_context: n_ubatch      = 512
ollama             | llama_context: causal_attn   = 1
ollama             | llama_context: flash_attn    = disabled
ollama             | llama_context: kv_unified    = false
ollama             | llama_context: freq_base     = 1000000000.0
ollama             | llama_context: freq_scale    = 1
ollama             | llama_context: n_ctx_per_seq (32000) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ollama             | llama_context:        CPU  output buffer size =     0.52 MiB
ollama             | llama_kv_cache:      ROCm0 KV buffer size =  3625.00 MiB
ollama             | llama_kv_cache:        CPU KV buffer size =  1375.00 MiB
ollama             | llama_kv_cache: size = 5000.00 MiB ( 32000 cells,  40 layers,  1/1 seqs), K (f16): 2500.00 MiB, V (f16): 2500.00 MiB
ollama             | graph_reserve: failed to allocate compute buffers
ollama             | SIGSEGV: segmentation violation
ollama             | PC=0x7f3e3679c78a m=3 sigcode=1 addr=0x7f413461d0d8
ollama             | signal arrived during cgo execution
ollama             | 
ollama             | goroutine 25 gp=0xc000003dc0 m=3 mp=0xc00006b008 [syscall]:
ollama             | runtime.cgocall(0x55e5e3541b50, 0xc000319c00)
ollama             | 	runtime/cgocall.go:167 +0x4b fp=0xc000319bd8 sp=0xc000319ba0 pc=0x55e5e2824b0b
ollama             | github.com/ollama/ollama/llama._Cfunc_llama_init_from_model(0x7f3e70000dc0, {0x7d00, 0x200, 0x200, 0x1, 0x6, 0x6, 0xffffffff, 0xffffffff, 0xffffffff, ...})
ollama             | 	_cgo_gotypes.go:762 +0x4e fp=0xc000319c00 sp=0xc000319bd8 pc=0x55e5e2bddaae
ollama             | github.com/ollama/ollama/llama.NewContextWithModel.func1(...)
ollama             | 	github.com/ollama/ollama/llama/llama.go:317
ollama             | github.com/ollama/ollama/llama.NewContextWithModel(0xc0001d53a0, {{0x7d00, 0x200, 0x200, 0x1, 0x6, 0x6, 0xffffffff, 0xffffffff, 0xffffffff, ...}})
ollama             | 	github.com/ollama/ollama/llama/llama.go:317 +0x158 fp=0xc000319da0 sp=0xc000319c00 pc=0x55e5e2be1cd8
ollama             | github.com/ollama/ollama/runner/llamarunner.(*Server).loadModel(0xc0000d0dc0, {{0xc0001d4d90, 0x1, 0x1}, 0x1d, 0x0, 0x0, {0xc0001d4d88, 0x1, 0x2}, ...}, ...)
ollama             | 	github.com/ollama/ollama/runner/llamarunner/runner.go:845 +0x178 fp=0xc000319ee8 sp=0xc000319da0 pc=0x55e5e2c9a418
ollama             | github.com/ollama/ollama/runner/llamarunner.(*Server).load.gowrap2()
ollama             | 	github.com/ollama/ollama/runner/llamarunner/runner.go:932 +0x115 fp=0xc000319fe0 sp=0xc000319ee8 pc=0x55e5e2c9b635
ollama             | runtime.goexit({})
ollama             | 	runtime/asm_amd64.s:1700 +0x1 fp=0xc000319fe8 sp=0xc000319fe0 pc=0x55e5e282fe21
ollama             | created by github.com/ollama/ollama/runner/llamarunner.(*Server).load in goroutine 7
ollama             | 	github.com/ollama/ollama/runner/llamarunner/runner.go:932 +0x88a
ollama             | 
ollama             | goroutine 1 gp=0xc000002380 m=nil [IO wait]:
ollama             | runtime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)
ollama             | 	runtime/proc.go:435 +0xce fp=0xc00051d790 sp=0xc00051d770 pc=0x55e5e2827f8e
ollama             | runtime.netpollblock(0xc00051d7e0?, 0xe27c16c6?, 0xe5?)
ollama             | 	runtime/netpoll.go:575 +0xf7 fp=0xc00051d7c8 sp=0xc00051d790 pc=0x55e5e27ed2b7
ollama             | internal/poll.runtime_pollWait(0x7f3ec3e13de0, 0x72)
ollama             | 	runtime/netpoll.go:351 +0x85 fp=0xc00051d7e8 sp=0xc00051d7c8 pc=0x55e5e28271a5
ollama             | internal/poll.(*pollDesc).wait(0xc0000a4d00?, 0x900000036?, 0x0)
ollama             | 	internal/poll/fd_poll_runtime.go:84 +0x27 fp=0xc00051d810 sp=0xc00051d7e8 pc=0x55e5e28af0e7
ollama             | internal/poll.(*pollDesc).waitRead(...)
ollama             | 	internal/poll/fd_poll_runtime.go:89
ollama             | internal/poll.(*FD).Accept(0xc0000a4d00)
ollama             | 	internal/poll/fd_unix.go:620 +0x295 fp=0xc00051d8b8 sp=0xc00051d810 pc=0x55e5e28b44b5
ollama             | net.(*netFD).accept(0xc0000a4d00)
ollama             | 	net/fd_unix.go:172 +0x29 fp=0xc00051d970 sp=0xc00051d8b8 pc=0x55e5e2927389
ollama             | net.(*TCPListener).accept(0xc000092f40)
ollama             | 	net/tcpsock_posix.go:159 +0x1b fp=0xc00051d9c0 sp=0xc00051d970 pc=0x55e5e293cd3b
ollama             | net.(*TCPListener).Accept(0xc000092f40)
ollama             | 	net/tcpsock.go:380 +0x30 fp=0xc00051d9f0 sp=0xc00051d9c0 pc=0x55e5e293bbf0
ollama             | net/http.(*onceCloseListener).Accept(0xc0000ce3f0?)
ollama             | 	<autogenerated>:1 +0x24 fp=0xc00051da08 sp=0xc00051d9f0 pc=0x55e5e2b533c4
ollama             | net/http.(*Server).Serve(0xc0001f9700, {0x55e5e3d4ff60, 0xc000092f40})
ollama             | 	net/http/server.go:3424 +0x30c fp=0xc00051db38 sp=0xc00051da08 pc=0x55e5e2b2ac8c
ollama             | github.com/ollama/ollama/runner/llamarunner.Execute({0xc00012e140, 0x4, 0x4})
ollama             | 	github.com/ollama/ollama/runner/llamarunner/runner.go:1000 +0x8f5 fp=0xc00051dd08 sp=0xc00051db38 pc=0x55e5e2c9bff5
ollama             | github.com/ollama/ollama/runner.Execute({0xc00012e130?, 0x0?, 0x0?})
ollama             | 	github.com/ollama/ollama/runner/runner.go:22 +0xd4 fp=0xc00051dd30 sp=0xc00051dd08 pc=0x55e5e2d42454
ollama             | github.com/ollama/ollama/cmd.NewCLI.func2(0xc0001f9400?, {0x55e5e38510ab?, 0x4?, 0x55e5e38510af?})
ollama             | 	github.com/ollama/ollama/cmd/cmd.go:1841 +0x45 fp=0xc00051dd58 sp=0xc00051dd30 pc=0x55e5e34d1d85
ollama             | github.com/spf13/cobra.(*Command).execute(0xc0000d5508, {0xc000092d40, 0x4, 0x4})
ollama             | 	github.com/spf13/cobra@v1.7.0/command.go:940 +0x85c fp=0xc00051de78 sp=0xc00051dd58 pc=0x55e5e29a09dc
ollama             | github.com/spf13/cobra.(*Command).ExecuteC(0xc00054cf08)
ollama             | 	github.com/spf13/cobra@v1.7.0/command.go:1068 +0x3a5 fp=0xc00051df30 sp=0xc00051de78 pc=0x55e5e29a1225
ollama             | github.com/spf13/cobra.(*Command).Execute(...)
ollama             | 	github.com/spf13/cobra@v1.7.0/command.go:992
ollama             | github.com/spf13/cobra.(*Command).ExecuteContext(...)
ollama             | 	github.com/spf13/cobra@v1.7.0/command.go:985
ollama             | main.main()
ollama             | 	github.com/ollama/ollama/main.go:12 +0x4d fp=0xc00051df50 sp=0xc00051df30 pc=0x55e5e34d286d
ollama             | runtime.main()
ollama             | 	runtime/proc.go:283 +0x29d fp=0xc00051dfe0 sp=0xc00051df50 pc=0x55e5e27f493d
ollama             | runtime.goexit({})
ollama             | 	runtime/asm_amd64.s:1700 +0x1 fp=0xc00051dfe8 sp=0xc00051dfe0 pc=0x55e5e282fe21
ollama             | 
ollama             | goroutine 2 gp=0xc000002e00 m=nil [force gc (idle)]:
ollama             | runtime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)
ollama             | 	runtime/proc.go:435 +0xce fp=0xc000064fa8 sp=0xc000064f88 pc=0x55e5e2827f8e
ollama             | runtime.goparkunlock(...)
ollama             | 	runtime/proc.go:441
ollama             | runtime.forcegchelper()
ollama             | 	runtime/proc.go:348 +0xb8 fp=0xc000064fe0 sp=0xc000064fa8 pc=0x55e5e27f4c78
ollama             | runtime.goexit({})
ollama             | 	runtime/asm_amd64.s:1700 +0x1 fp=0xc000064fe8 sp=0xc000064fe0 pc=0x55e5e282fe21
ollama             | created by runtime.init.7 in goroutine 1
ollama             | 	runtime/proc.go:336 +0x1a
ollama             | 
ollama             | goroutine 3 gp=0xc000003340 m=nil [GC sweep wait]:
ollama             | runtime.gopark(0x1?, 0x0?, 0x0?, 0x0?, 0x0?)
ollama             | 	runtime/proc.go:435 +0xce fp=0xc000065780 sp=0xc000065760 pc=0x55e5e2827f8e
ollama             | runtime.goparkunlock(...)
ollama             | 	runtime/proc.go:441
ollama             | runtime.bgsweep(0xc000090000)
ollama             | 	runtime/mgcsweep.go:316 +0xdf fp=0xc0000657c8 sp=0xc000065780 pc=0x55e5e27df41f
ollama             | runtime.gcenable.gowrap1()
ollama             | 	runtime/mgc.go:204 +0x25 fp=0xc0000657e0 sp=0xc0000657c8 pc=0x55e5e27d3805
ollama             | runtime.goexit({})
ollama             | 	runtime/asm_amd64.s:1700 +0x1 fp=0xc0000657e8 sp=0xc0000657e0 pc=0x55e5e282fe21
ollama             | created by runtime.gcenable in goroutine 1
ollama             | 	runtime/mgc.go:204 +0x66
ollama             | 
ollama             | goroutine 4 gp=0xc000003500 m=nil [GC scavenge wait]:
ollama             | runtime.gopark(0x10000?, 0x55e5e3a1b148?, 0x0?, 0x0?, 0x0?)
ollama             | 	runtime/proc.go:435 +0xce fp=0xc000065f78 sp=0xc000065f58 pc=0x55e5e2827f8e
ollama             | runtime.goparkunlock(...)
ollama             | 	runtime/proc.go:441
ollama             | runtime.(*scavengerState).park(0x55e5e4612100)
ollama             | 	runtime/mgcscavenge.go:425 +0x49 fp=0xc000065fa8 sp=0xc000065f78 pc=0x55e5e27dce69
ollama             | runtime.bgscavenge(0xc000090000)
ollama             | 	runtime/mgcscavenge.go:658 +0x59 fp=0xc000065fc8 sp=0xc000065fa8 pc=0x55e5e27dd3f9
ollama             | runtime.gcenable.gowrap2()
ollama             | 	runtime/mgc.go:205 +0x25 fp=0xc000065fe0 sp=0xc000065fc8 pc=0x55e5e27d37a5
ollama             | runtime.goexit({})
ollama             | 	runtime/asm_amd64.s:1700 +0x1 fp=0xc000065fe8 sp=0xc000065fe0 pc=0x55e5e282fe21
ollama             | created by runtime.gcenable in goroutine 1
ollama             | 	runtime/mgc.go:205 +0xa5
ollama             | 
ollama             | goroutine 18 gp=0xc000102700 m=nil [finalizer wait]:
ollama             | runtime.gopark(0x1b8?, 0xc000002380?, 0x1?, 0x23?, 0xc000064688?)
ollama             | 	runtime/proc.go:435 +0xce fp=0xc000064630 sp=0xc000064610 pc=0x55e5e2827f8e
ollama             | runtime.runfinq()
ollama             | 	runtime/mfinal.go:196 +0x107 fp=0xc0000647e0 sp=0xc000064630 pc=0x55e5e27d27c7
ollama             | runtime.goexit({})
ollama             | 	runtime/asm_amd64.s:1700 +0x1 fp=0xc0000647e8 sp=0xc0000647e0 pc=0x55e5e282fe21
ollama             | created by runtime.createfing in goroutine 1
ollama             | 	runtime/mfinal.go:166 +0x3d
ollama             | 
ollama             | goroutine 19 gp=0xc000103180 m=nil [chan receive]:
ollama             | runtime.gopark(0xc0001d3860?, 0xc000590018?, 0x60?, 0x7?, 0x55e5e290dfc8?)
ollama             | 	runtime/proc.go:435 +0xce fp=0xc000060718 sp=0xc0000606f8 pc=0x55e5e2827f8e
ollama             | runtime.chanrecv(0xc000110310, 0x0, 0x1)
ollama             | 	runtime/chan.go:664 +0x445 fp=0xc000060790 sp=0xc000060718 pc=0x55e5e27c42a5
ollama             | runtime.chanrecv1(0x0?, 0x0?)
ollama             | 	runtime/chan.go:506 +0x12 fp=0xc0000607b8 sp=0xc000060790 pc=0x55e5e27c3e32
ollama             | runtime.unique_runtime_registerUniqueMapCleanup.func2(...)
ollama             | 	runtime/mgc.go:1796
ollama             | runtime.unique_runtime_registerUniqueMapCleanup.gowrap1()
ollama             | 	runtime/mgc.go:1799 +0x2f fp=0xc0000607e0 sp=0xc0000607b8 pc=0x55e5e27d69af
ollama             | runtime.goexit({})
ollama             | 	runtime/asm_amd64.s:1700 +0x1 fp=0xc0000607e8 sp=0xc0000607e0 pc=0x55e5e282fe21
ollama             | created by unique.runtime_registerUniqueMapCleanup in goroutine 1
ollama             | 	runtime/mgc.go:1794 +0x85
ollama             | 
ollama             | goroutine 20 gp=0xc000103500 m=nil [GC worker (idle)]:
ollama             | runtime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)
ollama             | 	runtime/proc.go:435 +0xce fp=0xc000060f38 sp=0xc000060f18 pc=0x55e5e2827f8e
ollama             | runtime.gcBgMarkWorker(0xc000111730)
ollama             | 	runtime/mgc.go:1423 +0xe9 fp=0xc000060fc8 sp=0xc000060f38 pc=0x55e5e27d5cc9
ollama             | runtime.gcBgMarkStartWorkers.gowrap1()
ollama             | 	runtime/mgc.go:1339 +0x25 fp=0xc000060fe0 sp=0xc000060fc8 pc=0x55e5e27d5ba5
ollama             | runtime.goexit({})
ollama             | 	runtime/asm_amd64.s:1700 +0x1 fp=0xc000060fe8 sp=0xc000060fe0 pc=0x55e5e282fe21
ollama             | created by runtime.gcBgMarkStartWorkers in goroutine 1
ollama             | 	runtime/mgc.go:1339 +0x105
ollama             | 
ollama             | goroutine 21 gp=0xc0001036c0 m=nil [GC worker (idle)]:
ollama             | runtime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)
ollama             | 	runtime/proc.go:435 +0xce fp=0xc000061738 sp=0xc000061718 pc=0x55e5e2827f8e
ollama             | runtime.gcBgMarkWorker(0xc000111730)
ollama             | 	runtime/mgc.go:1423 +0xe9 fp=0xc0000617c8 sp=0xc000061738 pc=0x55e5e27d5cc9
ollama             | runtime.gcBgMarkStartWorkers.gowrap1()
ollama             | 	runtime/mgc.go:1339 +0x25 fp=0xc0000617e0 sp=0xc0000617c8 pc=0x55e5e27d5ba5
ollama             | runtime.goexit({})
ollama             | 	runtime/asm_amd64.s:1700 +0x1 fp=0xc0000617e8 sp=0xc0000617e0 pc=0x55e5e282fe21
ollama             | created by runtime.gcBgMarkStartWorkers in goroutine 1
ollama             | 	runtime/mgc.go:1339 +0x105
ollama             | 
ollama             | goroutine 34 gp=0xc000504000 m=nil [GC worker (idle)]:
ollama             | runtime.gopark(0x2b4209e582fe?, 0x0?, 0x0?, 0x0?, 0x0?)
ollama             | 	runtime/proc.go:435 +0xce fp=0xc00050a738 sp=0xc00050a718 pc=0x55e5e2827f8e
ollama             | runtime.gcBgMarkWorker(0xc000111730)
ollama             | 	runtime/mgc.go:1423 +0xe9 fp=0xc00050a7c8 sp=0xc00050a738 pc=0x55e5e27d5cc9
ollama             | runtime.gcBgMarkStartWorkers.gowrap1()
ollama             | 	runtime/mgc.go:1339 +0x25 fp=0xc00050a7e0 sp=0xc00050a7c8 pc=0x55e5e27d5ba5
ollama             | runtime.goexit({})
ollama             | 	runtime/asm_amd64.s:1700 +0x1 fp=0xc00050a7e8 sp=0xc00050a7e0 pc=0x55e5e282fe21
ollama             | created by runtime.gcBgMarkStartWorkers in goroutine 1
ollama             | 	runtime/mgc.go:1339 +0x105
ollama             | 
ollama             | goroutine 5 gp=0xc000003a40 m=nil [GC worker (idle)]:
ollama             | runtime.gopark(0x2b4209e586dc?, 0x0?, 0x0?, 0x0?, 0x0?)
ollama             | 	runtime/proc.go:435 +0xce fp=0xc000066738 sp=0xc000066718 pc=0x55e5e2827f8e
ollama             | runtime.gcBgMarkWorker(0xc000111730)
ollama             | 	runtime/mgc.go:1423 +0xe9 fp=0xc0000667c8 sp=0xc000066738 pc=0x55e5e27d5cc9
ollama             | runtime.gcBgMarkStartWorkers.gowrap1()
ollama             | 	runtime/mgc.go:1339 +0x25 fp=0xc0000667e0 sp=0xc0000667c8 pc=0x55e5e27d5ba5
ollama             | runtime.goexit({})
ollama             | 	runtime/asm_amd64.s:1700 +0x1 fp=0xc0000667e8 sp=0xc0000667e0 pc=0x55e5e282fe21
ollama             | created by runtime.gcBgMarkStartWorkers in goroutine 1
ollama             | 	runtime/mgc.go:1339 +0x105
ollama             | 
ollama             | goroutine 22 gp=0xc000103880 m=nil [GC worker (idle)]:
ollama             | runtime.gopark(0x2b4209e59078?, 0x0?, 0x0?, 0x0?, 0x0?)
ollama             | 	runtime/proc.go:435 +0xce fp=0xc000061f38 sp=0xc000061f18 pc=0x55e5e2827f8e
ollama             | runtime.gcBgMarkWorker(0xc000111730)
ollama             | 	runtime/mgc.go:1423 +0xe9 fp=0xc000061fc8 sp=0xc000061f38 pc=0x55e5e27d5cc9
ollama             | runtime.gcBgMarkStartWorkers.gowrap1()
ollama             | 	runtime/mgc.go:1339 +0x25 fp=0xc000061fe0 sp=0xc000061fc8 pc=0x55e5e27d5ba5
ollama             | runtime.goexit({})
ollama             | 	runtime/asm_amd64.s:1700 +0x1 fp=0xc000061fe8 sp=0xc000061fe0 pc=0x55e5e282fe21
ollama             | created by runtime.gcBgMarkStartWorkers in goroutine 1
ollama             | 	runtime/mgc.go:1339 +0x105
ollama             | 
ollama             | goroutine 23 gp=0xc000103a40 m=nil [GC worker (idle)]:
ollama             | runtime.gopark(0x2b4209e58254?, 0x0?, 0x0?, 0x0?, 0x0?)
ollama             | 	runtime/proc.go:435 +0xce fp=0xc000062738 sp=0xc000062718 pc=0x55e5e2827f8e
ollama             | runtime.gcBgMarkWorker(0xc000111730)
ollama             | 	runtime/mgc.go:1423 +0xe9 fp=0xc0000627c8 sp=0xc000062738 pc=0x55e5e27d5cc9
ollama             | runtime.gcBgMarkStartWorkers.gowrap1()
ollama             | 	runtime/mgc.go:1339 +0x25 fp=0xc0000627e0 sp=0xc0000627c8 pc=0x55e5e27d5ba5
ollama             | runtime.goexit({})
ollama             | 	runtime/asm_amd64.s:1700 +0x1 fp=0xc0000627e8 sp=0xc0000627e0 pc=0x55e5e282fe21
ollama             | created by runtime.gcBgMarkStartWorkers in goroutine 1
ollama             | 	runtime/mgc.go:1339 +0x105
ollama             | 
ollama             | goroutine 6 gp=0xc000504c40 m=nil [sync.WaitGroup.Wait]:
ollama             | runtime.gopark(0x0?, 0x0?, 0x60?, 0x40?, 0x0?)
ollama             | 	runtime/proc.go:435 +0xce fp=0xc00050d620 sp=0xc00050d600 pc=0x55e5e2827f8e
ollama             | runtime.goparkunlock(...)
ollama             | 	runtime/proc.go:441
ollama             | runtime.semacquire1(0xc0000d0de0, 0x0, 0x1, 0x0, 0x18)
ollama             | 	runtime/sema.go:188 +0x229 fp=0xc00050d688 sp=0xc00050d620 pc=0x55e5e2807f09
ollama             | sync.runtime_SemacquireWaitGroup(0x0?)
ollama             | 	runtime/sema.go:110 +0x25 fp=0xc00050d6c0 sp=0xc00050d688 pc=0x55e5e28298c5
ollama             | sync.(*WaitGroup).Wait(0x0?)
ollama             | 	sync/waitgroup.go:118 +0x48 fp=0xc00050d6e8 sp=0xc00050d6c0 pc=0x55e5e283b768
ollama             | github.com/ollama/ollama/runner/llamarunner.(*Server).run(0xc0000d0dc0, {0x55e5e3d52580, 0xc00009ebe0})
ollama             | 	github.com/ollama/ollama/runner/llamarunner/runner.go:359 +0x4b fp=0xc00050d7b8 sp=0xc00050d6e8 pc=0x55e5e2c96dcb
ollama             | github.com/ollama/ollama/runner/llamarunner.Execute.gowrap1()
ollama             | 	github.com/ollama/ollama/runner/llamarunner/runner.go:979 +0x28 fp=0xc00050d7e0 sp=0xc00050d7b8 pc=0x55e5e2c9c268
ollama             | runtime.goexit({})
ollama             | 	runtime/asm_amd64.s:1700 +0x1 fp=0xc00050d7e8 sp=0xc00050d7e0 pc=0x55e5e282fe21
ollama             | created by github.com/ollama/ollama/runner/llamarunner.Execute in goroutine 1
ollama             | 	github.com/ollama/ollama/runner/llamarunner/runner.go:979 +0x4c5
ollama             | 
ollama             | goroutine 7 gp=0xc000504e00 m=nil [IO wait]:
ollama             | runtime.gopark(0x55e5e28b26e5?, 0xc0000a4d80?, 0x40?, 0x9a?, 0xb?)
ollama             | 	runtime/proc.go:435 +0xce fp=0xc000049948 sp=0xc000049928 pc=0x55e5e2827f8e
ollama             | runtime.netpollblock(0x55e5e284b638?, 0xe27c16c6?, 0xe5?)
ollama             | 	runtime/netpoll.go:575 +0xf7 fp=0xc000049980 sp=0xc000049948 pc=0x55e5e27ed2b7
ollama             | internal/poll.runtime_pollWait(0x7f3ec3e13cc8, 0x72)
ollama             | 	runtime/netpoll.go:351 +0x85 fp=0xc0000499a0 sp=0xc000049980 pc=0x55e5e28271a5
ollama             | internal/poll.(*pollDesc).wait(0xc0000a4d80?, 0xc0000f8000?, 0x0)
ollama             | 	internal/poll/fd_poll_runtime.go:84 +0x27 fp=0xc0000499c8 sp=0xc0000499a0 pc=0x55e5e28af0e7
ollama             | internal/poll.(*pollDesc).waitRead(...)
ollama             | 	internal/poll/fd_poll_runtime.go:89
ollama             | internal/poll.(*FD).Read(0xc0000a4d80, {0xc0000f8000, 0x1000, 0x1000})
ollama             | 	internal/poll/fd_unix.go:165 +0x27a fp=0xc000049a60 sp=0xc0000499c8 pc=0x55e5e28b03da
ollama             | net.(*netFD).Read(0xc0000a4d80, {0xc0000f8000?, 0xc000049ad0?, 0x55e5e28af5a5?})
ollama             | 	net/fd_posix.go:55 +0x25 fp=0xc000049aa8 sp=0xc000049a60 pc=0x55e5e29253e5
ollama             | net.(*conn).Read(0xc000526580, {0xc0000f8000?, 0x0?, 0x0?})
ollama             | 	net/net.go:194 +0x45 fp=0xc000049af0 sp=0xc000049aa8 pc=0x55e5e29337a5
ollama             | net/http.(*connReader).Read(0xc000608fc0, {0xc0000f8000, 0x1000, 0x1000})
ollama             | 	net/http/server.go:798 +0x159 fp=0xc000049b40 sp=0xc000049af0 pc=0x55e5e2b1fb39
ollama             | bufio.(*Reader).fill(0xc000034840)
ollama             | 	bufio/bufio.go:113 +0x103 fp=0xc000049b78 sp=0xc000049b40 pc=0x55e5e294af43
ollama             | bufio.(*Reader).Peek(0xc000034840, 0x4)
ollama             | 	bufio/bufio.go:152 +0x53 fp=0xc000049b98 sp=0xc000049b78 pc=0x55e5e294b073
ollama             | net/http.(*conn).serve(0xc0000ce3f0, {0x55e5e3d52548, 0xc000608ed0})
ollama             | 	net/http/server.go:2137 +0x785 fp=0xc000049fb8 sp=0xc000049b98 pc=0x55e5e2b25925
ollama             | net/http.(*Server).Serve.gowrap3()
ollama             | 	net/http/server.go:3454 +0x28 fp=0xc000049fe0 sp=0xc000049fb8 pc=0x55e5e2b2b088
ollama             | runtime.goexit({})
ollama             | 	runtime/asm_amd64.s:1700 +0x1 fp=0xc000049fe8 sp=0xc000049fe0 pc=0x55e5e282fe21
ollama             | created by net/http.(*Server).Serve in goroutine 1
ollama             | 	net/http/server.go:3454 +0x485
ollama             | 
ollama             | rax    0x7f3e73f19720
ollama             | rbx    0x1
ollama             | rcx    0x580e06a0
ollama             | rdx    0x2
ollama             | rdi    0x7f39580e7f90
ollama             | rsi    0x0
ollama             | rbp    0x7f3958119450
ollama             | rsp    0x7f3e74bfeaa8
ollama             | r8     0x7f3e73f1b
ollama             | r9     0x7
ollama             | r10    0x7f3e73f1be80
ollama             | r11    0xca517d945fa33498
ollama             | r12    0x7f3958119450
ollama             | r13    0x0
ollama             | r14    0x0
ollama             | r15    0x7f3e70000dc0
ollama             | rip    0x7f3e3679c78a
ollama             | rflags 0x10202
ollama             | cs     0x33
ollama             | fs     0x0
ollama             | gs     0x0
ollama             | time=2025-11-24T06:34:48.710Z level=INFO source=server.go:1328 msg=""waiting for server to become available"" status=""llm server not responding""
ollama             | time=2025-11-24T06:34:48.961Z level=INFO source=sched.go:470 msg=""Load failed"" model=/root/.ollama/models/blobs/sha256-b3a2c9a8fef9be8d2ef951aecca36a36b9ea0b70abe9359eab4315bf4cd9be01 error=""llama runner process has terminated: exit status 2""
```

### OS

Docker

### GPU

AMD

### CPU

AMD

### Ollama version

0.13.0",,"{'url': 'https://api.github.com/repos/ollama/ollama/issues/13224/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}",https://api.github.com/repos/ollama/ollama/issues/13224/timeline,,,,
https://api.github.com/repos/ollama/ollama/issues/13223,https://api.github.com/repos/ollama/ollama,https://api.github.com/repos/ollama/ollama/issues/13223/labels{/name},https://api.github.com/repos/ollama/ollama/issues/13223/comments,https://api.github.com/repos/ollama/ollama/issues/13223/events,https://github.com/ollama/ollama/issues/13223,3657023159,I_kwDOJ0Z1Ps7Z-b63,13223,Ollama Exposes the Local Model Directory via Network Requests in ollama/ollama,"{'login': 'ylwango613', 'id': 128395302, 'node_id': 'U_kgDOB6coJg', 'avatar_url': 'https://avatars.githubusercontent.com/u/128395302?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/ylwango613', 'html_url': 'https://github.com/ylwango613', 'followers_url': 'https://api.github.com/users/ylwango613/followers', 'following_url': 'https://api.github.com/users/ylwango613/following{/other_user}', 'gists_url': 'https://api.github.com/users/ylwango613/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/ylwango613/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/ylwango613/subscriptions', 'organizations_url': 'https://api.github.com/users/ylwango613/orgs', 'repos_url': 'https://api.github.com/users/ylwango613/repos', 'events_url': 'https://api.github.com/users/ylwango613/events{/privacy}', 'received_events_url': 'https://api.github.com/users/ylwango613/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}","[{'id': 5667396184, 'node_id': 'LA_kwDOJ0Z1Ps8AAAABUc2aWA', 'url': 'https://api.github.com/repos/ollama/ollama/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': ""Something isn't working""}]",open,False,,[],,0,2025-11-24T04:22:44Z,2025-11-24T04:22:44Z,,NONE,,,"{'total': 0, 'completed': 0, 'percent_completed': 0}","{'blocked_by': 0, 'total_blocked_by': 0, 'blocking': 0, 'total_blocking': 0}","### What is the issue?

## Description

Ollama performs improper error handling, which leads to leakage of sensitive information from the host machine.

When specific malformed input is sent to the `/api/create` endpoint, the server returns detailed filesystem paths in its error messages. These paths reveal both the username under which Ollama is running and the exact directory where model files are stored.

------

## Proof of Concept

A local Ollama server was started to simulate a network-accessible environment:

```bash
ollama serve
```

Then the following request was executed:

```bash
curl http://localhost:11434/api/create -d '{
  ""model"": ""my-gguf-model"",
  ""files"": {
    ""0.gguf"": """"
  }
}'
```

The response was:

```
{""status"":""parsing GGUF""}
{""error"":""read /home/yuelinwang/.ollama/models/blobs: is a directory""}
```

The returned error message exposes:

- The current username (`yuelinwang`)
- The exact model directory used by Ollama (`/home/yuelinwang/.ollama/models/blobs`)

------

## Impact

An attacker can obtain:

- The username under which the Ollama service is executing.
- The absolute filesystem path of Ollama’s model storage directory.

Although no direct code execution or privilege escalation occurs from this issue alone, the leaked information can significantly aid reconnaissance and facilitate targeted attacks in more complex threat scenarios.

------

## Root Cause Analysis & Remediation Advice

The root cause lies in the implementation of `GetBlobsPath` within the following code:

https://github.com/ollama/ollama/blob/main/server/modelpath.go#L125-L146

```go
func GetBlobsPath(digest string) (string, error) {
    // only accept actual sha256 digests
    pattern := ""^sha256[:-][0-9a-fA-F]{64}$""
    re := regexp.MustCompile(pattern)

    if digest != """" && !re.MatchString(digest) {
        return """", ErrInvalidDigestFormat
    }

    digest = strings.ReplaceAll(digest, "":"", ""-"")
    path := filepath.Join(envconfig.Models(), ""blobs"", digest)
    dirPath := filepath.Dir(path)
    if digest == """" {
        dirPath = path
    }

    if err := os.MkdirAll(dirPath, 0o755); err != nil {
        return """", fmt.Errorf(""%w: ensure path elements are traversable"", err)
    }

    return path, nil
}
```

### Issue

When `digest == """"`, the function does not reject the input.
 Instead, it:

1. Treats the empty digest as valid,
2. Computes the blob directory path,
3. Returns the full absolute model path on the server.

This leads to path disclosure when an upstream function attempts to open this “file,” producing an OS-level error message containing the full directory path.

### **Recommendation**

Add an explicit check for empty digests at the beginning of `GetBlobsPath`:

```go
if digest == """" {
    return """", fmt.Errorf(""digest must not be empty"")
}
```

This prevents the system from returning internal paths and avoids exposing sensitive information to remote clients.

### Relevant log output

```shell

```

### OS

Linux

### GPU

Nvidia

### CPU

Intel

### Ollama version

0.12.11",,"{'url': 'https://api.github.com/repos/ollama/ollama/issues/13223/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}",https://api.github.com/repos/ollama/ollama/issues/13223/timeline,,,,
https://api.github.com/repos/ollama/ollama/issues/13222,https://api.github.com/repos/ollama/ollama,https://api.github.com/repos/ollama/ollama/issues/13222/labels{/name},https://api.github.com/repos/ollama/ollama/issues/13222/comments,https://api.github.com/repos/ollama/ollama/issues/13222/events,https://github.com/ollama/ollama/pull/13222,3656859636,PR_kwDOJ0Z1Ps61F8NK,13222,Add Atlas UI to community integrations,"{'login': 'garland3', 'id': 1162675, 'node_id': 'MDQ6VXNlcjExNjI2NzU=', 'avatar_url': 'https://avatars.githubusercontent.com/u/1162675?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/garland3', 'html_url': 'https://github.com/garland3', 'followers_url': 'https://api.github.com/users/garland3/followers', 'following_url': 'https://api.github.com/users/garland3/following{/other_user}', 'gists_url': 'https://api.github.com/users/garland3/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/garland3/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/garland3/subscriptions', 'organizations_url': 'https://api.github.com/users/garland3/orgs', 'repos_url': 'https://api.github.com/users/garland3/repos', 'events_url': 'https://api.github.com/users/garland3/events{/privacy}', 'received_events_url': 'https://api.github.com/users/garland3/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}",[],open,False,,[],,0,2025-11-24T02:38:41Z,2025-11-24T02:39:58Z,,NONE,,,,,"This PR adds Atlas UI to the Web & Desktop section of the Community Integrations.

Atlas UI is a chat interface for US Government use, developed by Sandia National Labs, a DOE national laboratory.

GitHub repository: https://github.com/sandialabs/atlas-ui-3",,"{'url': 'https://api.github.com/repos/ollama/ollama/issues/13222/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}",https://api.github.com/repos/ollama/ollama/issues/13222/timeline,,,False,"{'url': 'https://api.github.com/repos/ollama/ollama/pulls/13222', 'html_url': 'https://github.com/ollama/ollama/pull/13222', 'diff_url': 'https://github.com/ollama/ollama/pull/13222.diff', 'patch_url': 'https://github.com/ollama/ollama/pull/13222.patch', 'merged_at': None}"
https://api.github.com/repos/ollama/ollama/issues/13221,https://api.github.com/repos/ollama/ollama,https://api.github.com/repos/ollama/ollama/issues/13221/labels{/name},https://api.github.com/repos/ollama/ollama/issues/13221/comments,https://api.github.com/repos/ollama/ollama/issues/13221/events,https://github.com/ollama/ollama/issues/13221,3656623516,I_kwDOJ0Z1Ps7Z86Wc,13221,🦙 Stable build Ollama with latest bugs fixed🦙,"{'login': 'ciotdvelvet9', 'id': 233345082, 'node_id': 'U_kgDODeiQOg', 'avatar_url': 'https://avatars.githubusercontent.com/u/233345082?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/ciotdvelvet9', 'html_url': 'https://github.com/ciotdvelvet9', 'followers_url': 'https://api.github.com/users/ciotdvelvet9/followers', 'following_url': 'https://api.github.com/users/ciotdvelvet9/following{/other_user}', 'gists_url': 'https://api.github.com/users/ciotdvelvet9/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/ciotdvelvet9/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/ciotdvelvet9/subscriptions', 'organizations_url': 'https://api.github.com/users/ciotdvelvet9/orgs', 'repos_url': 'https://api.github.com/users/ciotdvelvet9/repos', 'events_url': 'https://api.github.com/users/ciotdvelvet9/events{/privacy}', 'received_events_url': 'https://api.github.com/users/ciotdvelvet9/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}",[],closed,False,,[],,0,2025-11-24T00:02:01Z,2025-11-24T00:32:55Z,2025-11-24T00:22:54Z,NONE,,,"{'total': 0, 'completed': 0, 'percent_completed': 0}","{'blocked_by': 0, 'total_blocked_by': 0, 'blocking': 0, 'total_blocking': 0}","We're a small team of enthusiasts who simply enjoy writing code. That’s why we jumped in to help fix bugs in Ollama and make things a bit better for everyone.

If you find our work useful, dropping a ⭐ on the repo would mean a lot to us. Thanks for the support!

https://github.com/ollama-stable-build/ollama-stable-build

> Open source — built by enthusiasts.","{'login': 'rick-github', 'id': 14946854, 'node_id': 'MDQ6VXNlcjE0OTQ2ODU0', 'avatar_url': 'https://avatars.githubusercontent.com/u/14946854?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/rick-github', 'html_url': 'https://github.com/rick-github', 'followers_url': 'https://api.github.com/users/rick-github/followers', 'following_url': 'https://api.github.com/users/rick-github/following{/other_user}', 'gists_url': 'https://api.github.com/users/rick-github/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/rick-github/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/rick-github/subscriptions', 'organizations_url': 'https://api.github.com/users/rick-github/orgs', 'repos_url': 'https://api.github.com/users/rick-github/repos', 'events_url': 'https://api.github.com/users/rick-github/events{/privacy}', 'received_events_url': 'https://api.github.com/users/rick-github/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}","{'url': 'https://api.github.com/repos/ollama/ollama/issues/13221/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}",https://api.github.com/repos/ollama/ollama/issues/13221/timeline,,duplicate,,
https://api.github.com/repos/ollama/ollama/issues/13220,https://api.github.com/repos/ollama/ollama,https://api.github.com/repos/ollama/ollama/issues/13220/labels{/name},https://api.github.com/repos/ollama/ollama/issues/13220/comments,https://api.github.com/repos/ollama/ollama/issues/13220/events,https://github.com/ollama/ollama/pull/13220,3656428764,PR_kwDOJ0Z1Ps61Ejwj,13220,docs: fix link to modelfile.mdx,"{'login': 'familom', 'id': 946303, 'node_id': 'MDQ6VXNlcjk0NjMwMw==', 'avatar_url': 'https://avatars.githubusercontent.com/u/946303?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/familom', 'html_url': 'https://github.com/familom', 'followers_url': 'https://api.github.com/users/familom/followers', 'following_url': 'https://api.github.com/users/familom/following{/other_user}', 'gists_url': 'https://api.github.com/users/familom/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/familom/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/familom/subscriptions', 'organizations_url': 'https://api.github.com/users/familom/orgs', 'repos_url': 'https://api.github.com/users/familom/repos', 'events_url': 'https://api.github.com/users/familom/events{/privacy}', 'received_events_url': 'https://api.github.com/users/familom/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}",[],open,False,,[],,0,2025-11-23T20:31:06Z,2025-11-23T20:31:06Z,,NONE,,,,,"# Description

Following https://github.com/ollama/ollama/pull/12804 some of the links are still pointing to the old location. This PR fixes those links.",,"{'url': 'https://api.github.com/repos/ollama/ollama/issues/13220/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}",https://api.github.com/repos/ollama/ollama/issues/13220/timeline,,,False,"{'url': 'https://api.github.com/repos/ollama/ollama/pulls/13220', 'html_url': 'https://github.com/ollama/ollama/pull/13220', 'diff_url': 'https://github.com/ollama/ollama/pull/13220.diff', 'patch_url': 'https://github.com/ollama/ollama/pull/13220.patch', 'merged_at': None}"
https://api.github.com/repos/ollama/ollama/issues/13219,https://api.github.com/repos/ollama/ollama,https://api.github.com/repos/ollama/ollama/issues/13219/labels{/name},https://api.github.com/repos/ollama/ollama/issues/13219/comments,https://api.github.com/repos/ollama/ollama/issues/13219/events,https://github.com/ollama/ollama/issues/13219,3656272686,I_kwDOJ0Z1Ps7Z7ksu,13219,Request for 2 vision/document layout models,"{'login': 'CanadianHusky', 'id': 35332003, 'node_id': 'MDQ6VXNlcjM1MzMyMDAz', 'avatar_url': 'https://avatars.githubusercontent.com/u/35332003?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/CanadianHusky', 'html_url': 'https://github.com/CanadianHusky', 'followers_url': 'https://api.github.com/users/CanadianHusky/followers', 'following_url': 'https://api.github.com/users/CanadianHusky/following{/other_user}', 'gists_url': 'https://api.github.com/users/CanadianHusky/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/CanadianHusky/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/CanadianHusky/subscriptions', 'organizations_url': 'https://api.github.com/users/CanadianHusky/orgs', 'repos_url': 'https://api.github.com/users/CanadianHusky/repos', 'events_url': 'https://api.github.com/users/CanadianHusky/events{/privacy}', 'received_events_url': 'https://api.github.com/users/CanadianHusky/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}",[],open,False,,[],,1,2025-11-23T17:38:35Z,2025-11-23T18:54:34Z,,NONE,,,"{'total': 0, 'completed': 0, 'percent_completed': 0}","{'blocked_by': 0, 'total_blocked_by': 0, 'blocking': 0, 'total_blocking': 0}","Ollama and a few models that I tested worked 'out of the box' and brilliantly well for me. Awesome work by the Team. Congratulations!
In the future, would it to possible to incorporate these 2 vision/document layout models into the list of available models please. 

1-
https://www.paddleocr.ai/latest/en/version3.x/pipeline_usage/PaddleOCR-VL.html
Here the HF Link with live demo
https://huggingface.co/PaddlePaddle/PaddleOCR-VL
It is a relatively small model (0.9B) compared to the other offerings but it does a great job for Document analysis

2-
https://huggingface.co/rednote-hilab/dots.ocr
This is a very good OCR/Layout model as well that can restore human reading order for complex multicolumn layout

These models may not work for 'agentic' use and may not respond with human readable text, which is totally fine and not required.
Getting back the models raw data (json output) is totally ok as the output will be parsed by code.

Thank you",,"{'url': 'https://api.github.com/repos/ollama/ollama/issues/13219/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}",https://api.github.com/repos/ollama/ollama/issues/13219/timeline,,,,
https://api.github.com/repos/ollama/ollama/issues/13218,https://api.github.com/repos/ollama/ollama,https://api.github.com/repos/ollama/ollama/issues/13218/labels{/name},https://api.github.com/repos/ollama/ollama/issues/13218/comments,https://api.github.com/repos/ollama/ollama/issues/13218/events,https://github.com/ollama/ollama/issues/13218,3656227132,I_kwDOJ0Z1Ps7Z7Zk8,13218,Pulling heretic gpt-oss is broken,"{'login': 'walnutseal1', 'id': 123715597, 'node_id': 'U_kgDOB1_ADQ', 'avatar_url': 'https://avatars.githubusercontent.com/u/123715597?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/walnutseal1', 'html_url': 'https://github.com/walnutseal1', 'followers_url': 'https://api.github.com/users/walnutseal1/followers', 'following_url': 'https://api.github.com/users/walnutseal1/following{/other_user}', 'gists_url': 'https://api.github.com/users/walnutseal1/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/walnutseal1/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/walnutseal1/subscriptions', 'organizations_url': 'https://api.github.com/users/walnutseal1/orgs', 'repos_url': 'https://api.github.com/users/walnutseal1/repos', 'events_url': 'https://api.github.com/users/walnutseal1/events{/privacy}', 'received_events_url': 'https://api.github.com/users/walnutseal1/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}","[{'id': 5667396184, 'node_id': 'LA_kwDOJ0Z1Ps8AAAABUc2aWA', 'url': 'https://api.github.com/repos/ollama/ollama/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': ""Something isn't working""}]",closed,False,,[],,10,2025-11-23T16:54:36Z,2025-11-23T21:08:18Z,2025-11-23T21:08:03Z,NONE,,,"{'total': 0, 'completed': 0, 'percent_completed': 0}","{'blocked_by': 0, 'total_blocked_by': 0, 'blocking': 0, 'total_blocking': 0}","### What is the issue?

pulling mxfp4 of https://huggingface.co/mradermacher/gpt-oss-20b-heretic-GGUF/ causes it to download, but the downloded model produces a blank output.

### Relevant log output

```shell

```

### OS

Windows

### GPU

NVIDIA 5060TI, 16gb vram.

### CPU

AMD Ryzen 5 9600X 6-core processor. 64gb ram

### Ollama version

0.12.10","{'login': 'walnutseal1', 'id': 123715597, 'node_id': 'U_kgDOB1_ADQ', 'avatar_url': 'https://avatars.githubusercontent.com/u/123715597?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/walnutseal1', 'html_url': 'https://github.com/walnutseal1', 'followers_url': 'https://api.github.com/users/walnutseal1/followers', 'following_url': 'https://api.github.com/users/walnutseal1/following{/other_user}', 'gists_url': 'https://api.github.com/users/walnutseal1/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/walnutseal1/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/walnutseal1/subscriptions', 'organizations_url': 'https://api.github.com/users/walnutseal1/orgs', 'repos_url': 'https://api.github.com/users/walnutseal1/repos', 'events_url': 'https://api.github.com/users/walnutseal1/events{/privacy}', 'received_events_url': 'https://api.github.com/users/walnutseal1/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}","{'url': 'https://api.github.com/repos/ollama/ollama/issues/13218/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}",https://api.github.com/repos/ollama/ollama/issues/13218/timeline,,completed,,
https://api.github.com/repos/ollama/ollama/issues/13217,https://api.github.com/repos/ollama/ollama,https://api.github.com/repos/ollama/ollama/issues/13217/labels{/name},https://api.github.com/repos/ollama/ollama/issues/13217/comments,https://api.github.com/repos/ollama/ollama/issues/13217/events,https://github.com/ollama/ollama/issues/13217,3656155698,I_kwDOJ0Z1Ps7Z7IIy,13217,Power adjustment during use,"{'login': 'LaoDi-Sama', 'id': 155820936, 'node_id': 'U_kgDOCUmjiA', 'avatar_url': 'https://avatars.githubusercontent.com/u/155820936?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/LaoDi-Sama', 'html_url': 'https://github.com/LaoDi-Sama', 'followers_url': 'https://api.github.com/users/LaoDi-Sama/followers', 'following_url': 'https://api.github.com/users/LaoDi-Sama/following{/other_user}', 'gists_url': 'https://api.github.com/users/LaoDi-Sama/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/LaoDi-Sama/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/LaoDi-Sama/subscriptions', 'organizations_url': 'https://api.github.com/users/LaoDi-Sama/orgs', 'repos_url': 'https://api.github.com/users/LaoDi-Sama/repos', 'events_url': 'https://api.github.com/users/LaoDi-Sama/events{/privacy}', 'received_events_url': 'https://api.github.com/users/LaoDi-Sama/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}",[],open,False,,[],,7,2025-11-23T15:43:32Z,2025-11-23T16:04:52Z,,NONE,,,"{'total': 0, 'completed': 0, 'percent_completed': 0}","{'blocked_by': 0, 'total_blocked_by': 0, 'blocking': 0, 'total_blocking': 0}","I've found that when using the Astr-bot service to call ollama, it only utilizes about 50W of GPU power, which is clearly not the speed I want.

When running the same model directly in the command prompt (cmd), it utilizes about 100W, and the speed is acceptable.

Is this a bug or is there a setting that can be adjusted? If so, please let me know. Thank you.",,"{'url': 'https://api.github.com/repos/ollama/ollama/issues/13217/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}",https://api.github.com/repos/ollama/ollama/issues/13217/timeline,,,,
https://api.github.com/repos/ollama/ollama/issues/13216,https://api.github.com/repos/ollama/ollama,https://api.github.com/repos/ollama/ollama/issues/13216/labels{/name},https://api.github.com/repos/ollama/ollama/issues/13216/comments,https://api.github.com/repos/ollama/ollama/issues/13216/events,https://github.com/ollama/ollama/pull/13216,3656154136,PR_kwDOJ0Z1Ps61DshK,13216,cmd/bench: fix options table in cmd/bench/README.md,"{'login': 'pythongirl325', 'id': 9940914, 'node_id': 'MDQ6VXNlcjk5NDA5MTQ=', 'avatar_url': 'https://avatars.githubusercontent.com/u/9940914?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/pythongirl325', 'html_url': 'https://github.com/pythongirl325', 'followers_url': 'https://api.github.com/users/pythongirl325/followers', 'following_url': 'https://api.github.com/users/pythongirl325/following{/other_user}', 'gists_url': 'https://api.github.com/users/pythongirl325/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/pythongirl325/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/pythongirl325/subscriptions', 'organizations_url': 'https://api.github.com/users/pythongirl325/orgs', 'repos_url': 'https://api.github.com/users/pythongirl325/repos', 'events_url': 'https://api.github.com/users/pythongirl325/events{/privacy}', 'received_events_url': 'https://api.github.com/users/pythongirl325/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}",[],open,False,,[],,0,2025-11-23T15:42:07Z,2025-11-23T15:48:08Z,,NONE,,,,,,,"{'url': 'https://api.github.com/repos/ollama/ollama/issues/13216/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}",https://api.github.com/repos/ollama/ollama/issues/13216/timeline,,,False,"{'url': 'https://api.github.com/repos/ollama/ollama/pulls/13216', 'html_url': 'https://github.com/ollama/ollama/pull/13216', 'diff_url': 'https://github.com/ollama/ollama/pull/13216.diff', 'patch_url': 'https://github.com/ollama/ollama/pull/13216.patch', 'merged_at': None}"
https://api.github.com/repos/ollama/ollama/issues/13215,https://api.github.com/repos/ollama/ollama,https://api.github.com/repos/ollama/ollama/issues/13215/labels{/name},https://api.github.com/repos/ollama/ollama/issues/13215/comments,https://api.github.com/repos/ollama/ollama/issues/13215/events,https://github.com/ollama/ollama/issues/13215,3656152243,I_kwDOJ0Z1Ps7Z7HSz,13215,Gemma3n broken,"{'login': 'retblast', 'id': 233486673, 'node_id': 'U_kgDODeq5UQ', 'avatar_url': 'https://avatars.githubusercontent.com/u/233486673?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/retblast', 'html_url': 'https://github.com/retblast', 'followers_url': 'https://api.github.com/users/retblast/followers', 'following_url': 'https://api.github.com/users/retblast/following{/other_user}', 'gists_url': 'https://api.github.com/users/retblast/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/retblast/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/retblast/subscriptions', 'organizations_url': 'https://api.github.com/users/retblast/orgs', 'repos_url': 'https://api.github.com/users/retblast/repos', 'events_url': 'https://api.github.com/users/retblast/events{/privacy}', 'received_events_url': 'https://api.github.com/users/retblast/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}","[{'id': 5667396184, 'node_id': 'LA_kwDOJ0Z1Ps8AAAABUc2aWA', 'url': 'https://api.github.com/repos/ollama/ollama/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': ""Something isn't working""}, {'id': 9653160222, 'node_id': 'LA_kwDOJ0Z1Ps8AAAACP1-JHg', 'url': 'https://api.github.com/repos/ollama/ollama/labels/vulkan', 'name': 'vulkan', 'color': 'A41E22', 'default': False, 'description': ''}]",open,False,,[],,4,2025-11-23T15:40:21Z,2025-11-24T14:14:33Z,,NONE,,,"{'total': 0, 'completed': 0, 'percent_completed': 0}","{'blocked_by': 0, 'total_blocked_by': 0, 'blocking': 0, 'total_blocking': 0}","### What is the issue?

I try to run unsloth gemma3n-e4b-q4_k_m, worked fine on 0.12.11, broke on 0.13. 
Arch Linux.

### Relevant log output

```shell
/usr/src/debug/ollama/ollama/ml/backend/ggml/ggml/src/ggml-vulkan/ggml-vulkan.cpp:8669: GGML_ASSERT(op_supports_incontiguous || (ggml_is_contiguous(src0) && (src1 == nullptr || ggml_is_contiguous(src1)))) failed
[New LWP 1983514]
[New LWP 1983513]
[New LWP 1983512]
[New LWP 1983505]
[New LWP 1983504]
[New LWP 1983500]
[New LWP 1983499]
[New LWP 1983498]
[New LWP 1983497]
[New LWP 1983496]
[New LWP 1983495]
[New LWP 1983494]
[New LWP 1983476]
[New LWP 1983469]
[New LWP 1983468]
[New LWP 1983467]
[New LWP 1983466]
[New LWP 1983465]
[New LWP 1983464]
[New LWP 1983463]
[New LWP 1983462]
[New LWP 1983461]
[New LWP 1983457]
[New LWP 1983456]
[New LWP 1983455]
[New LWP 1983454]
[New LWP 1983453]
[New LWP 1983452]
[New LWP 1983451]
[New LWP 1983450]
[New LWP 1983449]
[New LWP 1983448]
[New LWP 1983447]
[New LWP 1983446]
[New LWP 1983445]

This GDB supports auto-downloading debuginfo from the following URLs:
  <https://debuginfod.archlinux.org>
Enable debuginfod for this session? (y or [n]) [answered N; input not from terminal]
Debuginfod has been disabled.
To make this setting permanent, add 'set debuginfod enabled off' to .gdbinit.
[Thread debugging using libthread_db enabled]
Using host libthread_db library ""/usr/lib/libthread_db.so.1"".
0x000055dcf7e6e4c3 in ?? ()
#0  0x000055dcf7e6e4c3 in ?? ()
#1  0x000055dcf7e28eb0 in ?? ()
#2  0x000055dcf9c0e170 in ?? ()
#3  0x0000000000000080 in ?? ()
#4  0x0000000000000000 in ?? ()
[Inferior 1 (process 1983444) detached]
SIGABRT: abort
PC=0x7fbceea9890c m=19 sigcode=18446744073709551610
signal arrived during cgo execution

goroutine 1014 gp=0xc000504700 m=19 mp=0xc000601808 [syscall]:
runtime.cgocall(0x55dcf8b76240, 0xc00009ea80)
	/usr/lib/go/src/runtime/cgocall.go:167 +0x4b fp=0xc00009ea58 sp=0xc00009ea20 pc=0x55dcf7e610eb
github.com/ollama/ollama/ml/backend/ggml._Cfunc_ggml_backend_sched_graph_compute_async(0x55dd2bf0c590, 0x7fbc48008500)
	_cgo_gotypes.go:967 +0x4a fp=0xc00009ea80 sp=0xc00009ea58 pc=0x55dcf82992ea
github.com/ollama/ollama/ml/backend/ggml.(*Context).ComputeWithNotify.func2(...)
	/build/ollama/src/ollama/ml/backend/ggml/ggml.go:825
github.com/ollama/ollama/ml/backend/ggml.(*Context).ComputeWithNotify(0xc005d72080, 0xc0001923f0?, {0xc00004d8b0, 0x1, 0x2?})
	/build/ollama/src/ollama/ml/backend/ggml/ggml.go:825 +0x1b2 fp=0xc00009eb58 sp=0xc00009ea80 pc=0x55dcf82a6f12
github.com/ollama/ollama/runner/ollamarunner.(*Server).computeBatch(0xc0006363c0, {0x0, {0x55dcf9341370, 0xc005d72080}, {0x55dcf934b920, 0xc005cbbbf0}, {0xc000489200, 0x21, 0x40}, {{0x55dcf934b920, ...}, ...}, ...})
	/build/ollama/src/ollama/runner/ollamarunner/runner.go:723 +0x894 fp=0xc00009eef0 sp=0xc00009eb58 pc=0x55dcf837e3b4
github.com/ollama/ollama/runner/ollamarunner.(*Server).run.gowrap1()
	/build/ollama/src/ollama/runner/ollamarunner/runner.go:458 +0x58 fp=0xc00009efe0 sp=0xc00009eef0 pc=0x55dcf837bfb8
runtime.goexit({})
	/usr/lib/go/src/runtime/asm_amd64.s:1693 +0x1 fp=0xc00009efe8 sp=0xc00009efe0 pc=0x55dcf7e6c6a1
created by github.com/ollama/ollama/runner/ollamarunner.(*Server).run in goroutine 20
	/build/ollama/src/ollama/runner/ollamarunner/runner.go:458 +0x2cd

goroutine 1 gp=0xc000002380 m=nil [IO wait]:
runtime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)
	/usr/lib/go/src/runtime/proc.go:460 +0xce fp=0xc000e85790 sp=0xc000e85770 pc=0x55dcf7e6456e
runtime.netpollblock(0xc0005977e0?, 0xf7df9546?, 0xdc?)
	/usr/lib/go/src/runtime/netpoll.go:575 +0xf7 fp=0xc000e857c8 sp=0xc000e85790 pc=0x55dcf7e28157
internal/poll.runtime_pollWait(0x7fbcef0ac400, 0x72)
	/usr/lib/go/src/runtime/netpoll.go:351 +0x85 fp=0xc000e857e8 sp=0xc000e857c8 pc=0x55dcf7e63745
internal/poll.(*pollDesc).wait(0xc000706d00?, 0x900000036?, 0x0)
	/usr/lib/go/src/internal/poll/fd_poll_runtime.go:84 +0x27 fp=0xc000e85810 sp=0xc000e857e8 pc=0x55dcf7eec1c7
internal/poll.(*pollDesc).waitRead(...)
	/usr/lib/go/src/internal/poll/fd_poll_runtime.go:89
internal/poll.(*FD).Accept(0xc000706d00)
	/usr/lib/go/src/internal/poll/fd_unix.go:613 +0x28c fp=0xc000e858b8 sp=0xc000e85810 pc=0x55dcf7ef15ec
net.(*netFD).accept(0xc000706d00)
	/usr/lib/go/src/net/fd_unix.go:161 +0x29 fp=0xc000e85970 sp=0xc000e858b8 pc=0x55dcf7f5ba69
net.(*TCPListener).accept(0xc0006269c0)
	/usr/lib/go/src/net/tcpsock_posix.go:159 +0x1b fp=0xc000e859c0 sp=0xc000e85970 pc=0x55dcf7f7119b
net.(*TCPListener).Accept(0xc0006269c0)
	/usr/lib/go/src/net/tcpsock.go:380 +0x30 fp=0xc000e859f0 sp=0xc000e859c0 pc=0x55dcf7f70030
net/http.(*onceCloseListener).Accept(0xc0004ba3f0?)
	<autogenerated>:1 +0x24 fp=0xc000e85a08 sp=0xc000e859f0 pc=0x55dcf81929e4
net/http.(*Server).Serve(0xc00020d600, {0x55dcf9333e80, 0xc0006269c0})
	/usr/lib/go/src/net/http/server.go:3463 +0x30c fp=0xc000e85b38 sp=0xc000e85a08 pc=0x55dcf816a3cc
github.com/ollama/ollama/runner/ollamarunner.Execute({0xc0000360a0, 0x4, 0x4})
	/build/ollama/src/ollama/runner/ollamarunner/runner.go:1434 +0x954 fp=0xc000e85d08 sp=0xc000e85b38 pc=0x55dcf8385074
github.com/ollama/ollama/runner.Execute({0xc000036080?, 0x0?, 0x0?})
	/build/ollama/src/ollama/runner/runner.go:20 +0xc9 fp=0xc000e85d30 sp=0xc000e85d08 pc=0x55dcf8385969
github.com/ollama/ollama/cmd.NewCLI.func2(0xc00020d400?, {0x55dcf8e302cc?, 0x4?, 0x55dcf8e302d0?})
	/build/ollama/src/ollama/cmd/cmd.go:1841 +0x45 fp=0xc000e85d58 sp=0xc000e85d30 pc=0x55dcf8b0d925
github.com/spf13/cobra.(*Command).execute(0xc0004bd508, {0xc00062a820, 0x5, 0x5})
	/build/ollama/src/pkg/mod/github.com/spf13/cobra@v1.7.0/command.go:940 +0x88a fp=0xc000e85e78 sp=0xc000e85d58 pc=0x55dcf7fd522a
github.com/spf13/cobra.(*Command).ExecuteC(0xc0004a6f08)
	/build/ollama/src/pkg/mod/github.com/spf13/cobra@v1.7.0/command.go:1068 +0x398 fp=0xc000e85f30 sp=0xc000e85e78 pc=0x55dcf7fd5a58
github.com/spf13/cobra.(*Command).Execute(...)
	/build/ollama/src/pkg/mod/github.com/spf13/cobra@v1.7.0/command.go:992
github.com/spf13/cobra.(*Command).ExecuteContext(...)
	/build/ollama/src/pkg/mod/github.com/spf13/cobra@v1.7.0/command.go:985
main.main()
	/build/ollama/src/ollama/main.go:12 +0x4d fp=0xc000e85f50 sp=0xc000e85f30 pc=0x55dcf8b0e40d
runtime.main()
	/usr/lib/go/src/runtime/proc.go:285 +0x29d fp=0xc000e85fe0 sp=0xc000e85f50 pc=0x55dcf7e2f9fd
runtime.goexit({})
	/usr/lib/go/src/runtime/asm_amd64.s:1693 +0x1 fp=0xc000e85fe8 sp=0xc000e85fe0 pc=0x55dcf7e6c6a1

goroutine 2 gp=0xc000002e00 m=nil [force gc (idle)]:
runtime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)
	/usr/lib/go/src/runtime/proc.go:460 +0xce fp=0xc00008cfa8 sp=0xc00008cf88 pc=0x55dcf7e6456e
runtime.goparkunlock(...)
	/usr/lib/go/src/runtime/proc.go:466
runtime.forcegchelper()
	/usr/lib/go/src/runtime/proc.go:373 +0xb8 fp=0xc00008cfe0 sp=0xc00008cfa8 pc=0x55dcf7e2fd38
runtime.goexit({})
	/usr/lib/go/src/runtime/asm_amd64.s:1693 +0x1 fp=0xc00008cfe8 sp=0xc00008cfe0 pc=0x55dcf7e6c6a1
created by runtime.init.7 in goroutine 1
	/usr/lib/go/src/runtime/proc.go:361 +0x1a

goroutine 3 gp=0xc000003340 m=nil [GC sweep wait]:
runtime.gopark(0x1?, 0x0?, 0x0?, 0x0?, 0x0?)
	/usr/lib/go/src/runtime/proc.go:460 +0xce fp=0xc00008d780 sp=0xc00008d760 pc=0x55dcf7e6456e
runtime.goparkunlock(...)
	/usr/lib/go/src/runtime/proc.go:466
runtime.bgsweep(0xc000046080)
	/usr/lib/go/src/runtime/mgcsweep.go:323 +0xdf fp=0xc00008d7c8 sp=0xc00008d780 pc=0x55dcf7e19a5f
runtime.gcenable.gowrap1()
	/usr/lib/go/src/runtime/mgc.go:212 +0x25 fp=0xc00008d7e0 sp=0xc00008d7c8 pc=0x55dcf7e0d9e5
runtime.goexit({})
	/usr/lib/go/src/runtime/asm_amd64.s:1693 +0x1 fp=0xc00008d7e8 sp=0xc00008d7e0 pc=0x55dcf7e6c6a1
created by runtime.gcenable in goroutine 1
	/usr/lib/go/src/runtime/mgc.go:212 +0x66

goroutine 4 gp=0xc000003500 m=nil [GC scavenge wait]:
runtime.gopark(0x10000?, 0xd0423?, 0x0?, 0x0?, 0x0?)
	/usr/lib/go/src/runtime/proc.go:460 +0xce fp=0xc00008df78 sp=0xc00008df58 pc=0x55dcf7e6456e
runtime.goparkunlock(...)
	/usr/lib/go/src/runtime/proc.go:466
runtime.(*scavengerState).park(0x55dcf9c0b0a0)
	/usr/lib/go/src/runtime/mgcscavenge.go:425 +0x49 fp=0xc00008dfa8 sp=0xc00008df78 pc=0x55dcf7e174c9
runtime.bgscavenge(0xc000046080)
	/usr/lib/go/src/runtime/mgcscavenge.go:658 +0x59 fp=0xc00008dfc8 sp=0xc00008dfa8 pc=0x55dcf7e17a79
runtime.gcenable.gowrap2()
	/usr/lib/go/src/runtime/mgc.go:213 +0x25 fp=0xc00008dfe0 sp=0xc00008dfc8 pc=0x55dcf7e0d985
runtime.goexit({})
	/usr/lib/go/src/runtime/asm_amd64.s:1693 +0x1 fp=0xc00008dfe8 sp=0xc00008dfe0 pc=0x55dcf7e6c6a1
created by runtime.gcenable in goroutine 1
	/usr/lib/go/src/runtime/mgc.go:213 +0xa5

goroutine 5 gp=0xc000003dc0 m=nil [finalizer wait]:
runtime.gopark(0x55dcf7e3ed37?, 0x55dcf7e05305?, 0xb8?, 0x1?, 0xc000002380?)
	/usr/lib/go/src/runtime/proc.go:460 +0xce fp=0xc00008c620 sp=0xc00008c600 pc=0x55dcf7e6456e
runtime.runFinalizers()
	/usr/lib/go/src/runtime/mfinal.go:210 +0x107 fp=0xc00008c7e0 sp=0xc00008c620 pc=0x55dcf7e0c8e7
runtime.goexit({})
	/usr/lib/go/src/runtime/asm_amd64.s:1693 +0x1 fp=0xc00008c7e8 sp=0xc00008c7e0 pc=0x55dcf7e6c6a1
created by runtime.createfing in goroutine 1
	/usr/lib/go/src/runtime/mfinal.go:172 +0x3d

goroutine 6 gp=0xc0001ea8c0 m=nil [cleanup wait]:
runtime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)
	/usr/lib/go/src/runtime/proc.go:460 +0xce fp=0xc00008e768 sp=0xc00008e748 pc=0x55dcf7e6456e
runtime.goparkunlock(...)
	/usr/lib/go/src/runtime/proc.go:466
runtime.(*cleanupQueue).dequeue(0x55dcf9c0ba00)
	/usr/lib/go/src/runtime/mcleanup.go:439 +0xc5 fp=0xc00008e7a0 sp=0xc00008e768 pc=0x55dcf7e09ac5
runtime.runCleanups()
	/usr/lib/go/src/runtime/mcleanup.go:635 +0x45 fp=0xc00008e7e0 sp=0xc00008e7a0 pc=0x55dcf7e0a185
runtime.goexit({})
	/usr/lib/go/src/runtime/asm_amd64.s:1693 +0x1 fp=0xc00008e7e8 sp=0xc00008e7e0 pc=0x55dcf7e6c6a1
created by runtime.(*cleanupQueue).createGs in goroutine 1
	/usr/lib/go/src/runtime/mcleanup.go:589 +0xa5

goroutine 7 gp=0xc0001eae00 m=nil [GC worker (idle)]:
runtime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)
	/usr/lib/go/src/runtime/proc.go:460 +0xce fp=0xc00008ef38 sp=0xc00008ef18 pc=0x55dcf7e6456e
runtime.gcBgMarkWorker(0xc0000c56c0)
	/usr/lib/go/src/runtime/mgc.go:1463 +0xeb fp=0xc00008efc8 sp=0xc00008ef38 pc=0x55dcf7e1010b
runtime.gcBgMarkStartWorkers.gowrap1()
	/usr/lib/go/src/runtime/mgc.go:1373 +0x25 fp=0xc00008efe0 sp=0xc00008efc8 pc=0x55dcf7e0ffe5
runtime.goexit({})
	/usr/lib/go/src/runtime/asm_amd64.s:1693 +0x1 fp=0xc00008efe8 sp=0xc00008efe0 pc=0x55dcf7e6c6a1
created by runtime.gcBgMarkStartWorkers in goroutine 1
	/usr/lib/go/src/runtime/mgc.go:1373 +0x105

goroutine 8 gp=0xc0001eafc0 m=nil [GC worker (idle)]:
runtime.gopark(0x1a197309cb4f?, 0x0?, 0x0?, 0x0?, 0x0?)
	/usr/lib/go/src/runtime/proc.go:460 +0xce fp=0xc00008f738 sp=0xc00008f718 pc=0x55dcf7e6456e
runtime.gcBgMarkWorker(0xc0000c56c0)
	/usr/lib/go/src/runtime/mgc.go:1463 +0xeb fp=0xc00008f7c8 sp=0xc00008f738 pc=0x55dcf7e1010b
runtime.gcBgMarkStartWorkers.gowrap1()
	/usr/lib/go/src/runtime/mgc.go:1373 +0x25 fp=0xc00008f7e0 sp=0xc00008f7c8 pc=0x55dcf7e0ffe5
runtime.goexit({})
	/usr/lib/go/src/runtime/asm_amd64.s:1693 +0x1 fp=0xc00008f7e8 sp=0xc00008f7e0 pc=0x55dcf7e6c6a1
created by runtime.gcBgMarkStartWorkers in goroutine 1
	/usr/lib/go/src/runtime/mgc.go:1373 +0x105

goroutine 9 gp=0xc0001eb180 m=nil [GC worker (idle)]:
runtime.gopark(0x1a19730a37cf?, 0x0?, 0x0?, 0x0?, 0x0?)
	/usr/lib/go/src/runtime/proc.go:460 +0xce fp=0xc00008ff38 sp=0xc00008ff18 pc=0x55dcf7e6456e
runtime.gcBgMarkWorker(0xc0000c56c0)
	/usr/lib/go/src/runtime/mgc.go:1463 +0xeb fp=0xc00008ffc8 sp=0xc00008ff38 pc=0x55dcf7e1010b
runtime.gcBgMarkStartWorkers.gowrap1()
	/usr/lib/go/src/runtime/mgc.go:1373 +0x25 fp=0xc00008ffe0 sp=0xc00008ffc8 pc=0x55dcf7e0ffe5
runtime.goexit({})
	/usr/lib/go/src/runtime/asm_amd64.s:1693 +0x1 fp=0xc00008ffe8 sp=0xc00008ffe0 pc=0x55dcf7e6c6a1
created by runtime.gcBgMarkStartWorkers in goroutine 1
	/usr/lib/go/src/runtime/mgc.go:1373 +0x105

goroutine 10 gp=0xc0001eb340 m=nil [GC worker (idle)]:
runtime.gopark(0x1a19730a7327?, 0x0?, 0x0?, 0x0?, 0x0?)
	/usr/lib/go/src/runtime/proc.go:460 +0xce fp=0xc000088738 sp=0xc000088718 pc=0x55dcf7e6456e
runtime.gcBgMarkWorker(0xc0000c56c0)
	/usr/lib/go/src/runtime/mgc.go:1463 +0xeb fp=0xc0000887c8 sp=0xc000088738 pc=0x55dcf7e1010b
runtime.gcBgMarkStartWorkers.gowrap1()
	/usr/lib/go/src/runtime/mgc.go:1373 +0x25 fp=0xc0000887e0 sp=0xc0000887c8 pc=0x55dcf7e0ffe5
runtime.goexit({})
	/usr/lib/go/src/runtime/asm_amd64.s:1693 +0x1 fp=0xc0000887e8 sp=0xc0000887e0 pc=0x55dcf7e6c6a1
created by runtime.gcBgMarkStartWorkers in goroutine 1
	/usr/lib/go/src/runtime/mgc.go:1373 +0x105

goroutine 11 gp=0xc0001eb500 m=nil [GC worker (idle)]:
runtime.gopark(0x1a19731945fc?, 0x3?, 0x5b?, 0x85?, 0x0?)
	/usr/lib/go/src/runtime/proc.go:460 +0xce fp=0xc000088f38 sp=0xc000088f18 pc=0x55dcf7e6456e
runtime.gcBgMarkWorker(0xc0000c56c0)
	/usr/lib/go/src/runtime/mgc.go:1463 +0xeb fp=0xc000088fc8 sp=0xc000088f38 pc=0x55dcf7e1010b
runtime.gcBgMarkStartWorkers.gowrap1()
	/usr/lib/go/src/runtime/mgc.go:1373 +0x25 fp=0xc000088fe0 sp=0xc000088fc8 pc=0x55dcf7e0ffe5
runtime.goexit({})
	/usr/lib/go/src/runtime/asm_amd64.s:1693 +0x1 fp=0xc000088fe8 sp=0xc000088fe0 pc=0x55dcf7e6c6a1
created by runtime.gcBgMarkStartWorkers in goroutine 1
	/usr/lib/go/src/runtime/mgc.go:1373 +0x105

goroutine 12 gp=0xc0001eb6c0 m=nil [GC worker (idle)]:
runtime.gopark(0x1a197309d213?, 0x1?, 0xef?, 0xbb?, 0x0?)
	/usr/lib/go/src/runtime/proc.go:460 +0xce fp=0xc000089738 sp=0xc000089718 pc=0x55dcf7e6456e
runtime.gcBgMarkWorker(0xc0000c56c0)
	/usr/lib/go/src/runtime/mgc.go:1463 +0xeb fp=0xc0000897c8 sp=0xc000089738 pc=0x55dcf7e1010b
runtime.gcBgMarkStartWorkers.gowrap1()
	/usr/lib/go/src/runtime/mgc.go:1373 +0x25 fp=0xc0000897e0 sp=0xc0000897c8 pc=0x55dcf7e0ffe5
runtime.goexit({})
	/usr/lib/go/src/runtime/asm_amd64.s:1693 +0x1 fp=0xc0000897e8 sp=0xc0000897e0 pc=0x55dcf7e6c6a1
created by runtime.gcBgMarkStartWorkers in goroutine 1
	/usr/lib/go/src/runtime/mgc.go:1373 +0x105

goroutine 13 gp=0xc0001eb880 m=nil [GC worker (idle)]:
runtime.gopark(0x1a197309fed2?, 0x3?, 0x73?, 0xea?, 0x0?)
	/usr/lib/go/src/runtime/proc.go:460 +0xce fp=0xc000089f38 sp=0xc000089f18 pc=0x55dcf7e6456e
runtime.gcBgMarkWorker(0xc0000c56c0)
	/usr/lib/go/src/runtime/mgc.go:1463 +0xeb fp=0xc000089fc8 sp=0xc000089f38 pc=0x55dcf7e1010b
runtime.gcBgMarkStartWorkers.gowrap1()
	/usr/lib/go/src/runtime/mgc.go:1373 +0x25 fp=0xc000089fe0 sp=0xc000089fc8 pc=0x55dcf7e0ffe5
runtime.goexit({})
	/usr/lib/go/src/runtime/asm_amd64.s:1693 +0x1 fp=0xc000089fe8 sp=0xc000089fe0 pc=0x55dcf7e6c6a1
created by runtime.gcBgMarkStartWorkers in goroutine 1
	/usr/lib/go/src/runtime/mgc.go:1373 +0x105

goroutine 14 gp=0xc0001eba40 m=nil [GC worker (idle)]:
runtime.gopark(0x1a19730a2a31?, 0x1?, 0x11?, 0x8c?, 0x0?)
	/usr/lib/go/src/runtime/proc.go:460 +0xce fp=0xc00008a738 sp=0xc00008a718 pc=0x55dcf7e6456e
runtime.gcBgMarkWorker(0xc0000c56c0)
	/usr/lib/go/src/runtime/mgc.go:1463 +0xeb fp=0xc00008a7c8 sp=0xc00008a738 pc=0x55dcf7e1010b
runtime.gcBgMarkStartWorkers.gowrap1()
	/usr/lib/go/src/runtime/mgc.go:1373 +0x25 fp=0xc00008a7e0 sp=0xc00008a7c8 pc=0x55dcf7e0ffe5
runtime.goexit({})
	/usr/lib/go/src/runtime/asm_amd64.s:1693 +0x1 fp=0xc00008a7e8 sp=0xc00008a7e0 pc=0x55dcf7e6c6a1
created by runtime.gcBgMarkStartWorkers in goroutine 1
	/usr/lib/go/src/runtime/mgc.go:1373 +0x105

goroutine 15 gp=0xc0001ebc00 m=nil [GC worker (idle)]:
runtime.gopark(0x1a19731c972a?, 0x1?, 0x59?, 0xcc?, 0x0?)
	/usr/lib/go/src/runtime/proc.go:460 +0xce fp=0xc000116f38 sp=0xc000116f18 pc=0x55dcf7e6456e
runtime.gcBgMarkWorker(0xc0000c56c0)
	/usr/lib/go/src/runtime/mgc.go:1463 +0xeb fp=0xc000116fc8 sp=0xc000116f38 pc=0x55dcf7e1010b
runtime.gcBgMarkStartWorkers.gowrap1()
	/usr/lib/go/src/runtime/mgc.go:1373 +0x25 fp=0xc000116fe0 sp=0xc000116fc8 pc=0x55dcf7e0ffe5
runtime.goexit({})
	/usr/lib/go/src/runtime/asm_amd64.s:1693 +0x1 fp=0xc000116fe8 sp=0xc000116fe0 pc=0x55dcf7e6c6a1
created by runtime.gcBgMarkStartWorkers in goroutine 1
	/usr/lib/go/src/runtime/mgc.go:1373 +0x105

goroutine 16 gp=0xc0001ebdc0 m=nil [GC worker (idle)]:
runtime.gopark(0x1a19730aa1c4?, 0x0?, 0x0?, 0x0?, 0x0?)
	/usr/lib/go/src/runtime/proc.go:460 +0xce fp=0xc00008b738 sp=0xc00008b718 pc=0x55dcf7e6456e
runtime.gcBgMarkWorker(0xc0000c56c0)
	/usr/lib/go/src/runtime/mgc.go:1463 +0xeb fp=0xc00008b7c8 sp=0xc00008b738 pc=0x55dcf7e1010b
runtime.gcBgMarkStartWorkers.gowrap1()
	/usr/lib/go/src/runtime/mgc.go:1373 +0x25 fp=0xc00008b7e0 sp=0xc00008b7c8 pc=0x55dcf7e0ffe5
runtime.goexit({})
	/usr/lib/go/src/runtime/asm_amd64.s:1693 +0x1 fp=0xc00008b7e8 sp=0xc00008b7e0 pc=0x55dcf7e6c6a1
created by runtime.gcBgMarkStartWorkers in goroutine 1
	/usr/lib/go/src/runtime/mgc.go:1373 +0x105

goroutine 18 gp=0xc0004aa000 m=nil [GC worker (idle)]:
runtime.gopark(0x1a1973096f4f?, 0x1?, 0x46?, 0xa5?, 0x0?)
	/usr/lib/go/src/runtime/proc.go:460 +0xce fp=0xc000f4d738 sp=0xc000f4d718 pc=0x55dcf7e6456e
runtime.gcBgMarkWorker(0xc0000c56c0)
	/usr/lib/go/src/runtime/mgc.go:1463 +0xeb fp=0xc000f4d7c8 sp=0xc000f4d738 pc=0x55dcf7e1010b
runtime.gcBgMarkStartWorkers.gowrap1()
	/usr/lib/go/src/runtime/mgc.go:1373 +0x25 fp=0xc000f4d7e0 sp=0xc000f4d7c8 pc=0x55dcf7e0ffe5
runtime.goexit({})
	/usr/lib/go/src/runtime/asm_amd64.s:1693 +0x1 fp=0xc000f4d7e8 sp=0xc000f4d7e0 pc=0x55dcf7e6c6a1
created by runtime.gcBgMarkStartWorkers in goroutine 1
	/usr/lib/go/src/runtime/mgc.go:1373 +0x105

goroutine 19 gp=0xc0004aa1c0 m=nil [GC worker (idle)]:
runtime.gopark(0x55dcf9cbb180?, 0x1?, 0xfe?, 0xba?, 0x0?)
	/usr/lib/go/src/runtime/proc.go:460 +0xce fp=0xc0004b0738 sp=0xc0004b0718 pc=0x55dcf7e6456e
runtime.gcBgMarkWorker(0xc0000c56c0)
	/usr/lib/go/src/runtime/mgc.go:1463 +0xeb fp=0xc0004b07c8 sp=0xc0004b0738 pc=0x55dcf7e1010b
runtime.gcBgMarkStartWorkers.gowrap1()
	/usr/lib/go/src/runtime/mgc.go:1373 +0x25 fp=0xc0004b07e0 sp=0xc0004b07c8 pc=0x55dcf7e0ffe5
runtime.goexit({})
	/usr/lib/go/src/runtime/asm_amd64.s:1693 +0x1 fp=0xc0004b07e8 sp=0xc0004b07e0 pc=0x55dcf7e6c6a1
created by runtime.gcBgMarkStartWorkers in goroutine 1
	/usr/lib/go/src/runtime/mgc.go:1373 +0x105

goroutine 34 gp=0xc000504000 m=nil [GC worker (idle)]:
runtime.gopark(0x1a19730a97cb?, 0x3?, 0x67?, 0xb0?, 0x0?)
	/usr/lib/go/src/runtime/proc.go:460 +0xce fp=0xc00011a738 sp=0xc00011a718 pc=0x55dcf7e6456e
runtime.gcBgMarkWorker(0xc0000c56c0)
	/usr/lib/go/src/runtime/mgc.go:1463 +0xeb fp=0xc00011a7c8 sp=0xc00011a738 pc=0x55dcf7e1010b
runtime.gcBgMarkStartWorkers.gowrap1()
	/usr/lib/go/src/runtime/mgc.go:1373 +0x25 fp=0xc00011a7e0 sp=0xc00011a7c8 pc=0x55dcf7e0ffe5
runtime.goexit({})
	/usr/lib/go/src/runtime/asm_amd64.s:1693 +0x1 fp=0xc00011a7e8 sp=0xc00011a7e0 pc=0x55dcf7e6c6a1
created by runtime.gcBgMarkStartWorkers in goroutine 1
	/usr/lib/go/src/runtime/mgc.go:1373 +0x105

goroutine 50 gp=0xc000102380 m=nil [GC worker (idle)]:
runtime.gopark(0x1a19730b0a98?, 0x1?, 0x9c?, 0x90?, 0x0?)
	/usr/lib/go/src/runtime/proc.go:460 +0xce fp=0xc0004b1f38 sp=0xc0004b1f18 pc=0x55dcf7e6456e
runtime.gcBgMarkWorker(0xc0000c56c0)
	/usr/lib/go/src/runtime/mgc.go:1463 +0xeb fp=0xc0004b1fc8 sp=0xc0004b1f38 pc=0x55dcf7e1010b
runtime.gcBgMarkStartWorkers.gowrap1()
	/usr/lib/go/src/runtime/mgc.go:1373 +0x25 fp=0xc0004b1fe0 sp=0xc0004b1fc8 pc=0x55dcf7e0ffe5
runtime.goexit({})
	/usr/lib/go/src/runtime/asm_amd64.s:1693 +0x1 fp=0xc0004b1fe8 sp=0xc0004b1fe0 pc=0x55dcf7e6c6a1
created by runtime.gcBgMarkStartWorkers in goroutine 1
	/usr/lib/go/src/runtime/mgc.go:1373 +0x105

goroutine 51 gp=0xc000102540 m=nil [GC worker (idle)]:
runtime.gopark(0x1a19730a9d49?, 0x3?, 0xe7?, 0x40?, 0x0?)
	/usr/lib/go/src/runtime/proc.go:460 +0xce fp=0xc000118f38 sp=0xc000118f18 pc=0x55dcf7e6456e
runtime.gcBgMarkWorker(0xc0000c56c0)
	/usr/lib/go/src/runtime/mgc.go:1463 +0xeb fp=0xc000118fc8 sp=0xc000118f38 pc=0x55dcf7e1010b
runtime.gcBgMarkStartWorkers.gowrap1()
	/usr/lib/go/src/runtime/mgc.go:1373 +0x25 fp=0xc000118fe0 sp=0xc000118fc8 pc=0x55dcf7e0ffe5
runtime.goexit({})
	/usr/lib/go/src/runtime/asm_amd64.s:1693 +0x1 fp=0xc000118fe8 sp=0xc000118fe0 pc=0x55dcf7e6c6a1
created by runtime.gcBgMarkStartWorkers in goroutine 1
	/usr/lib/go/src/runtime/mgc.go:1373 +0x105

goroutine 52 gp=0xc000102700 m=nil [GC worker (idle)]:
runtime.gopark(0x1a1973208ad2?, 0x3?, 0xcc?, 0x8?, 0x0?)
	/usr/lib/go/src/runtime/proc.go:460 +0xce fp=0xc0003ebf38 sp=0xc0003ebf18 pc=0x55dcf7e6456e
runtime.gcBgMarkWorker(0xc0000c56c0)
	/usr/lib/go/src/runtime/mgc.go:1463 +0xeb fp=0xc0003ebfc8 sp=0xc0003ebf38 pc=0x55dcf7e1010b
runtime.gcBgMarkStartWorkers.gowrap1()
	/usr/lib/go/src/runtime/mgc.go:1373 +0x25 fp=0xc0003ebfe0 sp=0xc0003ebfc8 pc=0x55dcf7e0ffe5
runtime.goexit({})
	/usr/lib/go/src/runtime/asm_amd64.s:1693 +0x1 fp=0xc0003ebfe8 sp=0xc0003ebfe0 pc=0x55dcf7e6c6a1
created by runtime.gcBgMarkStartWorkers in goroutine 1
	/usr/lib/go/src/runtime/mgc.go:1373 +0x105

goroutine 20 gp=0xc000103180 m=nil [chan receive]:
runtime.gopark(0x30?, 0x55dcf9291f00?, 0x1?, 0x0?, 0xc000e81798?)
	/usr/lib/go/src/runtime/proc.go:460 +0xce fp=0xc000e81750 sp=0xc000e81730 pc=0x55dcf7e6456e
runtime.chanrecv(0xc000612000, 0x0, 0x1)
	/usr/lib/go/src/runtime/chan.go:667 +0x473 fp=0xc000e817c8 sp=0xc000e81750 pc=0x55dcf7dfc473
runtime.chanrecv1(0x55dcf8e6a1c2?, 0x29?)
	/usr/lib/go/src/runtime/chan.go:509 +0x12 fp=0xc000e817f0 sp=0xc000e817c8 pc=0x55dcf7dfbfd2
github.com/ollama/ollama/runner/ollamarunner.(*Server).forwardBatch(_, {0x1, {0x55dcf9341370, 0xc000f48000}, {0x55dcf934b920, 0xc005d246c0}, {0xc000132010, 0x1, 0x1}, {{0x55dcf934b920, ...}, ...}, ...})
	/build/ollama/src/ollama/runner/ollamarunner/runner.go:475 +0xfa fp=0xc000e81b58 sp=0xc000e817f0 pc=0x55dcf837c0da
github.com/ollama/ollama/runner/ollamarunner.(*Server).run(0xc0006363c0, {0x55dcf9336520, 0xc00062a8c0})
	/build/ollama/src/ollama/runner/ollamarunner/runner.go:452 +0x18c fp=0xc000e81fb8 sp=0xc000e81b58 pc=0x55dcf837bd8c
github.com/ollama/ollama/runner/ollamarunner.Execute.gowrap1()
	/build/ollama/src/ollama/runner/ollamarunner/runner.go:1411 +0x28 fp=0xc000e81fe0 sp=0xc000e81fb8 pc=0x55dcf83852e8
runtime.goexit({})
	/usr/lib/go/src/runtime/asm_amd64.s:1693 +0x1 fp=0xc000e81fe8 sp=0xc000e81fe0 pc=0x55dcf7e6c6a1
created by github.com/ollama/ollama/runner/ollamarunner.Execute in goroutine 1
	/build/ollama/src/ollama/runner/ollamarunner/runner.go:1411 +0x4c9

goroutine 21 gp=0xc000103340 m=nil [select]:
runtime.gopark(0xc000051a00?, 0x2?, 0x1?, 0x0?, 0xc000051864?)
	/usr/lib/go/src/runtime/proc.go:460 +0xce fp=0xc000051688 sp=0xc000051668 pc=0x55dcf7e6456e
runtime.selectgo(0xc000051a00, 0xc000051860, 0x21?, 0x0, 0x1?, 0x1)
	/usr/lib/go/src/runtime/select.go:351 +0x8c5 fp=0xc0000517c8 sp=0xc000051688 pc=0x55dcf7e426a5
github.com/ollama/ollama/runner/ollamarunner.(*Server).completion(0xc0006363c0, {0x55dcf9334060, 0xc00011e690}, 0xc00013af00)
	/build/ollama/src/ollama/runner/ollamarunner/runner.go:950 +0xc45 fp=0xc000051ab8 sp=0xc0000517c8 pc=0x55dcf83804a5
github.com/ollama/ollama/runner/ollamarunner.(*Server).completion-fm({0x55dcf9334060?, 0xc00011e690?}, 0xc000f7bb38?)
	<autogenerated>:1 +0x36 fp=0xc000051ae8 sp=0xc000051ab8 pc=0x55dcf83857d6
net/http.HandlerFunc.ServeHTTP(0xc000145740?, {0x55dcf9334060?, 0xc00011e690?}, 0xc000f7bb58?)
	/usr/lib/go/src/net/http/server.go:2322 +0x29 fp=0xc000051b10 sp=0xc000051ae8 pc=0x55dcf8166a09
net/http.(*ServeMux).ServeHTTP(0x55dcf7e05305?, {0x55dcf9334060, 0xc00011e690}, 0xc00013af00)
	/usr/lib/go/src/net/http/server.go:2861 +0x1c7 fp=0xc000051b60 sp=0xc000051b10 pc=0x55dcf81688e7
net/http.serverHandler.ServeHTTP({0x55dcf9330ad0?}, {0x55dcf9334060?, 0xc00011e690?}, 0x1?)
	/usr/lib/go/src/net/http/server.go:3340 +0x8e fp=0xc000051b90 sp=0xc000051b60 pc=0x55dcf81861ce
net/http.(*conn).serve(0xc0004ba3f0, {0x55dcf93364e8, 0xc000712ff0})
	/usr/lib/go/src/net/http/server.go:2109 +0x665 fp=0xc000051fb8 sp=0xc000051b90 pc=0x55dcf8164b05
net/http.(*Server).Serve.gowrap3()
	/usr/lib/go/src/net/http/server.go:3493 +0x28 fp=0xc000051fe0 sp=0xc000051fb8 pc=0x55dcf816a7c8
runtime.goexit({})
	/usr/lib/go/src/runtime/asm_amd64.s:1693 +0x1 fp=0xc000051fe8 sp=0xc000051fe0 pc=0x55dcf7e6c6a1
created by net/http.(*Server).Serve in goroutine 1
	/usr/lib/go/src/net/http/server.go:3493 +0x485

goroutine 1042 gp=0xc0005041c0 m=nil [chan receive]:
runtime.gopark(0x30?, 0x55dcf9291f00?, 0x1?, 0xfe?, 0xc0003eab00?)
	/usr/lib/go/src/runtime/proc.go:460 +0xce fp=0xc0003eaab8 sp=0xc0003eaa98 pc=0x55dcf7e6456e
runtime.chanrecv(0xc00011c0e0, 0x0, 0x1)
	/usr/lib/go/src/runtime/chan.go:667 +0x473 fp=0xc0003eab30 sp=0xc0003eaab8 pc=0x55dcf7dfc473
runtime.chanrecv1(0x55dcf8e6d6e0?, 0x2c?)
	/usr/lib/go/src/runtime/chan.go:509 +0x12 fp=0xc0003eab58 sp=0xc0003eab30 pc=0x55dcf7dfbfd2
github.com/ollama/ollama/runner/ollamarunner.(*Server).computeBatch(0xc0006363c0, {0x1, {0x55dcf9341370, 0xc000f48000}, {0x55dcf934b920, 0xc005d246c0}, {0xc000132010, 0x1, 0x1}, {{0x55dcf934b920, ...}, ...}, ...})
	/build/ollama/src/ollama/runner/ollamarunner/runner.go:651 +0x185 fp=0xc0003eaef0 sp=0xc0003eab58 pc=0x55dcf837dca5
github.com/ollama/ollama/runner/ollamarunner.(*Server).run.gowrap1()
	/build/ollama/src/ollama/runner/ollamarunner/runner.go:458 +0x58 fp=0xc0003eafe0 sp=0xc0003eaef0 pc=0x55dcf837bfb8
runtime.goexit({})
	/usr/lib/go/src/runtime/asm_amd64.s:1693 +0x1 fp=0xc0003eafe8 sp=0xc0003eafe0 pc=0x55dcf7e6c6a1
created by github.com/ollama/ollama/runner/ollamarunner.(*Server).run in goroutine 20
	/build/ollama/src/ollama/runner/ollamarunner/runner.go:458 +0x2cd

goroutine 1013 gp=0xc000103880 m=nil [IO wait]:
runtime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0xb?)
	/usr/lib/go/src/runtime/proc.go:460 +0xce fp=0xc000114dd8 sp=0xc000114db8 pc=0x55dcf7e6456e
runtime.netpollblock(0x55dcf7e88718?, 0xf7df9546?, 0xdc?)
	/usr/lib/go/src/runtime/netpoll.go:575 +0xf7 fp=0xc000114e10 sp=0xc000114dd8 pc=0x55dcf7e28157
internal/poll.runtime_pollWait(0x7fbcef0ac200, 0x72)
	/usr/lib/go/src/runtime/netpoll.go:351 +0x85 fp=0xc000114e30 sp=0xc000114e10 pc=0x55dcf7e63745
internal/poll.(*pollDesc).wait(0xc000706d80?, 0xc000626a21?, 0x0)
	/usr/lib/go/src/internal/poll/fd_poll_runtime.go:84 +0x27 fp=0xc000114e58 sp=0xc000114e30 pc=0x55dcf7eec1c7
internal/poll.(*pollDesc).waitRead(...)
	/usr/lib/go/src/internal/poll/fd_poll_runtime.go:89
internal/poll.(*FD).Read(0xc000706d80, {0xc000626a21, 0x1, 0x1})
	/usr/lib/go/src/internal/poll/fd_unix.go:165 +0x279 fp=0xc000114ef0 sp=0xc000114e58 pc=0x55dcf7eed4b9
net.(*netFD).Read(0xc000706d80, {0xc000626a21?, 0x55dcf9b4de70?, 0xc000114f70?})
	/usr/lib/go/src/net/fd_posix.go:68 +0x25 fp=0xc000114f38 sp=0xc000114ef0 pc=0x55dcf7f59bc5
net.(*conn).Read(0xc000132548, {0xc000626a21?, 0xc000f48180?, 0x55dcf81d0580?})
	/usr/lib/go/src/net/net.go:196 +0x45 fp=0xc000114f80 sp=0xc000114f38 pc=0x55dcf7f67be5
net/http.(*connReader).backgroundRead(0xc000626a00)
	/usr/lib/go/src/net/http/server.go:702 +0x33 fp=0xc000114fc8 sp=0xc000114f80 pc=0x55dcf815efb3
net/http.(*connReader).startBackgroundRead.gowrap2()
	/usr/lib/go/src/net/http/server.go:698 +0x25 fp=0xc000114fe0 sp=0xc000114fc8 pc=0x55dcf815eee5
runtime.goexit({})
	/usr/lib/go/src/runtime/asm_amd64.s:1693 +0x1 fp=0xc000114fe8 sp=0xc000114fe0 pc=0x55dcf7e6c6a1
created by net/http.(*connReader).startBackgroundRead in goroutine 21
	/usr/lib/go/src/net/http/server.go:698 +0xb6

rax    0x0
rbx    0x1e43eb
rcx    0x7fbceea9890c
rdx    0x6
rdi    0x1e43d4
rsi    0x1e43eb
rbp    0x7fbc53ffcf30
rsp    0x7fbc53ffcef0
r8     0x0
r9     0x0
r10    0x0
r11    0x246
r12    0x7fbc8d1b901e
r13    0x21dd
r14    0x6
r15    0x7fbc48058e00
rip    0x7fbceea9890c
rflags 0x246
cs     0x33
fs     0x0
gs     0x0
time=2025-11-23T10:31:18.409-05:00 level=ERROR source=server.go:1539 msg=""post predict"" error=""Post \""http://127.0.0.1:34755/completion\"": EOF""
time=2025-11-23T10:31:18.409-05:00 level=ERROR source=server.go:265 msg=""llama runner terminated"" error=""exit status 2""
[GIN] 2025/11/23 - 10:31:18 | 500 |  291.981879ms |       127.0.0.1 | POST     ""/api/chat""
```

### OS

Linux

### GPU

Intel

### CPU

Intel

### Ollama version

0.13.0",,"{'url': 'https://api.github.com/repos/ollama/ollama/issues/13215/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}",https://api.github.com/repos/ollama/ollama/issues/13215/timeline,,,,
https://api.github.com/repos/ollama/ollama/issues/13214,https://api.github.com/repos/ollama/ollama,https://api.github.com/repos/ollama/ollama/issues/13214/labels{/name},https://api.github.com/repos/ollama/ollama/issues/13214/comments,https://api.github.com/repos/ollama/ollama/issues/13214/events,https://github.com/ollama/ollama/issues/13214,3656063664,I_kwDOJ0Z1Ps7Z6xqw,13214,m4 mac显示有问题,"{'login': 'adiudiuu', 'id': 31238774, 'node_id': 'MDQ6VXNlcjMxMjM4Nzc0', 'avatar_url': 'https://avatars.githubusercontent.com/u/31238774?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/adiudiuu', 'html_url': 'https://github.com/adiudiuu', 'followers_url': 'https://api.github.com/users/adiudiuu/followers', 'following_url': 'https://api.github.com/users/adiudiuu/following{/other_user}', 'gists_url': 'https://api.github.com/users/adiudiuu/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/adiudiuu/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/adiudiuu/subscriptions', 'organizations_url': 'https://api.github.com/users/adiudiuu/orgs', 'repos_url': 'https://api.github.com/users/adiudiuu/repos', 'events_url': 'https://api.github.com/users/adiudiuu/events{/privacy}', 'received_events_url': 'https://api.github.com/users/adiudiuu/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}","[{'id': 5667396184, 'node_id': 'LA_kwDOJ0Z1Ps8AAAABUc2aWA', 'url': 'https://api.github.com/repos/ollama/ollama/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': ""Something isn't working""}]",open,False,,[],,1,2025-11-23T14:21:24Z,2025-11-23T14:30:26Z,,NONE,,,"{'total': 0, 'completed': 0, 'percent_completed': 0}","{'blocked_by': 0, 'total_blocked_by': 0, 'blocking': 0, 'total_blocking': 0}","### m4 mac 文字显示有问题，会被遮住

<img width=""974"" height=""655"" alt=""Image"" src=""https://github.com/user-attachments/assets/9080fd5b-1bb3-4fc5-9270-1ed2e6b103b1"" />

### Relevant log output

```shell

```

### OS

m4
",,"{'url': 'https://api.github.com/repos/ollama/ollama/issues/13214/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}",https://api.github.com/repos/ollama/ollama/issues/13214/timeline,,,,
https://api.github.com/repos/ollama/ollama/issues/13213,https://api.github.com/repos/ollama/ollama,https://api.github.com/repos/ollama/ollama/issues/13213/labels{/name},https://api.github.com/repos/ollama/ollama/issues/13213/comments,https://api.github.com/repos/ollama/ollama/issues/13213/events,https://github.com/ollama/ollama/pull/13213,3656059327,PR_kwDOJ0Z1Ps61DZG0,13213,Add /api/models/size endpoint to report total storage usage,"{'login': 'cd-Ishita', 'id': 61752983, 'node_id': 'MDQ6VXNlcjYxNzUyOTgz', 'avatar_url': 'https://avatars.githubusercontent.com/u/61752983?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/cd-Ishita', 'html_url': 'https://github.com/cd-Ishita', 'followers_url': 'https://api.github.com/users/cd-Ishita/followers', 'following_url': 'https://api.github.com/users/cd-Ishita/following{/other_user}', 'gists_url': 'https://api.github.com/users/cd-Ishita/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/cd-Ishita/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/cd-Ishita/subscriptions', 'organizations_url': 'https://api.github.com/users/cd-Ishita/orgs', 'repos_url': 'https://api.github.com/users/cd-Ishita/repos', 'events_url': 'https://api.github.com/users/cd-Ishita/events{/privacy}', 'received_events_url': 'https://api.github.com/users/cd-Ishita/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}",[],open,False,,[],,0,2025-11-23T14:18:15Z,2025-11-23T14:18:15Z,,NONE,,,,,"## Description
Adds a new `/api/models/size` endpoint that returns the total disk space used by all downloaded models.

## Motivation
Users often need to quickly check how much storage their Ollama models are consuming without parsing the full model list. This endpoint provides a simple way to get total storage metrics.

## Changes
- Added `ModelsStorageHandler` server handler
- Registered new GET endpoint at `/api/models/size`
- Returns JSON with `total_size_bytes` and `models_count`
- Added unit tests
- Updated API documentation

## Testing
- [x] Manual testing with local endpoint
- [x] Unit tests pass (`go test ./server/...`)
- [x] Documentation updated

## Example Response
```json
{
  ""total_size_bytes"": 4661229568,
  ""models_count"": 3
}
```",,"{'url': 'https://api.github.com/repos/ollama/ollama/issues/13213/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}",https://api.github.com/repos/ollama/ollama/issues/13213/timeline,,,False,"{'url': 'https://api.github.com/repos/ollama/ollama/pulls/13213', 'html_url': 'https://github.com/ollama/ollama/pull/13213', 'diff_url': 'https://github.com/ollama/ollama/pull/13213.diff', 'patch_url': 'https://github.com/ollama/ollama/pull/13213.patch', 'merged_at': None}"
https://api.github.com/repos/ollama/ollama/issues/13212,https://api.github.com/repos/ollama/ollama,https://api.github.com/repos/ollama/ollama/issues/13212/labels{/name},https://api.github.com/repos/ollama/ollama/issues/13212/comments,https://api.github.com/repos/ollama/ollama/issues/13212/events,https://github.com/ollama/ollama/issues/13212,3655822537,I_kwDOJ0Z1Ps7Z52zJ,13212,Vulkan is enabled by default and can't be disabled with OLLAMA_VULKAN=0,"{'login': 'PaulGrandperrin', 'id': 1748936, 'node_id': 'MDQ6VXNlcjE3NDg5MzY=', 'avatar_url': 'https://avatars.githubusercontent.com/u/1748936?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/PaulGrandperrin', 'html_url': 'https://github.com/PaulGrandperrin', 'followers_url': 'https://api.github.com/users/PaulGrandperrin/followers', 'following_url': 'https://api.github.com/users/PaulGrandperrin/following{/other_user}', 'gists_url': 'https://api.github.com/users/PaulGrandperrin/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/PaulGrandperrin/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/PaulGrandperrin/subscriptions', 'organizations_url': 'https://api.github.com/users/PaulGrandperrin/orgs', 'repos_url': 'https://api.github.com/users/PaulGrandperrin/repos', 'events_url': 'https://api.github.com/users/PaulGrandperrin/events{/privacy}', 'received_events_url': 'https://api.github.com/users/PaulGrandperrin/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}","[{'id': 5667396184, 'node_id': 'LA_kwDOJ0Z1Ps8AAAABUc2aWA', 'url': 'https://api.github.com/repos/ollama/ollama/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': ""Something isn't working""}]",closed,False,,[],,4,2025-11-23T10:45:18Z,2025-11-23T11:10:39Z,2025-11-23T11:10:39Z,NONE,,,"{'total': 0, 'completed': 0, 'percent_completed': 0}","{'blocked_by': 0, 'total_blocked_by': 0, 'blocking': 0, 'total_blocking': 0}","### What is the issue?

According to the release log of version [v0.12.11](https://github.com/ollama/ollama/releases/tag/v0.12.11) Vulkan shouldn't be enabled by default:
```
Vulkan support (opt-in)
Ollama 0.12.11 includes support for Vulkan acceleration. Vulkan brings support for a broad range of GPUs from AMD, Intel, and iGPUs. Vulkan support is not yet enabled by default, and requires opting in by running Ollama with a custom environment variable:

OLLAMA_VULKAN=1 ollama serve
```

But it seems to be always enabled even when explicitly disabled with  `OLLAMA_VULKAN=0`.

I have this issue with both `v0.12.11` and `v0.13.0` binaries.

It is an issue because on some systems with integrated GPUs, this makes Ollama slower than when using the CPU only.

### Relevant log output

```shell
$ OLLAMA_VULKAN=0./bin/ollama serve                                                                                          
time=2025-11-23T11:38:41.941+01:00 level=INFO source=routes.go:1544 msg=""server config"" env=""map[CUDA_VISIBLE_DEVICES: GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/paulg/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false OLLAMA_VULKAN:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]""
time=2025-11-23T11:38:41.942+01:00 level=INFO source=images.go:522 msg=""total blobs: 5""
time=2025-11-23T11:38:41.942+01:00 level=INFO source=images.go:529 msg=""total unused blobs removed: 0""
time=2025-11-23T11:38:41.942+01:00 level=INFO source=routes.go:1597 msg=""Listening on 127.0.0.1:11434 (version 0.13.0)""
time=2025-11-23T11:38:41.943+01:00 level=INFO source=runner.go:67 msg=""discovering available GPUs...""
time=2025-11-23T11:38:41.945+01:00 level=INFO source=server.go:392 msg=""starting runner"" cmd=""/home/paulg/Downloads/ollama/bin/ollama runner --ollama-engine --port 46713""
time=2025-11-23T11:38:42.091+01:00 level=INFO source=server.go:392 msg=""starting runner"" cmd=""/home/paulg/Downloads/ollama/bin/ollama runner --ollama-engine --port 42989""
time=2025-11-23T11:38:42.308+01:00 level=INFO source=runner.go:102 msg=""experimental Vulkan support disabled.  To enable, set OLLAMA_VULKAN=1""
time=2025-11-23T11:38:42.308+01:00 level=INFO source=types.go:42 msg=""inference compute"" id=GPU-06f6a1f2-0ec1-1360-a8d4-b72add839377 filter_id="""" library=CUDA compute=6.1 name=CUDA0 description=""NVIDIA GeForce GTX 1050"" libdirs=ollama,cuda_v12 driver=13.0 pci_id=0000:01:00.0 type=discrete total=""4.0 GiB"" available=""3.9 GiB""
time=2025-11-23T11:38:42.308+01:00 level=INFO source=routes.go:1638 msg=""entering low vram mode"" ""total vram""=""4.0 GiB"" threshold=""20.0 GiB""
[GIN] 2025/11/23 - 11:38:48 | 200 |      68.093µs |       127.0.0.1 | HEAD     ""/""
[GIN] 2025/11/23 - 11:38:48 | 200 |   63.250467ms |       127.0.0.1 | POST     ""/api/show""
[GIN] 2025/11/23 - 11:38:48 | 200 |   61.566648ms |       127.0.0.1 | POST     ""/api/show""
time=2025-11-23T11:38:48.266+01:00 level=INFO source=server.go:392 msg=""starting runner"" cmd=""/home/paulg/Downloads/ollama/bin/ollama runner --ollama-engine --port 40989""
llama_model_loader: loaded meta data with 26 key-value pairs and 339 tensors from /home/paulg/.ollama/models/blobs/sha256-aabd4debf0c8f08881923f2c25fc0fdeed24435271c2b3e92c4af36704040dbc (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Qwen 1.5B
llama_model_loader: - kv   3:                           general.basename str              = DeepSeek-R1-Distill-Qwen
llama_model_loader: - kv   4:                         general.size_label str              = 1.5B
llama_model_loader: - kv   5:                          qwen2.block_count u32              = 28
llama_model_loader: - kv   6:                       qwen2.context_length u32              = 131072
llama_model_loader: - kv   7:                     qwen2.embedding_length u32              = 1536
llama_model_loader: - kv   8:                  qwen2.feed_forward_length u32              = 8960
llama_model_loader: - kv   9:                 qwen2.attention.head_count u32              = 12
llama_model_loader: - kv  10:              qwen2.attention.head_count_kv u32              = 2
llama_model_loader: - kv  11:                       qwen2.rope.freq_base f32              = 10000.000000
llama_model_loader: - kv  12:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  13:                          general.file_type u32              = 15
llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,151936]  = [""!"", ""\"""", ""#"", ""$"", ""%"", ""&"", ""'"", ...
llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  18:                      tokenizer.ggml.merges arr[str,151387]  = [""Ġ Ġ"", ""ĠĠ ĠĠ"", ""i n"", ""Ġ t"",...
llama_model_loader: - kv  19:                tokenizer.ggml.bos_token_id u32              = 151646
llama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 151643
llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  23:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
llama_model_loader: - kv  25:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 1.04 GiB (5.00 BPW) 
load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
load: printing all EOG tokens:
load:   - 151643 ('<｜end▁of▁sentence｜>')
load:   - 151662 ('<|fim_pad|>')
load:   - 151663 ('<|repo_name|>')
load:   - 151664 ('<|file_sep|>')
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 1.78 B
print_info: general.name     = DeepSeek R1 Distill Qwen 1.5B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151646 '<｜begin▁of▁sentence｜>'
print_info: EOS token        = 151643 '<｜end▁of▁sentence｜>'
print_info: EOT token        = 151643 '<｜end▁of▁sentence｜>'
print_info: PAD token        = 151643 '<｜end▁of▁sentence｜>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<｜end▁of▁sentence｜>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-11-23T11:38:48.795+01:00 level=INFO source=server.go:392 msg=""starting runner"" cmd=""/home/paulg/Downloads/ollama/bin/ollama runner --model /home/paulg/.ollama/models/blobs/sha256-aabd4debf0c8f08881923f2c25fc0fdeed24435271c2b3e92c4af36704040dbc --port 37021""
time=2025-11-23T11:38:48.795+01:00 level=INFO source=sched.go:443 msg=""system memory"" total=""31.2 GiB"" free=""10.7 GiB"" free_swap=""22.6 GiB""
time=2025-11-23T11:38:48.795+01:00 level=INFO source=sched.go:450 msg=""gpu memory"" id=GPU-06f6a1f2-0ec1-1360-a8d4-b72add839377 library=CUDA available=""3.5 GiB"" free=""3.9 GiB"" minimum=""457.0 MiB"" overhead=""0 B""
time=2025-11-23T11:38:48.795+01:00 level=INFO source=server.go:459 msg=""loading model"" ""model layers""=29 requested=-1
time=2025-11-23T11:38:48.796+01:00 level=INFO source=device.go:240 msg=""model weights"" device=CUDA0 size=""934.7 MiB""
time=2025-11-23T11:38:48.796+01:00 level=INFO source=device.go:251 msg=""kv cache"" device=CUDA0 size=""112.0 MiB""
time=2025-11-23T11:38:48.796+01:00 level=INFO source=device.go:262 msg=""compute graph"" device=CUDA0 size=""299.8 MiB""
time=2025-11-23T11:38:48.796+01:00 level=INFO source=device.go:272 msg=""total memory"" size=""1.3 GiB""
time=2025-11-23T11:38:48.807+01:00 level=INFO source=runner.go:963 msg=""starting go runner""
load_backend: loaded CPU backend from /home/paulg/Downloads/ollama/lib/ollama/libggml-cpu-haswell.so
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce GTX 1050, compute capability 6.1, VMM: yes, ID: GPU-06f6a1f2-0ec1-1360-a8d4-b72add839377
load_backend: loaded CUDA backend from /home/paulg/Downloads/ollama/lib/ollama/cuda_v12/libggml-cuda.so
time=2025-11-23T11:38:48.874+01:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,520,600,610,700,750,800,860,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-11-23T11:38:48.874+01:00 level=INFO source=runner.go:999 msg=""Server listening on 127.0.0.1:37021""
time=2025-11-23T11:38:48.881+01:00 level=INFO source=runner.go:893 msg=load request=""{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:false KvSize:4096 KvCacheType: NumThreads:4 GPULayers:29[ID:GPU-06f6a1f2-0ec1-1360-a8d4-b72add839377 Layers:29(0..28)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:true}""
time=2025-11-23T11:38:48.881+01:00 level=INFO source=server.go:1294 msg=""waiting for llama runner to start responding""
time=2025-11-23T11:38:48.882+01:00 level=INFO source=server.go:1328 msg=""waiting for server to become available"" status=""llm server loading model""
ggml_backend_cuda_device_get_memory device GPU-06f6a1f2-0ec1-1360-a8d4-b72add839377 utilizing NVML memory reporting free: 4227399680 total: 4294967296
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce GTX 1050) (0000:01:00.0) - 4031 MiB free
llama_model_loader: loaded meta data with 26 key-value pairs and 339 tensors from /home/paulg/.ollama/models/blobs/sha256-aabd4debf0c8f08881923f2c25fc0fdeed24435271c2b3e92c4af36704040dbc (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Qwen 1.5B
llama_model_loader: - kv   3:                           general.basename str              = DeepSeek-R1-Distill-Qwen
llama_model_loader: - kv   4:                         general.size_label str              = 1.5B
llama_model_loader: - kv   5:                          qwen2.block_count u32              = 28
llama_model_loader: - kv   6:                       qwen2.context_length u32              = 131072
llama_model_loader: - kv   7:                     qwen2.embedding_length u32              = 1536
llama_model_loader: - kv   8:                  qwen2.feed_forward_length u32              = 8960
llama_model_loader: - kv   9:                 qwen2.attention.head_count u32              = 12
llama_model_loader: - kv  10:              qwen2.attention.head_count_kv u32              = 2
llama_model_loader: - kv  11:                       qwen2.rope.freq_base f32              = 10000.000000
llama_model_loader: - kv  12:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  13:                          general.file_type u32              = 15
llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,151936]  = [""!"", ""\"""", ""#"", ""$"", ""%"", ""&"", ""'"", ...
llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  18:                      tokenizer.ggml.merges arr[str,151387]  = [""Ġ Ġ"", ""ĠĠ ĠĠ"", ""i n"", ""Ġ t"",...
llama_model_loader: - kv  19:                tokenizer.ggml.bos_token_id u32              = 151646
llama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 151643
llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  23:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
llama_model_loader: - kv  25:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 1.04 GiB (5.00 BPW) 
load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
load: printing all EOG tokens:
load:   - 151643 ('<｜end▁of▁sentence｜>')
load:   - 151662 ('<|fim_pad|>')
load:   - 151663 ('<|repo_name|>')
load:   - 151664 ('<|file_sep|>')
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 1536
print_info: n_layer          = 28
print_info: n_head           = 12
print_info: n_head_kv        = 2
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 6
print_info: n_embd_k_gqa     = 256
print_info: n_embd_v_gqa     = 256
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 8960
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = -1
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: model type       = 1.5B
print_info: model params     = 1.78 B
print_info: general.name     = DeepSeek R1 Distill Qwen 1.5B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151646 '<｜begin▁of▁sentence｜>'
print_info: EOS token        = 151643 '<｜end▁of▁sentence｜>'
print_info: EOT token        = 151643 '<｜end▁of▁sentence｜>'
print_info: PAD token        = 151643 '<｜end▁of▁sentence｜>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<｜end▁of▁sentence｜>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 28 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 29/29 layers to GPU
load_tensors:        CUDA0 model buffer size =   934.70 MiB
load_tensors:   CPU_Mapped model buffer size =   125.19 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = disabled
llama_context: kv_unified    = false
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     0.59 MiB
llama_kv_cache:      CUDA0 KV buffer size =   112.00 MiB
llama_kv_cache: size =  112.00 MiB (  4096 cells,  28 layers,  1/1 seqs), K (f16):   56.00 MiB, V (f16):   56.00 MiB
llama_context:      CUDA0 compute buffer size =   299.75 MiB
llama_context:  CUDA_Host compute buffer size =    12.01 MiB
llama_context: graph nodes  = 1098
llama_context: graph splits = 2
time=2025-11-23T11:38:49.884+01:00 level=INFO source=server.go:1332 msg=""llama runner started in 1.09 seconds""
time=2025-11-23T11:38:49.885+01:00 level=INFO source=sched.go:517 msg=""loaded runners"" count=1
time=2025-11-23T11:38:49.885+01:00 level=INFO source=server.go:1294 msg=""waiting for llama runner to start responding""
time=2025-11-23T11:38:49.885+01:00 level=INFO source=server.go:1332 msg=""llama runner started in 1.09 seconds""
[GIN] 2025/11/23 - 11:38:49 | 200 |   1.73076529s |       127.0.0.1 | POST     ""/api/generate""
```

### OS

Linux

### GPU

Nvidia

### CPU

Intel

### Ollama version

0.13.0","{'login': 'PaulGrandperrin', 'id': 1748936, 'node_id': 'MDQ6VXNlcjE3NDg5MzY=', 'avatar_url': 'https://avatars.githubusercontent.com/u/1748936?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/PaulGrandperrin', 'html_url': 'https://github.com/PaulGrandperrin', 'followers_url': 'https://api.github.com/users/PaulGrandperrin/followers', 'following_url': 'https://api.github.com/users/PaulGrandperrin/following{/other_user}', 'gists_url': 'https://api.github.com/users/PaulGrandperrin/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/PaulGrandperrin/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/PaulGrandperrin/subscriptions', 'organizations_url': 'https://api.github.com/users/PaulGrandperrin/orgs', 'repos_url': 'https://api.github.com/users/PaulGrandperrin/repos', 'events_url': 'https://api.github.com/users/PaulGrandperrin/events{/privacy}', 'received_events_url': 'https://api.github.com/users/PaulGrandperrin/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}","{'url': 'https://api.github.com/repos/ollama/ollama/issues/13212/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}",https://api.github.com/repos/ollama/ollama/issues/13212/timeline,,completed,,
https://api.github.com/repos/ollama/ollama/issues/13211,https://api.github.com/repos/ollama/ollama,https://api.github.com/repos/ollama/ollama/issues/13211/labels{/name},https://api.github.com/repos/ollama/ollama/issues/13211/comments,https://api.github.com/repos/ollama/ollama/issues/13211/events,https://github.com/ollama/ollama/issues/13211,3655672790,I_kwDOJ0Z1Ps7Z5SPW,13211,Windows GUI - Version 0.13.0 some models (gemma3:27b) does't accept images,"{'login': 'msvobodnikov', 'id': 237410039, 'node_id': 'U_kgDODiaW9w', 'avatar_url': 'https://avatars.githubusercontent.com/u/237410039?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/msvobodnikov', 'html_url': 'https://github.com/msvobodnikov', 'followers_url': 'https://api.github.com/users/msvobodnikov/followers', 'following_url': 'https://api.github.com/users/msvobodnikov/following{/other_user}', 'gists_url': 'https://api.github.com/users/msvobodnikov/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/msvobodnikov/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/msvobodnikov/subscriptions', 'organizations_url': 'https://api.github.com/users/msvobodnikov/orgs', 'repos_url': 'https://api.github.com/users/msvobodnikov/repos', 'events_url': 'https://api.github.com/users/msvobodnikov/events{/privacy}', 'received_events_url': 'https://api.github.com/users/msvobodnikov/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}","[{'id': 5667396184, 'node_id': 'LA_kwDOJ0Z1Ps8AAAABUc2aWA', 'url': 'https://api.github.com/repos/ollama/ollama/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': ""Something isn't working""}]",open,False,,[],,0,2025-11-23T08:50:33Z,2025-11-23T08:50:33Z,,NONE,,,"{'total': 0, 'completed': 0, 'percent_completed': 0}","{'blocked_by': 0, 'total_blocked_by': 0, 'blocking': 0, 'total_blocking': 0}","### What is the issue?

Problem with windows GUI. For gemma3:27b model, if I try to add image to message, I get error: ""This model does not support images"", If I change model to something else - like qwen3-vl, paste image, then change model back to gemma3, it works. So model is definetly ok (and I was using it in previous versions) but after last update, something is broken. Model works with images if I use CLI.


### Relevant log output

```shell

```

### OS

Windows

### GPU

Nvidia

### CPU

AMD

### Ollama version

0.13.0",,"{'url': 'https://api.github.com/repos/ollama/ollama/issues/13211/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}",https://api.github.com/repos/ollama/ollama/issues/13211/timeline,,,,
https://api.github.com/repos/ollama/ollama/issues/13210,https://api.github.com/repos/ollama/ollama,https://api.github.com/repos/ollama/ollama/issues/13210/labels{/name},https://api.github.com/repos/ollama/ollama/issues/13210/comments,https://api.github.com/repos/ollama/ollama/issues/13210/events,https://github.com/ollama/ollama/issues/13210,3655647842,I_kwDOJ0Z1Ps7Z5MJi,13210,v0.13.0 release notes,"{'login': 'maternion', 'id': 98753158, 'node_id': 'U_kgDOBeLahg', 'avatar_url': 'https://avatars.githubusercontent.com/u/98753158?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/maternion', 'html_url': 'https://github.com/maternion', 'followers_url': 'https://api.github.com/users/maternion/followers', 'following_url': 'https://api.github.com/users/maternion/following{/other_user}', 'gists_url': 'https://api.github.com/users/maternion/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/maternion/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/maternion/subscriptions', 'organizations_url': 'https://api.github.com/users/maternion/orgs', 'repos_url': 'https://api.github.com/users/maternion/repos', 'events_url': 'https://api.github.com/users/maternion/events{/privacy}', 'received_events_url': 'https://api.github.com/users/maternion/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}",[],closed,False,,[],,0,2025-11-23T08:18:49Z,2025-11-23T15:11:51Z,2025-11-23T15:11:51Z,NONE,,,"{'total': 0, 'completed': 0, 'percent_completed': 0}","{'blocked_by': 0, 'total_blocked_by': 0, 'blocking': 0, 'total_blocking': 0}","This is a minor error in the hyperlink for the new model Cogito-v2.1 which was announced with Ollama v0.13.0. The [link](https://ollama.com/library/cogito-v2.1) provided in the release leads to a 404 page, and it should be replaced with 
https://ollama.com/library/cogito-2.1.  ","{'login': 'maternion', 'id': 98753158, 'node_id': 'U_kgDOBeLahg', 'avatar_url': 'https://avatars.githubusercontent.com/u/98753158?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/maternion', 'html_url': 'https://github.com/maternion', 'followers_url': 'https://api.github.com/users/maternion/followers', 'following_url': 'https://api.github.com/users/maternion/following{/other_user}', 'gists_url': 'https://api.github.com/users/maternion/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/maternion/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/maternion/subscriptions', 'organizations_url': 'https://api.github.com/users/maternion/orgs', 'repos_url': 'https://api.github.com/users/maternion/repos', 'events_url': 'https://api.github.com/users/maternion/events{/privacy}', 'received_events_url': 'https://api.github.com/users/maternion/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}","{'url': 'https://api.github.com/repos/ollama/ollama/issues/13210/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}",https://api.github.com/repos/ollama/ollama/issues/13210/timeline,,completed,,
https://api.github.com/repos/ollama/ollama/issues/13209,https://api.github.com/repos/ollama/ollama,https://api.github.com/repos/ollama/ollama/issues/13209/labels{/name},https://api.github.com/repos/ollama/ollama/issues/13209/comments,https://api.github.com/repos/ollama/ollama/issues/13209/events,https://github.com/ollama/ollama/issues/13209,3655580363,I_kwDOJ0Z1Ps7Z47rL,13209,ollama ui does not launch on linux,"{'login': 'olumolu', 'id': 162728301, 'node_id': 'U_kgDOCbMJbQ', 'avatar_url': 'https://avatars.githubusercontent.com/u/162728301?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/olumolu', 'html_url': 'https://github.com/olumolu', 'followers_url': 'https://api.github.com/users/olumolu/followers', 'following_url': 'https://api.github.com/users/olumolu/following{/other_user}', 'gists_url': 'https://api.github.com/users/olumolu/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/olumolu/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/olumolu/subscriptions', 'organizations_url': 'https://api.github.com/users/olumolu/orgs', 'repos_url': 'https://api.github.com/users/olumolu/repos', 'events_url': 'https://api.github.com/users/olumolu/events{/privacy}', 'received_events_url': 'https://api.github.com/users/olumolu/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}","[{'id': 5667396184, 'node_id': 'LA_kwDOJ0Z1Ps8AAAABUc2aWA', 'url': 'https://api.github.com/repos/ollama/ollama/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': ""Something isn't working""}]",open,False,,[],,1,2025-11-23T06:47:18Z,2025-11-23T07:54:49Z,,CONTRIBUTOR,,,"{'total': 0, 'completed': 0, 'percent_completed': 0}","{'blocked_by': 0, 'total_blocked_by': 0, 'blocking': 0, 'total_blocking': 0}","### What is the issue?

i have installed ollama but i failed to launch ollama chat ui on my linux desktop.
thanks

### Relevant log output

```shell

```

### OS

Linux

### GPU

AMD

### CPU

AMD

### Ollama version

_No response_",,"{'url': 'https://api.github.com/repos/ollama/ollama/issues/13209/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}",https://api.github.com/repos/ollama/ollama/issues/13209/timeline,,,,
https://api.github.com/repos/ollama/ollama/issues/13208,https://api.github.com/repos/ollama/ollama,https://api.github.com/repos/ollama/ollama/issues/13208/labels{/name},https://api.github.com/repos/ollama/ollama/issues/13208/comments,https://api.github.com/repos/ollama/ollama/issues/13208/events,https://github.com/ollama/ollama/issues/13208,3655439555,I_kwDOJ0Z1Ps7Z4ZTD,13208,Cache Question,"{'login': 'sammyvoncheese', 'id': 24466350, 'node_id': 'MDQ6VXNlcjI0NDY2MzUw', 'avatar_url': 'https://avatars.githubusercontent.com/u/24466350?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/sammyvoncheese', 'html_url': 'https://github.com/sammyvoncheese', 'followers_url': 'https://api.github.com/users/sammyvoncheese/followers', 'following_url': 'https://api.github.com/users/sammyvoncheese/following{/other_user}', 'gists_url': 'https://api.github.com/users/sammyvoncheese/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/sammyvoncheese/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/sammyvoncheese/subscriptions', 'organizations_url': 'https://api.github.com/users/sammyvoncheese/orgs', 'repos_url': 'https://api.github.com/users/sammyvoncheese/repos', 'events_url': 'https://api.github.com/users/sammyvoncheese/events{/privacy}', 'received_events_url': 'https://api.github.com/users/sammyvoncheese/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}","[{'id': 5667396184, 'node_id': 'LA_kwDOJ0Z1Ps8AAAABUc2aWA', 'url': 'https://api.github.com/repos/ollama/ollama/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': ""Something isn't working""}]",open,False,,[],,3,2025-11-23T03:26:10Z,2025-11-23T16:15:45Z,,NONE,,,"{'total': 0, 'completed': 0, 'percent_completed': 0}","{'blocked_by': 0, 'total_blocked_by': 0, 'blocking': 0, 'total_blocking': 0}","### What is the issue?

I use the python API to send requests to the Ollama server.

Is there any way to flush the server cache via API, or force an API request top skip the cache? 

I run text generation in loops with small differences in the prompts, I see about (1-2 in 10) failure rates.
The model returns unexpected results that looks like something partially cached.  

I want to test with the cache off/disabled to eliminate it as a possible point of failure in debugging, without having to resort to server restart.



### Relevant log output

```shell

```

### OS

Windows

### GPU

Nvidia

### CPU

AMD

### Ollama version

0.13.0",,"{'url': 'https://api.github.com/repos/ollama/ollama/issues/13208/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}",https://api.github.com/repos/ollama/ollama/issues/13208/timeline,,,,
https://api.github.com/repos/ollama/ollama/issues/13207,https://api.github.com/repos/ollama/ollama,https://api.github.com/repos/ollama/ollama/issues/13207/labels{/name},https://api.github.com/repos/ollama/ollama/issues/13207/comments,https://api.github.com/repos/ollama/ollama/issues/13207/events,https://github.com/ollama/ollama/issues/13207,3654982803,I_kwDOJ0Z1Ps7Z2pyT,13207,Discord invites aren't working,"{'login': 'yurivict', 'id': 271906, 'node_id': 'MDQ6VXNlcjI3MTkwNg==', 'avatar_url': 'https://avatars.githubusercontent.com/u/271906?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/yurivict', 'html_url': 'https://github.com/yurivict', 'followers_url': 'https://api.github.com/users/yurivict/followers', 'following_url': 'https://api.github.com/users/yurivict/following{/other_user}', 'gists_url': 'https://api.github.com/users/yurivict/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/yurivict/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/yurivict/subscriptions', 'organizations_url': 'https://api.github.com/users/yurivict/orgs', 'repos_url': 'https://api.github.com/users/yurivict/repos', 'events_url': 'https://api.github.com/users/yurivict/events{/privacy}', 'received_events_url': 'https://api.github.com/users/yurivict/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}",[],open,False,,[],,3,2025-11-22T19:04:39Z,2025-11-22T21:10:17Z,,NONE,,,"{'total': 0, 'completed': 0, 'percent_completed': 0}","{'blocked_by': 0, 'total_blocked_by': 0, 'blocking': 0, 'total_blocking': 0}","Unable to accept invite:
<img width=""1041"" height=""673"" alt=""Image"" src=""https://github.com/user-attachments/assets/43ae13b8-7b9d-4c0c-ad0e-0b8e2733f641"" />",,"{'url': 'https://api.github.com/repos/ollama/ollama/issues/13207/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}",https://api.github.com/repos/ollama/ollama/issues/13207/timeline,,,,
https://api.github.com/repos/ollama/ollama/issues/13206,https://api.github.com/repos/ollama/ollama,https://api.github.com/repos/ollama/ollama/issues/13206/labels{/name},https://api.github.com/repos/ollama/ollama/issues/13206/comments,https://api.github.com/repos/ollama/ollama/issues/13206/events,https://github.com/ollama/ollama/issues/13206,3654621699,I_kwDOJ0Z1Ps7Z1RoD,13206,Structured output request don't work in Cloud's Qwen3-coder,"{'login': 'pavelai', 'id': 159125934, 'node_id': 'U_kgDOCXwRrg', 'avatar_url': 'https://avatars.githubusercontent.com/u/159125934?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/pavelai', 'html_url': 'https://github.com/pavelai', 'followers_url': 'https://api.github.com/users/pavelai/followers', 'following_url': 'https://api.github.com/users/pavelai/following{/other_user}', 'gists_url': 'https://api.github.com/users/pavelai/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/pavelai/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/pavelai/subscriptions', 'organizations_url': 'https://api.github.com/users/pavelai/orgs', 'repos_url': 'https://api.github.com/users/pavelai/repos', 'events_url': 'https://api.github.com/users/pavelai/events{/privacy}', 'received_events_url': 'https://api.github.com/users/pavelai/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}","[{'id': 5667396184, 'node_id': 'LA_kwDOJ0Z1Ps8AAAABUc2aWA', 'url': 'https://api.github.com/repos/ollama/ollama/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': ""Something isn't working""}]",open,False,,[],,2,2025-11-22T12:12:00Z,2025-11-22T19:02:03Z,,NONE,,,"{'total': 0, 'completed': 0, 'percent_completed': 0}","{'blocked_by': 0, 'total_blocked_by': 0, 'blocking': 0, 'total_blocking': 0}","### What is the issue?

No structured output provided when using cloud models with format set. I'm using Qwen3 cloud model.

Request example:
```json
{
  ""messages"": [
    {
      ""role"": ""system"",
      ""content"": ""Read the conversation and return short summary.""
    },
    {
      ""role"": ""user"",
      ""content"": ""What is quantum physics""
    },
  ],
  ""model"": ""qwen3-coder:480b-cloud"",
  ""options"": {
    ""temperature"": 0.7
  },
  ""format"": {
    ""type"": ""object"",
    ""properties"": {
      ""result"": {
        ""type"": ""string"",
        ""description"": ""Result of the query""
      }
    },
    ""required"": [
      ""result""
    ],
    ""additionalProperties"": false,
    ""$schema"": ""http://json-schema.org/draft-07/schema#""
  },
  ""stream"": false
}
```

Response:
```json
{
  ""model"": ""qwen3-coder:480b"",
  ""created_at"": ""2025-11-14T18:54:27.746233968Z"",
  ""message"": {
    ""role"": ""assistant"",
    ""content"": ""Quantum physics is the branch of physics that studies the behavior and interactions of matter and energy at the smallest scales - typically atoms, subatomic particles, and photons. It describes how these tiny particles behave in ways that are fundamentally different from our everyday experience, exhibiting both wave-like and particle-like properties, existing in multiple states simultaneously (superposition), and being instantaneously connected across distances (entanglement). Key principles include the uncertainty principle, wave-particle duality, and quantized energy levels. Quantum physics has led to technologies like lasers, computer chips, and MRI machines.""
  },
  ""done"": true,
  ""done_reason"": ""stop"",
  ""total_duration"": 2984374561,
  ""prompt_eval_count"": 25,
  ""eval_count"": 117
}
```

Here the model response is a message with raw text not a JSON

* Ollama js package version is `v0.6.3`
* Cloud model `Qwen3:480b-cloud`
* Requests made in different JS environments

*Note* This issue was initially filed into wrong repository github.com/ollama/ollama-js as [ollama/ollama-js#264](https://github.com/ollama/ollama-js/issues/264).

### Relevant log output

```shell

```

### OS

_No response_

### GPU

_No response_

### CPU

_No response_

### Ollama version

cloud",,"{'url': 'https://api.github.com/repos/ollama/ollama/issues/13206/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}",https://api.github.com/repos/ollama/ollama/issues/13206/timeline,,,,
https://api.github.com/repos/ollama/ollama/issues/13205,https://api.github.com/repos/ollama/ollama,https://api.github.com/repos/ollama/ollama/issues/13205/labels{/name},https://api.github.com/repos/ollama/ollama/issues/13205/comments,https://api.github.com/repos/ollama/ollama/issues/13205/events,https://github.com/ollama/ollama/issues/13205,3654523926,I_kwDOJ0Z1Ps7Z05wW,13205,🦙 Stable build Ollama with latest bugs fixed🦙,"{'login': 'ciotdvelvet9', 'id': 233345082, 'node_id': 'U_kgDODeiQOg', 'avatar_url': 'https://avatars.githubusercontent.com/u/233345082?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/ciotdvelvet9', 'html_url': 'https://github.com/ciotdvelvet9', 'followers_url': 'https://api.github.com/users/ciotdvelvet9/followers', 'following_url': 'https://api.github.com/users/ciotdvelvet9/following{/other_user}', 'gists_url': 'https://api.github.com/users/ciotdvelvet9/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/ciotdvelvet9/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/ciotdvelvet9/subscriptions', 'organizations_url': 'https://api.github.com/users/ciotdvelvet9/orgs', 'repos_url': 'https://api.github.com/users/ciotdvelvet9/repos', 'events_url': 'https://api.github.com/users/ciotdvelvet9/events{/privacy}', 'received_events_url': 'https://api.github.com/users/ciotdvelvet9/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}",[],closed,False,,[],,0,2025-11-22T10:53:15Z,2025-11-22T11:06:58Z,2025-11-22T11:06:58Z,NONE,,,"{'total': 0, 'completed': 0, 'percent_completed': 0}","{'blocked_by': 0, 'total_blocked_by': 0, 'blocking': 0, 'total_blocking': 0}","We're a small team of enthusiasts who simply enjoy writing code. That’s why we jumped in to help fix bugs in Ollama and make things a bit better for everyone.

If you find our work useful, dropping a ⭐ on the repo would mean a lot to us. Thanks for the support!

https://github.com/ollama-stable-build/ollama-stable-build

> Open source — built by enthusiasts.","{'login': 'mxyng', 'id': 2372640, 'node_id': 'MDQ6VXNlcjIzNzI2NDA=', 'avatar_url': 'https://avatars.githubusercontent.com/u/2372640?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/mxyng', 'html_url': 'https://github.com/mxyng', 'followers_url': 'https://api.github.com/users/mxyng/followers', 'following_url': 'https://api.github.com/users/mxyng/following{/other_user}', 'gists_url': 'https://api.github.com/users/mxyng/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/mxyng/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/mxyng/subscriptions', 'organizations_url': 'https://api.github.com/users/mxyng/orgs', 'repos_url': 'https://api.github.com/users/mxyng/repos', 'events_url': 'https://api.github.com/users/mxyng/events{/privacy}', 'received_events_url': 'https://api.github.com/users/mxyng/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}","{'url': 'https://api.github.com/repos/ollama/ollama/issues/13205/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}",https://api.github.com/repos/ollama/ollama/issues/13205/timeline,,duplicate,,
https://api.github.com/repos/ollama/ollama/issues/13204,https://api.github.com/repos/ollama/ollama,https://api.github.com/repos/ollama/ollama/issues/13204/labels{/name},https://api.github.com/repos/ollama/ollama/issues/13204/comments,https://api.github.com/repos/ollama/ollama/issues/13204/events,https://github.com/ollama/ollama/issues/13204,3654174107,I_kwDOJ0Z1Ps7ZzkWb,13204,"Local Ollama 3.3 stopped responding, I have 64 GB ram!","{'login': 'salahzoghaib-maker', 'id': 245516154, 'node_id': 'U_kgDODqJHeg', 'avatar_url': 'https://avatars.githubusercontent.com/u/245516154?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/salahzoghaib-maker', 'html_url': 'https://github.com/salahzoghaib-maker', 'followers_url': 'https://api.github.com/users/salahzoghaib-maker/followers', 'following_url': 'https://api.github.com/users/salahzoghaib-maker/following{/other_user}', 'gists_url': 'https://api.github.com/users/salahzoghaib-maker/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/salahzoghaib-maker/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/salahzoghaib-maker/subscriptions', 'organizations_url': 'https://api.github.com/users/salahzoghaib-maker/orgs', 'repos_url': 'https://api.github.com/users/salahzoghaib-maker/repos', 'events_url': 'https://api.github.com/users/salahzoghaib-maker/events{/privacy}', 'received_events_url': 'https://api.github.com/users/salahzoghaib-maker/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}","[{'id': 5667396184, 'node_id': 'LA_kwDOJ0Z1Ps8AAAABUc2aWA', 'url': 'https://api.github.com/repos/ollama/ollama/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': ""Something isn't working""}]",open,False,,[],,4,2025-11-22T08:02:54Z,2025-11-22T11:03:35Z,,NONE,,,"{'total': 0, 'completed': 0, 'percent_completed': 0}","{'blocked_by': 0, 'total_blocked_by': 0, 'blocking': 0, 'total_blocking': 0}","### What is the issue?

I have the surface pro 11th edition 64 GB RAM, local Ollama stopped responding today. any help would be greatly appreciated. 

### Relevant log output

```shell

```

### OS

_No response_

### GPU

_No response_

### CPU

_No response_

### Ollama version

_No response_",,"{'url': 'https://api.github.com/repos/ollama/ollama/issues/13204/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}",https://api.github.com/repos/ollama/ollama/issues/13204/timeline,,,,
https://api.github.com/repos/ollama/ollama/issues/13203,https://api.github.com/repos/ollama/ollama,https://api.github.com/repos/ollama/ollama/issues/13203/labels{/name},https://api.github.com/repos/ollama/ollama/issues/13203/comments,https://api.github.com/repos/ollama/ollama/issues/13203/events,https://github.com/ollama/ollama/pull/13203,3654056169,PR_kwDOJ0Z1Ps609Bbx,13203,fix: prevent overwriting existing systemd service file,"{'login': 'autoexpect', 'id': 6939585, 'node_id': 'MDQ6VXNlcjY5Mzk1ODU=', 'avatar_url': 'https://avatars.githubusercontent.com/u/6939585?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/autoexpect', 'html_url': 'https://github.com/autoexpect', 'followers_url': 'https://api.github.com/users/autoexpect/followers', 'following_url': 'https://api.github.com/users/autoexpect/following{/other_user}', 'gists_url': 'https://api.github.com/users/autoexpect/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/autoexpect/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/autoexpect/subscriptions', 'organizations_url': 'https://api.github.com/users/autoexpect/orgs', 'repos_url': 'https://api.github.com/users/autoexpect/repos', 'events_url': 'https://api.github.com/users/autoexpect/events{/privacy}', 'received_events_url': 'https://api.github.com/users/autoexpect/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}",[],open,False,,[],,1,2025-11-22T06:47:59Z,2025-11-22T10:49:02Z,,NONE,,,,,"### Changes
- Added existence check before creating ollama.service
- Display informative message when service file already exists
- Prevents accidental overwrite of custom systemd configurations

### Motivation
During installation/upgrade, the script was unconditionally overwriting
the systemd service file, which could destroy user customizations such as:
- Custom environment variables
- Modified resource limits
- Custom startup parameters

### Impact
- Safer upgrades that preserve user configurations
- Better user experience for those who have customized the service
- No breaking changes for fresh installations",,"{'url': 'https://api.github.com/repos/ollama/ollama/issues/13203/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}",https://api.github.com/repos/ollama/ollama/issues/13203/timeline,,,False,"{'url': 'https://api.github.com/repos/ollama/ollama/pulls/13203', 'html_url': 'https://github.com/ollama/ollama/pull/13203', 'diff_url': 'https://github.com/ollama/ollama/pull/13203.diff', 'patch_url': 'https://github.com/ollama/ollama/pull/13203.patch', 'merged_at': None}"
https://api.github.com/repos/ollama/ollama/issues/13202,https://api.github.com/repos/ollama/ollama,https://api.github.com/repos/ollama/ollama/issues/13202/labels{/name},https://api.github.com/repos/ollama/ollama/issues/13202/comments,https://api.github.com/repos/ollama/ollama/issues/13202/events,https://github.com/ollama/ollama/issues/13202,3653783094,I_kwDOJ0Z1Ps7ZyE42,13202,Low Priority - Link Error,"{'login': 'wintsworks', 'id': 123520308, 'node_id': 'U_kgDOB1zFNA', 'avatar_url': 'https://avatars.githubusercontent.com/u/123520308?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/wintsworks', 'html_url': 'https://github.com/wintsworks', 'followers_url': 'https://api.github.com/users/wintsworks/followers', 'following_url': 'https://api.github.com/users/wintsworks/following{/other_user}', 'gists_url': 'https://api.github.com/users/wintsworks/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/wintsworks/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/wintsworks/subscriptions', 'organizations_url': 'https://api.github.com/users/wintsworks/orgs', 'repos_url': 'https://api.github.com/users/wintsworks/repos', 'events_url': 'https://api.github.com/users/wintsworks/events{/privacy}', 'received_events_url': 'https://api.github.com/users/wintsworks/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}",[],open,False,,[],,0,2025-11-22T03:42:25Z,2025-11-22T03:43:10Z,,NONE,,,"{'total': 0, 'completed': 0, 'percent_completed': 0}","{'blocked_by': 0, 'total_blocked_by': 0, 'blocking': 0, 'total_blocking': 0}","I noticed that in your v0.13.0 release notes that you added a v in the cogito link that 404s on your site. I just thought I would mention it, I'm not sure if you can edit the notes. Thanks for the release.

The bottom link is the issue.

Within: https://github.com/ollama/ollama/releases/tag/v0.13.0

<img width=""553"" height=""182"" alt=""Image"" src=""https://github.com/user-attachments/assets/8b2e4e80-bdfc-42ac-b93c-d47869946bc0"" />",,"{'url': 'https://api.github.com/repos/ollama/ollama/issues/13202/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}",https://api.github.com/repos/ollama/ollama/issues/13202/timeline,,,,
https://api.github.com/repos/ollama/ollama/issues/13201,https://api.github.com/repos/ollama/ollama,https://api.github.com/repos/ollama/ollama/issues/13201/labels{/name},https://api.github.com/repos/ollama/ollama/issues/13201/comments,https://api.github.com/repos/ollama/ollama/issues/13201/events,https://github.com/ollama/ollama/pull/13201,3653497192,PR_kwDOJ0Z1Ps607FMf,13201,nomic-text-embed: set batchSize equal to contextLength ,"{'login': 'npardal', 'id': 109545900, 'node_id': 'U_kgDOBoeJrA', 'avatar_url': 'https://avatars.githubusercontent.com/u/109545900?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/npardal', 'html_url': 'https://github.com/npardal', 'followers_url': 'https://api.github.com/users/npardal/followers', 'following_url': 'https://api.github.com/users/npardal/following{/other_user}', 'gists_url': 'https://api.github.com/users/npardal/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/npardal/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/npardal/subscriptions', 'organizations_url': 'https://api.github.com/users/npardal/orgs', 'repos_url': 'https://api.github.com/users/npardal/repos', 'events_url': 'https://api.github.com/users/npardal/events{/privacy}', 'received_events_url': 'https://api.github.com/users/npardal/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}",[],open,False,,[],,0,2025-11-22T00:35:09Z,2025-11-22T00:35:09Z,,CONTRIBUTOR,,,,,,,"{'url': 'https://api.github.com/repos/ollama/ollama/issues/13201/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}",https://api.github.com/repos/ollama/ollama/issues/13201/timeline,,,True,"{'url': 'https://api.github.com/repos/ollama/ollama/pulls/13201', 'html_url': 'https://github.com/ollama/ollama/pull/13201', 'diff_url': 'https://github.com/ollama/ollama/pull/13201.diff', 'patch_url': 'https://github.com/ollama/ollama/pull/13201.patch', 'merged_at': None}"
https://api.github.com/repos/ollama/ollama/issues/13200,https://api.github.com/repos/ollama/ollama,https://api.github.com/repos/ollama/ollama/issues/13200/labels{/name},https://api.github.com/repos/ollama/ollama/issues/13200/comments,https://api.github.com/repos/ollama/ollama/issues/13200/events,https://github.com/ollama/ollama/issues/13200,3653355624,I_kwDOJ0Z1Ps7Zwcho,13200,Feature Proposal: GUI Support for Remote Ollama Instances,"{'login': 'jonameijers', 'id': 30556656, 'node_id': 'MDQ6VXNlcjMwNTU2NjU2', 'avatar_url': 'https://avatars.githubusercontent.com/u/30556656?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/jonameijers', 'html_url': 'https://github.com/jonameijers', 'followers_url': 'https://api.github.com/users/jonameijers/followers', 'following_url': 'https://api.github.com/users/jonameijers/following{/other_user}', 'gists_url': 'https://api.github.com/users/jonameijers/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/jonameijers/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/jonameijers/subscriptions', 'organizations_url': 'https://api.github.com/users/jonameijers/orgs', 'repos_url': 'https://api.github.com/users/jonameijers/repos', 'events_url': 'https://api.github.com/users/jonameijers/events{/privacy}', 'received_events_url': 'https://api.github.com/users/jonameijers/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}","[{'id': 5667396200, 'node_id': 'LA_kwDOJ0Z1Ps8AAAABUc2aaA', 'url': 'https://api.github.com/repos/ollama/ollama/labels/feature%20request', 'name': 'feature request', 'color': 'a2eeef', 'default': False, 'description': 'New feature or request'}]",open,False,,[],,1,2025-11-21T23:13:20Z,2025-11-21T23:48:49Z,,NONE,,,"{'total': 0, 'completed': 0, 'percent_completed': 0}","{'blocked_by': 0, 'total_blocked_by': 0, 'blocking': 0, 'total_blocking': 0}","Note: I have a working implementation of this feature that is nearly complete. Before submitting a pull request, I wanted to open this issue to ensure the change aligns with Ollama's roadmap and to gather feedback from the maintainers, as this is a non-trivial change per the contribution guidelines.

**Problem Statement**
Users who run Ollama on a remote server cannot currently use the Ollama GUI application to connect to their remote instance. While the CLI supports connecting to remote hosts via the OLLAMA_HOST environment variable, the GUI application is hardcoded to connect only to localhost:11434. This creates a fragmented user experience.

**Why the change is important**
- Feature Parity: The CLI already supports remote connections. The GUI should offer equivalent functionality.
- Incomplete Implementation: The GUI currently offers a switch for ""Expose Ollama to the network""  but lacks the complementary ability to connect to a remote instance. This creates an asymmetric experience where users can host but cannot join.


",,"{'url': 'https://api.github.com/repos/ollama/ollama/issues/13200/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}",https://api.github.com/repos/ollama/ollama/issues/13200/timeline,,,,
https://api.github.com/repos/ollama/ollama/issues/13199,https://api.github.com/repos/ollama/ollama,https://api.github.com/repos/ollama/ollama/issues/13199/labels{/name},https://api.github.com/repos/ollama/ollama/issues/13199/comments,https://api.github.com/repos/ollama/ollama/issues/13199/events,https://github.com/ollama/ollama/issues/13199,3653195923,I_kwDOJ0Z1Ps7Zv1iT,13199,🦙 Stable build Ollama with latest bugs fixed🦙,"{'login': 'ciotdvelvet9', 'id': 233345082, 'node_id': 'U_kgDODeiQOg', 'avatar_url': 'https://avatars.githubusercontent.com/u/233345082?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/ciotdvelvet9', 'html_url': 'https://github.com/ciotdvelvet9', 'followers_url': 'https://api.github.com/users/ciotdvelvet9/followers', 'following_url': 'https://api.github.com/users/ciotdvelvet9/following{/other_user}', 'gists_url': 'https://api.github.com/users/ciotdvelvet9/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/ciotdvelvet9/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/ciotdvelvet9/subscriptions', 'organizations_url': 'https://api.github.com/users/ciotdvelvet9/orgs', 'repos_url': 'https://api.github.com/users/ciotdvelvet9/repos', 'events_url': 'https://api.github.com/users/ciotdvelvet9/events{/privacy}', 'received_events_url': 'https://api.github.com/users/ciotdvelvet9/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}",[],closed,False,,[],,3,2025-11-21T21:54:51Z,2025-11-21T23:04:03Z,2025-11-21T23:03:32Z,NONE,,,"{'total': 0, 'completed': 0, 'percent_completed': 0}","{'blocked_by': 0, 'total_blocked_by': 0, 'blocking': 0, 'total_blocking': 0}","We're a small team of enthusiasts who simply enjoy writing code. That’s why we jumped in to help fix bugs in Ollama and make things a bit better for everyone.

If you find our work useful, dropping a ⭐ on the repo would mean a lot to us. Thanks for the support!

https://github.com/ollama-stable-build/ollama-stable-build

> Open source — built by enthusiasts.","{'login': 'rick-github', 'id': 14946854, 'node_id': 'MDQ6VXNlcjE0OTQ2ODU0', 'avatar_url': 'https://avatars.githubusercontent.com/u/14946854?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/rick-github', 'html_url': 'https://github.com/rick-github', 'followers_url': 'https://api.github.com/users/rick-github/followers', 'following_url': 'https://api.github.com/users/rick-github/following{/other_user}', 'gists_url': 'https://api.github.com/users/rick-github/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/rick-github/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/rick-github/subscriptions', 'organizations_url': 'https://api.github.com/users/rick-github/orgs', 'repos_url': 'https://api.github.com/users/rick-github/repos', 'events_url': 'https://api.github.com/users/rick-github/events{/privacy}', 'received_events_url': 'https://api.github.com/users/rick-github/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}","{'url': 'https://api.github.com/repos/ollama/ollama/issues/13199/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}",https://api.github.com/repos/ollama/ollama/issues/13199/timeline,,not_planned,,
https://api.github.com/repos/ollama/ollama/issues/13198,https://api.github.com/repos/ollama/ollama,https://api.github.com/repos/ollama/ollama/issues/13198/labels{/name},https://api.github.com/repos/ollama/ollama/issues/13198/comments,https://api.github.com/repos/ollama/ollama/issues/13198/events,https://github.com/ollama/ollama/issues/13198,3652778872,I_kwDOJ0Z1Ps7ZuPt4,13198,deepseek-ocr via UI only answers if you query it twice,"{'login': 'gparent', 'id': 370905, 'node_id': 'MDQ6VXNlcjM3MDkwNQ==', 'avatar_url': 'https://avatars.githubusercontent.com/u/370905?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/gparent', 'html_url': 'https://github.com/gparent', 'followers_url': 'https://api.github.com/users/gparent/followers', 'following_url': 'https://api.github.com/users/gparent/following{/other_user}', 'gists_url': 'https://api.github.com/users/gparent/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/gparent/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/gparent/subscriptions', 'organizations_url': 'https://api.github.com/users/gparent/orgs', 'repos_url': 'https://api.github.com/users/gparent/repos', 'events_url': 'https://api.github.com/users/gparent/events{/privacy}', 'received_events_url': 'https://api.github.com/users/gparent/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}","[{'id': 5667396184, 'node_id': 'LA_kwDOJ0Z1Ps8AAAABUc2aWA', 'url': 'https://api.github.com/repos/ollama/ollama/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': ""Something isn't working""}]",closed,False,,[],,6,2025-11-21T19:03:08Z,2025-11-21T19:21:15Z,2025-11-21T19:20:41Z,NONE,,,"{'total': 0, 'completed': 0, 'percent_completed': 0}","{'blocked_by': 0, 'total_blocked_by': 0, 'blocking': 0, 'total_blocking': 0}","### What is the issue?

NOTE: I am not super familiar with Ollama technicals so I'm sorry if there's not enough information in my initial report. I will add whatever is necessary as I learn.

When using the deepseek-ocr model, I have to ask the model a followup question to actually see the reply that was produced in my new chat. This only seems to affect the first prompt to the model.

The behavior that happens is that the model starts ""thinking"" (triple dots appear), then they suddenly disappear but no answer is shown.

Asking a followup prompt like ""What was the answer"" then causes the model to give the answer.

Example:

<img width=""737"" height=""260"" alt=""Image"" src=""https://github.com/user-attachments/assets/ee7dd4b1-dc5f-4b5b-9c93-77e1dbe2fbe7"" />

### Relevant log output

```shell

```

### OS

Windows

### GPU

Nvidia

### CPU

AMD

### Ollama version

0.13.0","{'login': 'gparent', 'id': 370905, 'node_id': 'MDQ6VXNlcjM3MDkwNQ==', 'avatar_url': 'https://avatars.githubusercontent.com/u/370905?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/gparent', 'html_url': 'https://github.com/gparent', 'followers_url': 'https://api.github.com/users/gparent/followers', 'following_url': 'https://api.github.com/users/gparent/following{/other_user}', 'gists_url': 'https://api.github.com/users/gparent/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/gparent/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/gparent/subscriptions', 'organizations_url': 'https://api.github.com/users/gparent/orgs', 'repos_url': 'https://api.github.com/users/gparent/repos', 'events_url': 'https://api.github.com/users/gparent/events{/privacy}', 'received_events_url': 'https://api.github.com/users/gparent/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}","{'url': 'https://api.github.com/repos/ollama/ollama/issues/13198/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}",https://api.github.com/repos/ollama/ollama/issues/13198/timeline,,not_planned,,
https://api.github.com/repos/ollama/ollama/issues/13197,https://api.github.com/repos/ollama/ollama,https://api.github.com/repos/ollama/ollama/issues/13197/labels{/name},https://api.github.com/repos/ollama/ollama/issues/13197/comments,https://api.github.com/repos/ollama/ollama/issues/13197/events,https://github.com/ollama/ollama/pull/13197,3652674040,PR_kwDOJ0Z1Ps604SHr,13197,Claude/audit mcp integration 016i r kd ztcf5 k1 wzq et l wr86,"{'login': 'Jabdow', 'id': 113376017, 'node_id': 'U_kgDOBsH7EQ', 'avatar_url': 'https://avatars.githubusercontent.com/u/113376017?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/Jabdow', 'html_url': 'https://github.com/Jabdow', 'followers_url': 'https://api.github.com/users/Jabdow/followers', 'following_url': 'https://api.github.com/users/Jabdow/following{/other_user}', 'gists_url': 'https://api.github.com/users/Jabdow/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/Jabdow/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/Jabdow/subscriptions', 'organizations_url': 'https://api.github.com/users/Jabdow/orgs', 'repos_url': 'https://api.github.com/users/Jabdow/repos', 'events_url': 'https://api.github.com/users/Jabdow/events{/privacy}', 'received_events_url': 'https://api.github.com/users/Jabdow/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}",[],closed,False,,[],,0,2025-11-21T18:21:07Z,2025-11-21T19:20:38Z,2025-11-21T19:20:38Z,NONE,,,,,,"{'login': 'jessegross', 'id': 6468499, 'node_id': 'MDQ6VXNlcjY0Njg0OTk=', 'avatar_url': 'https://avatars.githubusercontent.com/u/6468499?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/jessegross', 'html_url': 'https://github.com/jessegross', 'followers_url': 'https://api.github.com/users/jessegross/followers', 'following_url': 'https://api.github.com/users/jessegross/following{/other_user}', 'gists_url': 'https://api.github.com/users/jessegross/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/jessegross/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/jessegross/subscriptions', 'organizations_url': 'https://api.github.com/users/jessegross/orgs', 'repos_url': 'https://api.github.com/users/jessegross/repos', 'events_url': 'https://api.github.com/users/jessegross/events{/privacy}', 'received_events_url': 'https://api.github.com/users/jessegross/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}","{'url': 'https://api.github.com/repos/ollama/ollama/issues/13197/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}",https://api.github.com/repos/ollama/ollama/issues/13197/timeline,,,False,"{'url': 'https://api.github.com/repos/ollama/ollama/pulls/13197', 'html_url': 'https://github.com/ollama/ollama/pull/13197', 'diff_url': 'https://github.com/ollama/ollama/pull/13197.diff', 'patch_url': 'https://github.com/ollama/ollama/pull/13197.patch', 'merged_at': None}"
https://api.github.com/repos/ollama/ollama/issues/13196,https://api.github.com/repos/ollama/ollama,https://api.github.com/repos/ollama/ollama/issues/13196/labels{/name},https://api.github.com/repos/ollama/ollama/issues/13196/comments,https://api.github.com/repos/ollama/ollama/issues/13196/events,https://github.com/ollama/ollama/pull/13196,3652593756,PR_kwDOJ0Z1Ps604AgL,13196,amd: use GTT on iGPUs on linux,"{'login': 'dhiltgen', 'id': 4033016, 'node_id': 'MDQ6VXNlcjQwMzMwMTY=', 'avatar_url': 'https://avatars.githubusercontent.com/u/4033016?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/dhiltgen', 'html_url': 'https://github.com/dhiltgen', 'followers_url': 'https://api.github.com/users/dhiltgen/followers', 'following_url': 'https://api.github.com/users/dhiltgen/following{/other_user}', 'gists_url': 'https://api.github.com/users/dhiltgen/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/dhiltgen/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/dhiltgen/subscriptions', 'organizations_url': 'https://api.github.com/users/dhiltgen/orgs', 'repos_url': 'https://api.github.com/users/dhiltgen/repos', 'events_url': 'https://api.github.com/users/dhiltgen/events{/privacy}', 'received_events_url': 'https://api.github.com/users/dhiltgen/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}",[],open,False,,[],,8,2025-11-21T17:57:21Z,2025-11-23T11:48:28Z,,COLLABORATOR,,,,,"On Linux, look at the GTT memory information for iGPUs.

I've only been able to verify this on a single system so far, and it seems to work correctly.  Draft until we can verify on more systems to ensure we don't inadvertently report too much available memory and start causing OOM crashes or gibberish responses.

Inspired by:
- Closes #6282 
- Closes #5426 

Fixes:
- Fixes #2637 
- Fixes #4392
- Fixes #5471
- Fixes #6362
- Fixes #12062
- Fixes #12342 
- Fixes #12411
- Fixes #13107 
- Fixes #13173",,"{'url': 'https://api.github.com/repos/ollama/ollama/issues/13196/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}",https://api.github.com/repos/ollama/ollama/issues/13196/timeline,,,True,"{'url': 'https://api.github.com/repos/ollama/ollama/pulls/13196', 'html_url': 'https://github.com/ollama/ollama/pull/13196', 'diff_url': 'https://github.com/ollama/ollama/pull/13196.diff', 'patch_url': 'https://github.com/ollama/ollama/pull/13196.patch', 'merged_at': None}"
https://api.github.com/repos/ollama/ollama/issues/13195,https://api.github.com/repos/ollama/ollama,https://api.github.com/repos/ollama/ollama/issues/13195/labels{/name},https://api.github.com/repos/ollama/ollama/issues/13195/comments,https://api.github.com/repos/ollama/ollama/issues/13195/events,https://github.com/ollama/ollama/issues/13195,3651760617,I_kwDOJ0Z1Ps7ZqXHp,13195,Computer Crashes When Running the Deepseek-OCR Model,"{'login': 'yfdjchvlgv', 'id': 215229969, 'node_id': 'U_kgDODNQmEQ', 'avatar_url': 'https://avatars.githubusercontent.com/u/215229969?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/yfdjchvlgv', 'html_url': 'https://github.com/yfdjchvlgv', 'followers_url': 'https://api.github.com/users/yfdjchvlgv/followers', 'following_url': 'https://api.github.com/users/yfdjchvlgv/following{/other_user}', 'gists_url': 'https://api.github.com/users/yfdjchvlgv/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/yfdjchvlgv/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/yfdjchvlgv/subscriptions', 'organizations_url': 'https://api.github.com/users/yfdjchvlgv/orgs', 'repos_url': 'https://api.github.com/users/yfdjchvlgv/repos', 'events_url': 'https://api.github.com/users/yfdjchvlgv/events{/privacy}', 'received_events_url': 'https://api.github.com/users/yfdjchvlgv/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}","[{'id': 5667396184, 'node_id': 'LA_kwDOJ0Z1Ps8AAAABUc2aWA', 'url': 'https://api.github.com/repos/ollama/ollama/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': ""Something isn't working""}]",open,False,,[],,5,2025-11-21T13:48:29Z,2025-11-21T16:06:43Z,,NONE,,,"{'total': 0, 'completed': 0, 'percent_completed': 0}","{'blocked_by': 0, 'total_blocked_by': 0, 'blocking': 0, 'total_blocking': 0}","### What is the issue?

After downloading the computer model according to the tutorial and running ollama run deepseek-ocr ""/path/to/image\nParse the figure."", the computer froze and automatically restarted. It was observed that after running, the CPU usage was extremely high, while the GPU usage was minimal, with approximately 10GB of VRAM being occupied.

The GPU used is an A100-40G.

### Relevant log output

```shell

```

### OS

_No response_

### GPU

_No response_

### CPU

_No response_

### Ollama version

_No response_",,"{'url': 'https://api.github.com/repos/ollama/ollama/issues/13195/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}",https://api.github.com/repos/ollama/ollama/issues/13195/timeline,,,,
https://api.github.com/repos/ollama/ollama/issues/13194,https://api.github.com/repos/ollama/ollama,https://api.github.com/repos/ollama/ollama/issues/13194/labels{/name},https://api.github.com/repos/ollama/ollama/issues/13194/comments,https://api.github.com/repos/ollama/ollama/issues/13194/events,https://github.com/ollama/ollama/issues/13194,3651579274,I_kwDOJ0Z1Ps7Zpq2K,13194,Error: 500 Internal Server Error: unable to load model,"{'login': 'SergioInToronto', 'id': 66286679, 'node_id': 'MDQ6VXNlcjY2Mjg2Njc5', 'avatar_url': 'https://avatars.githubusercontent.com/u/66286679?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/SergioInToronto', 'html_url': 'https://github.com/SergioInToronto', 'followers_url': 'https://api.github.com/users/SergioInToronto/followers', 'following_url': 'https://api.github.com/users/SergioInToronto/following{/other_user}', 'gists_url': 'https://api.github.com/users/SergioInToronto/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/SergioInToronto/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/SergioInToronto/subscriptions', 'organizations_url': 'https://api.github.com/users/SergioInToronto/orgs', 'repos_url': 'https://api.github.com/users/SergioInToronto/repos', 'events_url': 'https://api.github.com/users/SergioInToronto/events{/privacy}', 'received_events_url': 'https://api.github.com/users/SergioInToronto/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}","[{'id': 5667396184, 'node_id': 'LA_kwDOJ0Z1Ps8AAAABUc2aWA', 'url': 'https://api.github.com/repos/ollama/ollama/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': ""Something isn't working""}]",closed,False,,[],,3,2025-11-21T12:54:35Z,2025-11-21T13:05:40Z,2025-11-21T12:57:45Z,NONE,,,"{'total': 0, 'completed': 0, 'percent_completed': 0}","{'blocked_by': 0, 'total_blocked_by': 0, 'blocking': 0, 'total_blocking': 0}","### What is the issue?

1. Pull model from Hugging Face
2. Run, crashes

```bash
$ ollama run --verbose hf.co/QuantStack/Qwen-Image-Edit-GGUF:Q2_K
Error: 500 Internal Server Error: unable to load model: /usr/share/ollama/.ollama/models/blobs/sha256-a3f1680339685f558cbdf0254684e3529aab52b7e37aa8055eed0e8844a2b304
```

Seems like this is the issue from the logs. Problem with the model itself?
```
llama_model_load: error loading model: error loading model architecture: unknown model architecture: 'qwen_image'
```

### Relevant log output

```shell
Nov 21 07:52:40 RogTop ollama[5985]: [GIN] 2025/11/21 - 07:52:40 | 200 |      13.022µs |       127.0.0.1 | HEAD     ""/""
Nov 21 07:52:40 RogTop ollama[5985]: [GIN] 2025/11/21 - 07:52:40 | 200 |       9.062µs |       127.0.0.1 | GET      ""/api/ps""
Nov 21 07:52:42 RogTop ollama[5985]: [GIN] 2025/11/21 - 07:52:42 | 200 |      13.764µs |       127.0.0.1 | HEAD     ""/""
Nov 21 07:52:42 RogTop ollama[5985]: [GIN] 2025/11/21 - 07:52:42 | 200 |    8.784425ms |       127.0.0.1 | GET      ""/api/tags""
Nov 21 07:52:50 RogTop ollama[5985]: [GIN] 2025/11/21 - 07:52:50 | 200 |      18.829µs |       127.0.0.1 | HEAD     ""/""
Nov 21 07:52:50 RogTop ollama[5985]: [GIN] 2025/11/21 - 07:52:50 | 200 |    3.954528ms |       127.0.0.1 | POST     ""/api/show""
Nov 21 07:52:50 RogTop ollama[5985]: [GIN] 2025/11/21 - 07:52:50 | 200 |    2.457067ms |       127.0.0.1 | POST     ""/api/show""
Nov 21 07:52:50 RogTop ollama[5985]: time=2025-11-21T07:52:50.075-05:00 level=INFO source=server.go:392 msg=""starting runner"" cmd=""/usr/local/bin/ollama runner --ollama-engine --port 43809""
Nov 21 07:52:50 RogTop ollama[5985]: llama_model_loader: loaded meta data with 3 key-value pairs and 1933 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-a3f1680339685f558cbdf0254684e3529aab52b7e37aa8055eed0e8844a2b304 (version GGUF V3 (latest))
Nov 21 07:52:50 RogTop ollama[5985]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Nov 21 07:52:50 RogTop ollama[5985]: llama_model_loader: - kv   0:                       general.architecture str              = qwen_image
Nov 21 07:52:50 RogTop ollama[5985]: llama_model_loader: - kv   1:               general.quantization_version u32              = 2
Nov 21 07:52:50 RogTop ollama[5985]: llama_model_loader: - kv   2:                          general.file_type u32              = 10
Nov 21 07:52:50 RogTop ollama[5985]: llama_model_loader: - type  f32: 1087 tensors
Nov 21 07:52:50 RogTop ollama[5985]: llama_model_loader: - type q2_K:  696 tensors
Nov 21 07:52:50 RogTop ollama[5985]: llama_model_loader: - type q3_K:  116 tensors
Nov 21 07:52:50 RogTop ollama[5985]: llama_model_loader: - type q4_K:   28 tensors
Nov 21 07:52:50 RogTop ollama[5985]: llama_model_loader: - type bf16:    6 tensors
Nov 21 07:52:50 RogTop ollama[5985]: print_info: file format = GGUF V3 (latest)
Nov 21 07:52:50 RogTop ollama[5985]: print_info: file type   = Q2_K - Medium
Nov 21 07:52:50 RogTop ollama[5985]: print_info: file size   = 6.58 GiB (2.77 BPW)
Nov 21 07:52:50 RogTop ollama[5985]: llama_model_load: error loading model: error loading model architecture: unknown model architecture: 'qwen_image'
Nov 21 07:52:50 RogTop ollama[5985]: llama_model_load_from_file_impl: failed to load model
Nov 21 07:52:50 RogTop ollama[5985]: time=2025-11-21T07:52:50.322-05:00 level=INFO source=sched.go:425 msg=""NewLlamaServer failed"" model=/usr/share/ollama/.ollama/models/blobs/sha256-a3f1680339685f558cbdf0254684e3529aab52b7e37aa8055eed0e8844a2b304 error=""unable to load model: /usr/share/ollama/.ollama/models/blobs/sha256-a3f1680339685f558cbdf0254684e3529aab52b7e37aa8055eed0e8844a2b304""
Nov 21 07:52:50 RogTop ollama[5985]: [GIN] 2025/11/21 - 07:52:50 | 500 |  247.160546ms |       127.0.0.1 | POST     ""/api/generate""
```

### OS

Linux

### GPU

Nvidia

### CPU

Intel

### Ollama version

0.12.11","{'login': 'rick-github', 'id': 14946854, 'node_id': 'MDQ6VXNlcjE0OTQ2ODU0', 'avatar_url': 'https://avatars.githubusercontent.com/u/14946854?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/rick-github', 'html_url': 'https://github.com/rick-github', 'followers_url': 'https://api.github.com/users/rick-github/followers', 'following_url': 'https://api.github.com/users/rick-github/following{/other_user}', 'gists_url': 'https://api.github.com/users/rick-github/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/rick-github/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/rick-github/subscriptions', 'organizations_url': 'https://api.github.com/users/rick-github/orgs', 'repos_url': 'https://api.github.com/users/rick-github/repos', 'events_url': 'https://api.github.com/users/rick-github/events{/privacy}', 'received_events_url': 'https://api.github.com/users/rick-github/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}","{'url': 'https://api.github.com/repos/ollama/ollama/issues/13194/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}",https://api.github.com/repos/ollama/ollama/issues/13194/timeline,,completed,,
https://api.github.com/repos/ollama/ollama/issues/13193,https://api.github.com/repos/ollama/ollama,https://api.github.com/repos/ollama/ollama/issues/13193/labels{/name},https://api.github.com/repos/ollama/ollama/issues/13193/comments,https://api.github.com/repos/ollama/ollama/issues/13193/events,https://github.com/ollama/ollama/issues/13193,3651498917,I_kwDOJ0Z1Ps7ZpXOl,13193,More DeepSeek-OCR quantizations,"{'login': 'th1nhhdk', 'id': 58503327, 'node_id': 'MDQ6VXNlcjU4NTAzMzI3', 'avatar_url': 'https://avatars.githubusercontent.com/u/58503327?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/th1nhhdk', 'html_url': 'https://github.com/th1nhhdk', 'followers_url': 'https://api.github.com/users/th1nhhdk/followers', 'following_url': 'https://api.github.com/users/th1nhhdk/following{/other_user}', 'gists_url': 'https://api.github.com/users/th1nhhdk/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/th1nhhdk/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/th1nhhdk/subscriptions', 'organizations_url': 'https://api.github.com/users/th1nhhdk/orgs', 'repos_url': 'https://api.github.com/users/th1nhhdk/repos', 'events_url': 'https://api.github.com/users/th1nhhdk/events{/privacy}', 'received_events_url': 'https://api.github.com/users/th1nhhdk/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}",[],closed,False,,[],,3,2025-11-21T12:29:19Z,2025-11-21T13:43:05Z,2025-11-21T13:43:05Z,NONE,,,"{'total': 0, 'completed': 0, 'percent_completed': 0}","{'blocked_by': 0, 'total_blocked_by': 0, 'blocking': 0, 'total_blocking': 0}","Hello, I would like for DeepSeek-OCR on Ollama Website to have more quantizations, such as Q4_K_M, Q6_K_M, ... currently F16 is too big for some low end systems

If you don't want to quantizatize then can you at least tell me how to do it myself? Thanks","{'login': 'th1nhhdk', 'id': 58503327, 'node_id': 'MDQ6VXNlcjU4NTAzMzI3', 'avatar_url': 'https://avatars.githubusercontent.com/u/58503327?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/th1nhhdk', 'html_url': 'https://github.com/th1nhhdk', 'followers_url': 'https://api.github.com/users/th1nhhdk/followers', 'following_url': 'https://api.github.com/users/th1nhhdk/following{/other_user}', 'gists_url': 'https://api.github.com/users/th1nhhdk/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/th1nhhdk/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/th1nhhdk/subscriptions', 'organizations_url': 'https://api.github.com/users/th1nhhdk/orgs', 'repos_url': 'https://api.github.com/users/th1nhhdk/repos', 'events_url': 'https://api.github.com/users/th1nhhdk/events{/privacy}', 'received_events_url': 'https://api.github.com/users/th1nhhdk/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}","{'url': 'https://api.github.com/repos/ollama/ollama/issues/13193/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}",https://api.github.com/repos/ollama/ollama/issues/13193/timeline,,completed,,
https://api.github.com/repos/ollama/ollama/issues/13192,https://api.github.com/repos/ollama/ollama,https://api.github.com/repos/ollama/ollama/issues/13192/labels{/name},https://api.github.com/repos/ollama/ollama/issues/13192/comments,https://api.github.com/repos/ollama/ollama/issues/13192/events,https://github.com/ollama/ollama/issues/13192,3651493913,I_kwDOJ0Z1Ps7ZpWAZ,13192,fuckkkkkk ollama piece of shit !!!! fuckkkkkk this ollam autoupdate which cannot be disable and fuckkkkkk this piece of shit not support moe on gpu,"{'login': 'jonipupet', 'id': 211747211, 'node_id': 'U_kgDODJ8Biw', 'avatar_url': 'https://avatars.githubusercontent.com/u/211747211?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/jonipupet', 'html_url': 'https://github.com/jonipupet', 'followers_url': 'https://api.github.com/users/jonipupet/followers', 'following_url': 'https://api.github.com/users/jonipupet/following{/other_user}', 'gists_url': 'https://api.github.com/users/jonipupet/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/jonipupet/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/jonipupet/subscriptions', 'organizations_url': 'https://api.github.com/users/jonipupet/orgs', 'repos_url': 'https://api.github.com/users/jonipupet/repos', 'events_url': 'https://api.github.com/users/jonipupet/events{/privacy}', 'received_events_url': 'https://api.github.com/users/jonipupet/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}","[{'id': 5667396184, 'node_id': 'LA_kwDOJ0Z1Ps8AAAABUc2aWA', 'url': 'https://api.github.com/repos/ollama/ollama/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': ""Something isn't working""}]",closed,False,,[],,1,2025-11-21T12:27:53Z,2025-11-21T19:22:49Z,2025-11-21T19:22:49Z,NONE,,,"{'total': 0, 'completed': 0, 'percent_completed': 0}","{'blocked_by': 0, 'total_blocked_by': 0, 'blocking': 0, 'total_blocking': 0}","### What is the issue?

fuueekccccc ollama piece of shit !!!! fueeeekkkk this autoupdate and fuueekkkk this piece of shit not support moe on gpu fuueekccccc ollama piece of shit !!!! fueeeekkkk this autoupdate and fuueekkkk this piece of shit not support moe on gpu fuueekccccc ollama piece of shit !!!! fueeeekkkk this autoupdate and fuueekkkk this piece of shit not support moe on gpu fuueekccccc ollama piece of shit !!!! fueeeekkkk this autoupdate and fuueekkkk this piece of shit not support moe on gpu fuueekccccc ollama piece of shit !!!! fueeeekkkk this autoupdate and fuueekkkk this piece of shit not support moe on gpu fuueekccccc ollama piece of shit !!!! fueeeekkkk this autoupdate and fuueekkkk this piece of shit not support moe on gpu fuueekccccc ollama piece of shit !!!! fueeeekkkk this autoupdate and fuueekkkk this piece of shit not support moe on gpu fuueekccccc ollama piece of shit !!!! fueeeekkkk this autoupdate and fuueekkkk this piece of shit not support moe on gpu fuueekccccc ollama piece of shit !!!! fueeeekkkk this autoupdate and fuueekkkk this piece of shit not support moe on gpu fuueekccccc ollama piece of shit !!!! fueeeekkkk this autoupdate and fuueekkkk this piece of shit not support moe on gpu fuueekccccc ollama piece of shit !!!! fueeeekkkk this autoupdate and fuueekkkk this piece of shit not support moe on gpu fuueekccccc ollama piece of shit !!!! fueeeekkkk this autoupdate and fuueekkkk this piece of shit not support moe on gpu fuueekccccc ollama piece of shit !!!! fueeeekkkk this autoupdate and fuueekkkk this piece of shit not support moe on gpu fuueekccccc ollama piece of shit !!!! fueeeekkkk this autoupdate and fuueekkkk this piece of shit not support moe on gpu 

### Relevant log output

```shell

```

### OS

_No response_

### GPU

_No response_

### CPU

_No response_

### Ollama version

_No response_","{'login': 'jessegross', 'id': 6468499, 'node_id': 'MDQ6VXNlcjY0Njg0OTk=', 'avatar_url': 'https://avatars.githubusercontent.com/u/6468499?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/jessegross', 'html_url': 'https://github.com/jessegross', 'followers_url': 'https://api.github.com/users/jessegross/followers', 'following_url': 'https://api.github.com/users/jessegross/following{/other_user}', 'gists_url': 'https://api.github.com/users/jessegross/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/jessegross/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/jessegross/subscriptions', 'organizations_url': 'https://api.github.com/users/jessegross/orgs', 'repos_url': 'https://api.github.com/users/jessegross/repos', 'events_url': 'https://api.github.com/users/jessegross/events{/privacy}', 'received_events_url': 'https://api.github.com/users/jessegross/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}","{'url': 'https://api.github.com/repos/ollama/ollama/issues/13192/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}",https://api.github.com/repos/ollama/ollama/issues/13192/timeline,,completed,,
https://api.github.com/repos/ollama/ollama/issues/13191,https://api.github.com/repos/ollama/ollama,https://api.github.com/repos/ollama/ollama/issues/13191/labels{/name},https://api.github.com/repos/ollama/ollama/issues/13191/comments,https://api.github.com/repos/ollama/ollama/issues/13191/events,https://github.com/ollama/ollama/pull/13191,3651439810,PR_kwDOJ0Z1Ps600GRe,13191,docs: add MindMap to community integrations,"{'login': 'azula1A89', 'id': 191768313, 'node_id': 'U_kgDOC24m-Q', 'avatar_url': 'https://avatars.githubusercontent.com/u/191768313?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/azula1A89', 'html_url': 'https://github.com/azula1A89', 'followers_url': 'https://api.github.com/users/azula1A89/followers', 'following_url': 'https://api.github.com/users/azula1A89/following{/other_user}', 'gists_url': 'https://api.github.com/users/azula1A89/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/azula1A89/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/azula1A89/subscriptions', 'organizations_url': 'https://api.github.com/users/azula1A89/orgs', 'repos_url': 'https://api.github.com/users/azula1A89/repos', 'events_url': 'https://api.github.com/users/azula1A89/events{/privacy}', 'received_events_url': 'https://api.github.com/users/azula1A89/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}",[],open,False,,[],,0,2025-11-21T12:13:05Z,2025-11-21T12:13:05Z,,NONE,,,,,"Hello! I would be very grateful if you could add the MindMap app to the community integration list.

MindMap is a simple yet fast mindmap editor written in C++. You can call Ollama to get some inspiration.

GitHub repository: https://github.com/azula1A89/mindmap

Thanks!",,"{'url': 'https://api.github.com/repos/ollama/ollama/issues/13191/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}",https://api.github.com/repos/ollama/ollama/issues/13191/timeline,,,False,"{'url': 'https://api.github.com/repos/ollama/ollama/pulls/13191', 'html_url': 'https://github.com/ollama/ollama/pull/13191', 'diff_url': 'https://github.com/ollama/ollama/pull/13191.diff', 'patch_url': 'https://github.com/ollama/ollama/pull/13191.patch', 'merged_at': None}"
https://api.github.com/repos/ollama/ollama/issues/13190,https://api.github.com/repos/ollama/ollama,https://api.github.com/repos/ollama/ollama/issues/13190/labels{/name},https://api.github.com/repos/ollama/ollama/issues/13190/comments,https://api.github.com/repos/ollama/ollama/issues/13190/events,https://github.com/ollama/ollama/issues/13190,3651095558,I_kwDOJ0Z1Ps7Zn0wG,13190,"Vulkan an, nur am loopen","{'login': 'Profex86', 'id': 35728406, 'node_id': 'MDQ6VXNlcjM1NzI4NDA2', 'avatar_url': 'https://avatars.githubusercontent.com/u/35728406?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/Profex86', 'html_url': 'https://github.com/Profex86', 'followers_url': 'https://api.github.com/users/Profex86/followers', 'following_url': 'https://api.github.com/users/Profex86/following{/other_user}', 'gists_url': 'https://api.github.com/users/Profex86/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/Profex86/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/Profex86/subscriptions', 'organizations_url': 'https://api.github.com/users/Profex86/orgs', 'repos_url': 'https://api.github.com/users/Profex86/repos', 'events_url': 'https://api.github.com/users/Profex86/events{/privacy}', 'received_events_url': 'https://api.github.com/users/Profex86/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}","[{'id': 5667396184, 'node_id': 'LA_kwDOJ0Z1Ps8AAAABUc2aWA', 'url': 'https://api.github.com/repos/ollama/ollama/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': ""Something isn't working""}, {'id': 9653160222, 'node_id': 'LA_kwDOJ0Z1Ps8AAAACP1-JHg', 'url': 'https://api.github.com/repos/ollama/ollama/labels/vulkan', 'name': 'vulkan', 'color': 'A41E22', 'default': False, 'description': ''}]",open,False,"{'login': 'dhiltgen', 'id': 4033016, 'node_id': 'MDQ6VXNlcjQwMzMwMTY=', 'avatar_url': 'https://avatars.githubusercontent.com/u/4033016?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/dhiltgen', 'html_url': 'https://github.com/dhiltgen', 'followers_url': 'https://api.github.com/users/dhiltgen/followers', 'following_url': 'https://api.github.com/users/dhiltgen/following{/other_user}', 'gists_url': 'https://api.github.com/users/dhiltgen/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/dhiltgen/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/dhiltgen/subscriptions', 'organizations_url': 'https://api.github.com/users/dhiltgen/orgs', 'repos_url': 'https://api.github.com/users/dhiltgen/repos', 'events_url': 'https://api.github.com/users/dhiltgen/events{/privacy}', 'received_events_url': 'https://api.github.com/users/dhiltgen/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}","[{'login': 'dhiltgen', 'id': 4033016, 'node_id': 'MDQ6VXNlcjQwMzMwMTY=', 'avatar_url': 'https://avatars.githubusercontent.com/u/4033016?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/dhiltgen', 'html_url': 'https://github.com/dhiltgen', 'followers_url': 'https://api.github.com/users/dhiltgen/followers', 'following_url': 'https://api.github.com/users/dhiltgen/following{/other_user}', 'gists_url': 'https://api.github.com/users/dhiltgen/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/dhiltgen/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/dhiltgen/subscriptions', 'organizations_url': 'https://api.github.com/users/dhiltgen/orgs', 'repos_url': 'https://api.github.com/users/dhiltgen/repos', 'events_url': 'https://api.github.com/users/dhiltgen/events{/privacy}', 'received_events_url': 'https://api.github.com/users/dhiltgen/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}]",,6,2025-11-21T10:27:21Z,2025-11-21T23:30:50Z,,NONE,,,"{'total': 0, 'completed': 0, 'percent_completed': 0}","{'blocked_by': 0, 'total_blocked_by': 0, 'blocking': 0, 'total_blocking': 0}","### What is the issue?

Das Bild sagt eigentlich alles aus. Ollama Version ist die Aktuelle.

<img width=""2559"" height=""1439"" alt=""Image"" src=""https://github.com/user-attachments/assets/33b4be46-8faf-439d-9a07-380500830f64"" />

### Relevant log output

```shell

```

### OS

Windows

### GPU

AMD

### CPU

AMD

### Ollama version

0.13.0",,"{'url': 'https://api.github.com/repos/ollama/ollama/issues/13190/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}",https://api.github.com/repos/ollama/ollama/issues/13190/timeline,,,,
https://api.github.com/repos/ollama/ollama/issues/13189,https://api.github.com/repos/ollama/ollama,https://api.github.com/repos/ollama/ollama/issues/13189/labels{/name},https://api.github.com/repos/ollama/ollama/issues/13189/comments,https://api.github.com/repos/ollama/ollama/issues/13189/events,https://github.com/ollama/ollama/issues/13189,3650877207,I_kwDOJ0Z1Ps7Zm_cX,13189,"Same prompt, inconsistent results based on ollama inference and direct inference","{'login': 'lemonblock98', 'id': 77443313, 'node_id': 'MDQ6VXNlcjc3NDQzMzEz', 'avatar_url': 'https://avatars.githubusercontent.com/u/77443313?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/lemonblock98', 'html_url': 'https://github.com/lemonblock98', 'followers_url': 'https://api.github.com/users/lemonblock98/followers', 'following_url': 'https://api.github.com/users/lemonblock98/following{/other_user}', 'gists_url': 'https://api.github.com/users/lemonblock98/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/lemonblock98/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/lemonblock98/subscriptions', 'organizations_url': 'https://api.github.com/users/lemonblock98/orgs', 'repos_url': 'https://api.github.com/users/lemonblock98/repos', 'events_url': 'https://api.github.com/users/lemonblock98/events{/privacy}', 'received_events_url': 'https://api.github.com/users/lemonblock98/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}","[{'id': 5667396184, 'node_id': 'LA_kwDOJ0Z1Ps8AAAABUc2aWA', 'url': 'https://api.github.com/repos/ollama/ollama/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': ""Something isn't working""}]",open,False,,[],,9,2025-11-21T09:23:01Z,2025-11-21T17:11:55Z,,NONE,,,"{'total': 0, 'completed': 0, 'percent_completed': 0}","{'blocked_by': 0, 'total_blocked_by': 0, 'blocking': 0, 'total_blocking': 0}","### What is the issue?

I fine-tuned a Qwen3-0.6B model using the Hugging Face Transformers Trainer.
- For model conversion, I used the `convert_hf_to_gguf.py` script from llama.cpp to convert my fine-tuned SafeTensors model into .gguf format.
- For the Modelfile, I directly reused the original Modelfile for Qwen3.

Now, when I run inference with the same list of messages using the following two methods, I get inconsistent results:

1. Using Ollama
```python
url = ""http://localhost:11434/api/chat""
data = {
    ""model"": ""qwen3-0.6b-ft"",
    ""messages"": messages,
    ""tools"": tools,
    ""top_p"": 0.8,
    ""temperature"": 0.2,
    ""top_k"": 20,
    ""max_tokens"": 2048,
    ""stream"": False,
    ""think"": True
}
response = requests.post(url, json=data).json()[""message""]
```
2. Using Transformers
```python
from transformers import AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained(""..."", use_fast=False, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(""..."")

instruction = qwen3_tokenizer.apply_chat_template(messages, tools=tools, tokenize=False, add_generation_prompt=True)

model_inputs = tokenizer(instruction, add_special_tokens=False, return_tensors=""pt"").to(model.device)

generated_ids = model.generate(
    **model_inputs,
    max_new_tokens=2048,
    top_p=0.8,
    temperature=0.2,
    top_k=20
)
output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()

try:
    index = len(output_ids) - output_ids[::-1].index(151668)
except ValueError:
    index = 0

thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(""\n"")
content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(""\n"")
```
I’ve already confirmed that the TEMPLATE in the Modelfile matches the original Qwen3 chat template exactly. What could be causing this discrepancy?

### Relevant log output

```shell

```

### OS

macOS

### GPU

_No response_

### CPU

Apple

### Ollama version

0.13.0",,"{'url': 'https://api.github.com/repos/ollama/ollama/issues/13189/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}",https://api.github.com/repos/ollama/ollama/issues/13189/timeline,,,,
https://api.github.com/repos/ollama/ollama/issues/13188,https://api.github.com/repos/ollama/ollama,https://api.github.com/repos/ollama/ollama/issues/13188/labels{/name},https://api.github.com/repos/ollama/ollama/issues/13188/comments,https://api.github.com/repos/ollama/ollama/issues/13188/events,https://github.com/ollama/ollama/issues/13188,3650493283,I_kwDOJ0Z1Ps7Zlhtj,13188,"AMD 780M GPU Acceleration Not Working with 2024.12 Driver, Works with 2025.11 (Ollama 0.13.0)","{'login': 'Zhucan123', 'id': 25190846, 'node_id': 'MDQ6VXNlcjI1MTkwODQ2', 'avatar_url': 'https://avatars.githubusercontent.com/u/25190846?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/Zhucan123', 'html_url': 'https://github.com/Zhucan123', 'followers_url': 'https://api.github.com/users/Zhucan123/followers', 'following_url': 'https://api.github.com/users/Zhucan123/following{/other_user}', 'gists_url': 'https://api.github.com/users/Zhucan123/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/Zhucan123/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/Zhucan123/subscriptions', 'organizations_url': 'https://api.github.com/users/Zhucan123/orgs', 'repos_url': 'https://api.github.com/users/Zhucan123/repos', 'events_url': 'https://api.github.com/users/Zhucan123/events{/privacy}', 'received_events_url': 'https://api.github.com/users/Zhucan123/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}","[{'id': 5667396184, 'node_id': 'LA_kwDOJ0Z1Ps8AAAABUc2aWA', 'url': 'https://api.github.com/repos/ollama/ollama/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': ""Something isn't working""}, {'id': 6433346500, 'node_id': 'LA_kwDOJ0Z1Ps8AAAABf3UTxA', 'url': 'https://api.github.com/repos/ollama/ollama/labels/amd', 'name': 'amd', 'color': '000000', 'default': False, 'description': 'Issues relating to AMD GPUs and ROCm'}]",closed,False,"{'login': 'dhiltgen', 'id': 4033016, 'node_id': 'MDQ6VXNlcjQwMzMwMTY=', 'avatar_url': 'https://avatars.githubusercontent.com/u/4033016?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/dhiltgen', 'html_url': 'https://github.com/dhiltgen', 'followers_url': 'https://api.github.com/users/dhiltgen/followers', 'following_url': 'https://api.github.com/users/dhiltgen/following{/other_user}', 'gists_url': 'https://api.github.com/users/dhiltgen/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/dhiltgen/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/dhiltgen/subscriptions', 'organizations_url': 'https://api.github.com/users/dhiltgen/orgs', 'repos_url': 'https://api.github.com/users/dhiltgen/repos', 'events_url': 'https://api.github.com/users/dhiltgen/events{/privacy}', 'received_events_url': 'https://api.github.com/users/dhiltgen/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}","[{'login': 'dhiltgen', 'id': 4033016, 'node_id': 'MDQ6VXNlcjQwMzMwMTY=', 'avatar_url': 'https://avatars.githubusercontent.com/u/4033016?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/dhiltgen', 'html_url': 'https://github.com/dhiltgen', 'followers_url': 'https://api.github.com/users/dhiltgen/followers', 'following_url': 'https://api.github.com/users/dhiltgen/following{/other_user}', 'gists_url': 'https://api.github.com/users/dhiltgen/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/dhiltgen/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/dhiltgen/subscriptions', 'organizations_url': 'https://api.github.com/users/dhiltgen/orgs', 'repos_url': 'https://api.github.com/users/dhiltgen/repos', 'events_url': 'https://api.github.com/users/dhiltgen/events{/privacy}', 'received_events_url': 'https://api.github.com/users/dhiltgen/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}]",,1,2025-11-21T07:03:31Z,2025-11-21T21:52:20Z,2025-11-21T21:52:19Z,NONE,,,"{'total': 0, 'completed': 0, 'percent_completed': 0}","{'blocked_by': 0, 'total_blocked_by': 0, 'blocking': 0, 'total_blocking': 0}","### What is the issue?

Hi Ollama team,

I’m encountering an issue with GPU acceleration on an AMD 780M graphics card using the official Ollama 0.13.0 release.

Issue Background:

When using the AMD 780M with driver version 2024.12, Ollama does not utilize GPU acceleration; all computation falls back to the CPU.
Upgrading the AMD driver to the latest 2025.11 version resolves the issue, and GPU acceleration works as expected.
System Info:

Ollama version: 0.13.0
GPU: AMD 780M
OS: [please specify, e.g. Windows 11, Ubuntu 22.04, etc.]
Driver version tested:
2024.12 (No GPU acceleration)
2025.11 (GPU acceleration works)
Steps to Reproduce:

Install AMD 780M graphics card and set driver version to 2024.12
Install Ollama 0.13.0
Attempt model inference; observe that GPU is not utilized
Upgrade driver to 2025.11
Retry inference; GPU acceleration works correctly
Expected Behavior:

Ollama should utilize GPU acceleration on AMD 780M regardless of using either the 2024.12 or 2025.11 drivers.

Actual Behavior:

GPU acceleration only works on 2025.11; does not work on 2024.12.

Logs & Additional Info:

[Attach logs or screenshots if available.]

Could you please help investigate why the older 2024.12 driver is not supported for GPU acceleration, and if possible, provide a fix or workaround?

Thanks!



### Relevant log output
[log.txt](https://github.com/user-attachments/files/23670100/log.txt)
```shell

```

### OS

amd windows



### GPU

radeon 780m

### CPU

ryzen 7 8745HS w

### Ollama version

0.13.0","{'login': 'dhiltgen', 'id': 4033016, 'node_id': 'MDQ6VXNlcjQwMzMwMTY=', 'avatar_url': 'https://avatars.githubusercontent.com/u/4033016?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/dhiltgen', 'html_url': 'https://github.com/dhiltgen', 'followers_url': 'https://api.github.com/users/dhiltgen/followers', 'following_url': 'https://api.github.com/users/dhiltgen/following{/other_user}', 'gists_url': 'https://api.github.com/users/dhiltgen/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/dhiltgen/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/dhiltgen/subscriptions', 'organizations_url': 'https://api.github.com/users/dhiltgen/orgs', 'repos_url': 'https://api.github.com/users/dhiltgen/repos', 'events_url': 'https://api.github.com/users/dhiltgen/events{/privacy}', 'received_events_url': 'https://api.github.com/users/dhiltgen/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}","{'url': 'https://api.github.com/repos/ollama/ollama/issues/13188/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}",https://api.github.com/repos/ollama/ollama/issues/13188/timeline,,completed,,
https://api.github.com/repos/ollama/ollama/issues/13187,https://api.github.com/repos/ollama/ollama,https://api.github.com/repos/ollama/ollama/issues/13187/labels{/name},https://api.github.com/repos/ollama/ollama/issues/13187/comments,https://api.github.com/repos/ollama/ollama/issues/13187/events,https://github.com/ollama/ollama/issues/13187,3650306110,I_kwDOJ0Z1Ps7Zk0A-,13187,Custom Qwen3VLMoE models not working,"{'login': 'Sweaterdog', 'id': 170126024, 'node_id': 'U_kgDOCiPqyA', 'avatar_url': 'https://avatars.githubusercontent.com/u/170126024?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/Sweaterdog', 'html_url': 'https://github.com/Sweaterdog', 'followers_url': 'https://api.github.com/users/Sweaterdog/followers', 'following_url': 'https://api.github.com/users/Sweaterdog/following{/other_user}', 'gists_url': 'https://api.github.com/users/Sweaterdog/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/Sweaterdog/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/Sweaterdog/subscriptions', 'organizations_url': 'https://api.github.com/users/Sweaterdog/orgs', 'repos_url': 'https://api.github.com/users/Sweaterdog/repos', 'events_url': 'https://api.github.com/users/Sweaterdog/events{/privacy}', 'received_events_url': 'https://api.github.com/users/Sweaterdog/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}","[{'id': 5667396184, 'node_id': 'LA_kwDOJ0Z1Ps8AAAABUc2aWA', 'url': 'https://api.github.com/repos/ollama/ollama/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': ""Something isn't working""}]",open,False,,[],,5,2025-11-21T05:45:56Z,2025-11-21T23:10:50Z,,NONE,,,"{'total': 0, 'completed': 0, 'percent_completed': 0}","{'blocked_by': 0, 'total_blocked_by': 0, 'blocking': 0, 'total_blocking': 0}","### What is the issue?

I have been playing with making my own models based on a supported architecture in Ollama, but to no avail. To reproduce, you can make your own model with the architecture with the code below for testing:
```python
import torch
import gc
import os
import copy
from transformers import (
    AutoModelForCausalLM,
    AutoModel,
    AutoProcessor,
    AutoConfig,
    Qwen3VLForConditionalGeneration,
    Qwen3VLConfig,
    Qwen3VLMoeConfig,
    Qwen3VLMoeForConditionalGeneration,
    Qwen3VLMoeTextConfig
)
from tqdm import tqdm

# === CONFIGURATION ===
# The ""Spine"" (Vision Tower + Shared Attention)
# Qwen3-VL-2B is the perfect chassis.
VISION_SPINE_ID = ""Qwen/Qwen3-VL-2B-Instruct"" 

# Your 8 Experts
# Note: All must have hidden_size=2048 (standard for Qwen3-1.7B / VL-2B)
EXPERT_SOURCE_MAP = {
    0: ""Qwen/Qwen3-VL-2B-Thinking"",         # Vision Instruct (Strong Generalist)
    1: ""Qwen/Qwen3-VL-2B-Thinking"",         # Vision Thinking (Logic Specialist)
    2: ""Qwen/Qwen3-VL-2B-Thinking"",
    3: ""Qwen/Qwen3-VL-2B-Thinking"",              # Text Base (Knowledge)
    4: ""Qwen/Qwen3-VL-2B-Thinking"",              # Text Base (Knowledge)
    5: ""Qwen/Qwen3-VL-2B-Thinking"",              # Text Base (Knowledge)
    6: ""Qwen/Qwen3-VL-2B-Thinking"",     # Text Coder (Backend)
    7: ""Qwen/Qwen3-VL-2B-Thinking""      # Text Coder (Frontend/Websight Specialist)
}

SAVE_PATH = ""./test_moe""
NUM_EXPERTS = 8
NUM_EXPERTS_PER_TOK = 2
# =====================

def build_grape_flash():
    print(f""--- 1. Loading Spine Model: {VISION_SPINE_ID} ---"")
    spine_model = Qwen3VLForConditionalGeneration.from_pretrained(
        VISION_SPINE_ID, 
        dtype=torch.float16,
        device_map=""cpu"",
        trust_remote_code=True
    )
    base_config = spine_model.config

    # 🔍 CRITICAL: Analyze architecture like we did for convert.py
    print(""\n🔍 SPINE MODEL ARCHITECTURE ANALYSIS:"")
    print(f""   Vision Config:"")
    print(f""   - Model has visual tower: {hasattr(spine_model, 'visual')}"")
    
    tc = base_config.text_config
    print(f""\n   Text Config reports:"")
    print(f""   - hidden_size: {tc.hidden_size}"")
    print(f""   - num_hidden_layers: {tc.num_hidden_layers}"")
    print(f""   - num_attention_heads: {tc.num_attention_heads}"")
    print(f""   - num_key_value_heads: {tc.num_key_value_heads}"")
    print(f""   - intermediate_size: {tc.intermediate_size}"")
    
    # Get actual shapes from layer 0
    lm = spine_model.language_model
    actual_q_shape = lm.layers[0].self_attn.q_proj.weight.shape
    actual_k_shape = lm.layers[0].self_attn.k_proj.weight.shape
    actual_q_norm_shape = lm.layers[0].self_attn.q_norm.weight.shape
    actual_mlp_gate_shape = lm.layers[0].mlp.gate_proj.weight.shape
    
    print(f""\n   Actual weight shapes (Layer 0):"")
    print(f""   - q_proj: {actual_q_shape}"")
    print(f""   - k_proj: {actual_k_shape}"")
    print(f""   - q_norm: {actual_q_norm_shape}"")
    print(f""   - mlp.gate_proj: {actual_mlp_gate_shape}"")
    
    # Infer head_dim from q_norm (like we did in convert.py)
    hidden_size = tc.hidden_size
    q_norm_dim = actual_q_norm_shape[0]
    head_dim = q_norm_dim  # q_norm dimension == head_dim
    
    true_q_dim = actual_q_shape[0]
    true_kv_dim = actual_k_shape[0]
    true_num_heads = true_q_dim // head_dim
    true_num_kv_heads = true_kv_dim // head_dim
    
    print(f""\n   🎯 INFERRED CORRECT ARCHITECTURE:"")
    print(f""   - head_dim: {head_dim} (inferred from q_norm dimension)"")
    print(f""   - num_attention_heads: {true_num_heads} (config said {tc.num_attention_heads})"")
    print(f""   - num_key_value_heads: {true_num_kv_heads} (config said {tc.num_key_value_heads})"")
    print(f""   ✅ All dimensions are consistent!"")

    print(""\n--- 2. Constructing Qwen3-VL-MoE Config with EXACT dimensions ---"")
    
    # Create MoE text config from scratch with all required attributes
    moe_text_config = Qwen3VLMoeTextConfig(
        vocab_size=tc.vocab_size,
        hidden_size=hidden_size,
        num_hidden_layers=tc.num_hidden_layers,
        num_attention_heads=true_num_heads,
        num_key_value_heads=true_num_kv_heads,
        head_dim=head_dim,
        intermediate_size=tc.intermediate_size,
        hidden_act=tc.hidden_act,
        max_position_embeddings=tc.max_position_embeddings,
        initializer_range=tc.initializer_range,
        rms_norm_eps=tc.rms_norm_eps,
        use_cache=True,
        rope_theta=tc.rope_theta,
        rope_scaling=tc.rope_scaling,  # Critical for Qwen3-VL!
        attention_dropout=tc.attention_dropout,
        
        # MoE specific parameters
        num_experts=NUM_EXPERTS,
        num_experts_per_tok=NUM_EXPERTS_PER_TOK,
        moe_intermediate_size=tc.intermediate_size,
        router_aux_loss_coef=0.01,
        mlp_only_layers=[],  # Empty list means all layers have MoE
        
        # Qwen3 specific
        q_norm_eps=getattr(tc, ""q_norm_eps"", 1e-6),
        k_norm_eps=getattr(tc, ""k_norm_eps"", 1e-6)
    )
    
    # Create full VL MoE config - pass vision_config as a dict to ensure compatibility
    vision_config_dict = base_config.vision_config.to_dict()
    
    moe_config = Qwen3VLMoeConfig(
        text_config=moe_text_config.to_dict(),
        vision_config=vision_config_dict,
        vision_start_token_id=base_config.vision_start_token_id,
        vision_end_token_id=base_config.vision_end_token_id,
        image_token_id=base_config.image_token_id,
        video_token_id=base_config.video_token_id
    )
    
    print(""\n--- 3. Initializing Empty MoE Shell ---"")
    model = Qwen3VLMoeForConditionalGeneration(moe_config).to(torch.float16)

    # 🔍 VERIFY: Check if shapes match
    moe_lm = model.language_model
    moe_q_shape = moe_lm.layers[0].self_attn.q_proj.weight.shape
    moe_k_shape = moe_lm.layers[0].self_attn.k_proj.weight.shape
    moe_q_norm_shape = moe_lm.layers[0].self_attn.q_norm.weight.shape
    
    print(""\n🔍 SHAPE VERIFICATION:"")
    print(f""   q_proj: MoE {moe_q_shape} vs Spine {actual_q_shape} {'✓' if moe_q_shape == actual_q_shape else '✗'}"")
    print(f""   k_proj: MoE {moe_k_shape} vs Spine {actual_k_shape} {'✓' if moe_k_shape == actual_k_shape else '✗'}"")
    print(f""   q_norm: MoE {moe_q_norm_shape} vs Spine {actual_q_norm_shape} {'✓' if moe_q_norm_shape == actual_q_norm_shape else '✗'}"")
    
    # Check ALL shapes match
    all_match = (moe_q_shape == actual_q_shape and 
                 moe_k_shape == actual_k_shape and 
                 moe_q_norm_shape == actual_q_norm_shape)
    
    if not all_match:
        print(""\n❌ FATAL: Shapes don't match!"")
        return
    
    print(""\n✅ PERFECT MATCH! All attention layer dimensions are identical."")

    print(""\n--- 4. Grafting Vision & Shared Layers (Spine) ---"")
    # 1. Vision Tower
    model.visual.load_state_dict(spine_model.visual.state_dict())
    
    # 2. Embeddings & Final Norm
    model.language_model.embed_tokens.load_state_dict(spine_model.language_model.embed_tokens.state_dict())
    model.language_model.norm.load_state_dict(spine_model.language_model.norm.state_dict())

    # 3. All Attention & Norm Layers
    for i in tqdm(range(tc.num_hidden_layers), desc=""Grafting Attention""):
        spine_attn = spine_model.language_model.layers[i].self_attn
        moe_attn = model.language_model.layers[i].self_attn
        
        # Copy attention projection weights
        moe_attn.q_proj.load_state_dict(spine_attn.q_proj.state_dict())
        moe_attn.k_proj.load_state_dict(spine_attn.k_proj.state_dict())
        moe_attn.v_proj.load_state_dict(spine_attn.v_proj.state_dict())
        moe_attn.o_proj.load_state_dict(spine_attn.o_proj.state_dict())
        
        # Copy norms
        moe_attn.q_norm.load_state_dict(spine_attn.q_norm.state_dict())
        moe_attn.k_norm.load_state_dict(spine_attn.k_norm.state_dict())
        
        # Copy layer norms
        model.language_model.layers[i].input_layernorm.load_state_dict(
            spine_model.language_model.layers[i].input_layernorm.state_dict())
        model.language_model.layers[i].post_attention_layernorm.load_state_dict(
            spine_model.language_model.layers[i].post_attention_layernorm.state_dict())

    del spine_model
    gc.collect()

    print(""\n--- 5. Implanting 8 Expert Brains ---"")
    loaded_donors = {}
    
    for expert_idx in range(NUM_EXPERTS):
        source_id = EXPERT_SOURCE_MAP[expert_idx]
        print(f""   > Injecting Expert {expert_idx}: {source_id.split('/')[-1]}"")
        
        if source_id not in loaded_donors:
            print(f""     (Loading {source_id}...)"")
            # Check if it's a VL model or text-only model
            if ""VL"" in source_id or ""vl"" in source_id.lower():
                loaded_donors[source_id] = Qwen3VLForConditionalGeneration.from_pretrained(
                    source_id, 
                    dtype=torch.float16,
                    trust_remote_code=True,
                    device_map=""cpu""
                )
            else:
                loaded_donors[source_id] = AutoModelForCausalLM.from_pretrained(
                    source_id, 
                    dtype=torch.float16,
                    trust_remote_code=True,
                    device_map=""cpu""
                )
        
        donor = loaded_donors[source_id]
        
        # Determine the correct path to layers
        if hasattr(donor, 'language_model'):
            donor_layers = donor.language_model.layers
        elif hasattr(donor, 'model') and hasattr(donor.model, 'layers'):
            donor_layers = donor.model.layers
        else:
            print(f""     ❌ ERROR: Cannot find layers in donor model!"")
            continue
        
        # Verify donor compatibility
        if hasattr(donor, 'config'):
            if hasattr(donor.config, 'text_config'):
                donor_hidden = donor.config.text_config.hidden_size
            else:
                donor_hidden = donor.config.hidden_size
            
            if donor_hidden != hidden_size:
                print(f""     ❌ ERROR: Donor hidden_size {donor_hidden} doesn't match spine {hidden_size}!"")
                continue
        
        # Copy MLP weights into the fused expert tensors
        for i in tqdm(range(tc.num_hidden_layers), desc=f""   Extracting MLPs"", leave=False):
            donor_mlp = donor_layers[i].mlp
            target_experts = model.language_model.layers[i].mlp.experts
            
            # Donors have separate gate_proj and up_proj
            # Target has fused gate_up_proj: [num_experts, hidden_size, 2*intermediate_size]
            # We need to copy into the expert_idx slice
            
            # Extract weights from donor
            gate_weight = donor_mlp.gate_proj.weight.data  # [intermediate, hidden]
            up_weight = donor_mlp.up_proj.weight.data      # [intermediate, hidden]
            down_weight = donor_mlp.down_proj.weight.data  # [hidden, intermediate]
            
            # Fuse gate and up for this expert
            # gate_up should be [hidden, 2*intermediate] then transposed
            gate_up_fused = torch.cat([gate_weight, up_weight], dim=0)  # [2*intermediate, hidden]
            
            # Copy into the expert slice using .data to avoid gradient tracking issues
            # target_experts.gate_up_proj shape: [num_experts, hidden, 2*intermediate]
            # We need to transpose: [2*intermediate, hidden] -> [hidden, 2*intermediate]
            target_experts.gate_up_proj.data[expert_idx].copy_(gate_up_fused.t())
            
            # down_proj shape: [num_experts, intermediate, hidden]
            # donor down_weight: [hidden, intermediate] -> transpose to [intermediate, hidden]
            target_experts.down_proj.data[expert_idx].copy_(down_weight.t())

    print(f""\n--- 6. Saving Model to {SAVE_PATH} ---"")
    model.save_pretrained(SAVE_PATH)
    
    # Load and save processor separately to avoid deepcopy issues
    print(""   Saving processor..."")
    processor = AutoProcessor.from_pretrained(VISION_SPINE_ID, trust_remote_code=True)
    processor.save_pretrained(SAVE_PATH)

if __name__ == ""__main__"":
    build_grape_flash()

```

The errors listed below shouldn't occur as the GGUF was validated by the architecture by llama.cpp's tools, and the model runs in HF Transformers / llama.cpp just fine.

### Relevant log output

```shell
Nov 16 23:13:18 Sweaterdogs-PC ollama[1509765]: ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
Nov 16 23:13:18 Sweaterdogs-PC ollama[1509765]: ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
Nov 16 23:13:18 Sweaterdogs-PC ollama[1509765]: ggml_cuda_init: found 1 CUDA devices:
Nov 16 23:13:18 Sweaterdogs-PC ollama[1509765]:   Device 0: NVIDIA GeForce RTX 3070, compute capability 8.6, VMM: yes, ID: GPU-7887e8c5-c5fe-b1c6-75ab-bc397783a5e8
Nov 16 23:13:18 Sweaterdogs-PC ollama[1509765]: load_backend: loaded CUDA backend from /usr/local/lib/ollama/cuda_v12/libggml-cuda.so
Nov 16 23:13:18 Sweaterdogs-PC ollama[1509765]: time=2025-11-16T23:13:18.282-08:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,520,600,610,700,750,800,860,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
Nov 16 23:13:18 Sweaterdogs-PC ollama[1509765]: time=2025-11-16T23:13:18.546-08:00 level=INFO source=server.go:3634 msg=""http: panic serving 127.0.0.1:49130: runtime error: invalid memory address or nil pointer dereference\ngoroutine 53 [running]:\nnet/http.(*conn).serve.func1()\n\tnet/http/server.go:1947 +0xbe\npanic({0x5c74c35e3ac0?, 0x5c74c3f483e0?})\n\truntime/panic.go:792 +0x132\ngithub.com/ollama/ollama/runner/ollamarunner.(*Server).allocModel.func1()\n\tgithub.com/ollama/ollama/runner/ollamarunner/runner.go:1186 +0x11a\npanic({0x5c74c35e3ac0?, 0x5c74c3f483e0?})\n\truntime/panic.go:792 +0x132\ngithub.com/ollama/ollama/ml/nn.(*Conv3D).Forward(0x0, {0x5c74c37568b0, 0xc000e95180}, {0x5c74c3760d20?, 0xc0014a4048?}, 0x10101c000600008?, 0x71e76c6cdb20?, 0x71e7b43bd108?, 0x10?, 0x0, ...)\n\tgithub.com/ollama/ollama/ml/nn/convolution.go:25 +0x3a\ngithub.com/ollama/ollama/model/models/qwen3vl.(*VisionModel).Forward(0xc0004e80c0, {0x5c74c37568b0, 0xc000e95180}, {0x5c74c3760d20, 0xc0014a4030}, 0xc000cee9f0)\n\tgithub.com/ollama/ollama/model/models/qwen3vl/model_vision.go:223 +0x118\ngithub.com/ollama/ollama/model/models/qwen3vl.(*Model).EncodeMultimodal(0xc0004fa0d0, {0x5c74c37568b0, 0xc000e95180}, {0xc001a48000, 0x400436, 0x700000})\n\tgithub.com/ollama/ollama/model/models/qwen3vl/model.go:43 +0x14e\ngithub.com/ollama/ollama/runner/ollamarunner.(*Server).reserveWorstCaseGraph(0xc000246f00, 0x1)\n\tgithub.com/ollama/ollama/runner/ollamarunner/runner.go:1097 +0x34e\ngithub.com/ollama/ollama/runner/ollamarunner.(*Server).allocModel(0xc000246f00, {0x7ffdbc628baf?, 0x5c74c252a11a?}, {0x0, 0xa, {0xc00024c1c0, 0x1, 0x1}, 0x1}, {0x0, ...}, ...)\n\tgithub.com/ollama/ollama/runner/ollamarunner/runner.go:1219 +0x2b1\ngithub.com/ollama/ollama/runner/ollamarunner.(*Server).load(0xc000246f00, {0x5c74c3749b08, 0xc0003220e0}, 0xc000252280)\n\tgithub.com/ollama/ollama/runner/ollamarunner/runner.go:1298 +0x54d\nnet/http.HandlerFunc.ServeHTTP(0xc0004e8540?, {0x5c74c3749b08?, 0xc0003220e0?}, 0xc00031fb60?)\n\tnet/http/server.go:2294 +0x29\nnet/http.(*ServeMux).ServeHTTP(0x5c74c21da805?, {0x5c74c3749b08, 0xc0003220e0}, 0xc000252280)\n\tnet/http/server.go:2822 +0x1c4\nnet/http.serverHandler.ServeHTTP({0x5c74c3746110?}, {0x5c74c3749b08?, 0xc0003220e0?}, 0x1?)\n\tnet/http/server.go:3301 +0x8e\nnet/http.(*conn).serve(0xc0001163f0, {0x5c74c374bec8, 0xc000114840})\n\tnet/http/server.go:2102 +0x625\ncreated by net/http.(*Server).Serve in goroutine 1\n\tnet/http/server.go:3454 +0x485""
Nov 16 23:13:18 Sweaterdogs-PC ollama[1509765]: time=2025-11-16T23:13:18.547-08:00 level=INFO source=runner.go:1271 msg=load request=""{Operation:close LoraPath:[] Parallel:0 BatchSize:0 FlashAttention:false KvSize:0 KvCacheType: NumThreads:0 GPULayers:[] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}""
Nov 16 23:13:18 Sweaterdogs-PC ollama[1509765]: time=2025-11-16T23:13:18.547-08:00 level=INFO source=sched.go:470 msg=""Load failed"" model=/usr/share/ollama/.ollama/models/blobs/sha256-00659be1e6e6e97e9092bb53e1fe60e562575e49ba0c0eb61c175c4477718e8b error=""do load request: Post \""http://127.0.0.1:42257/load\"": EOF""
Nov 16 23:13:18 Sweaterdogs-PC ollama[1509765]: time=2025-11-16T23:13:18.575-08:00 level=ERROR source=server.go:265 msg=""llama runner terminated"" error=""signal: killed""
Nov 16 23:13:18 Sweaterdogs-PC ollama[1509765]: [GIN] 2025/11/16 - 23:13:18 | 500 |  926.565907ms |       127.0.0.1 | POST     ""/api/generate""

msg=""key with type not found"" key=qwen3vlmoe.vision.patch_size default=14
msg=""key with type not found"" key=qwen3vlmoe.vision.embedding_length default=1280
msg=""key with type not found"" key=qwen3vlmoe.vision.block_count default=32
```

### OS

Linux

### GPU

Nvidia

### CPU

Intel

### Ollama version

0.13.0",,"{'url': 'https://api.github.com/repos/ollama/ollama/issues/13187/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}",https://api.github.com/repos/ollama/ollama/issues/13187/timeline,,,,
https://api.github.com/repos/ollama/ollama/issues/13186,https://api.github.com/repos/ollama/ollama,https://api.github.com/repos/ollama/ollama/issues/13186/labels{/name},https://api.github.com/repos/ollama/ollama/issues/13186/comments,https://api.github.com/repos/ollama/ollama/issues/13186/events,https://github.com/ollama/ollama/pull/13186,3650233158,PR_kwDOJ0Z1Ps60wCew,13186,discover: increase GPU discovery timeout when HSA_OVERRIDE_GFX_VERSION is set,"{'login': 'Guedxx', 'id': 148347673, 'node_id': 'U_kgDOCNebGQ', 'avatar_url': 'https://avatars.githubusercontent.com/u/148347673?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/Guedxx', 'html_url': 'https://github.com/Guedxx', 'followers_url': 'https://api.github.com/users/Guedxx/followers', 'following_url': 'https://api.github.com/users/Guedxx/following{/other_user}', 'gists_url': 'https://api.github.com/users/Guedxx/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/Guedxx/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/Guedxx/subscriptions', 'organizations_url': 'https://api.github.com/users/Guedxx/orgs', 'repos_url': 'https://api.github.com/users/Guedxx/repos', 'events_url': 'https://api.github.com/users/Guedxx/events{/privacy}', 'received_events_url': 'https://api.github.com/users/Guedxx/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}",[],open,False,,[],,0,2025-11-21T05:08:40Z,2025-11-21T05:18:21Z,,NONE,,,,,"This PR extends the GPU discovery timeout from 3 seconds to 10 seconds specifically when the HSA_OVERRIDE_GFX_VERSION environment variable is detected. This resolves an issue where ""unsupported"" AMD GPUs would fall back to CPU inference when switching models.

Deploying HSA_OVERRIDE_GFX_VERSION to force support for my specific AMD GPU often experiences slower initialization times (typically 4-6 seconds) due to JIT kernel compilation or driver overhead.

Currently, discover/runner.go has a hardcoded 3-second timeout for refreshing GPU state during a model switch. Because these overridden configurations take longer than 3 seconds to report back, the refresh times out. This causes Ollama to assume the GPU is unavailable or has 0 VRAM free, forcing a fallback to CPU inference for all subsequent models after the first one.

The initial server startup works correctly because it uses a generous 30-second ""bootstrap"" timeout. This fix applies a similar logic to the refresh loop only when the user has explicitly opted into using the override.

#### Related Issues
* Most likely fixes #13002 
* Relates to #13070 - Addresses GPU discovery failures associated with `HSA_OVERRIDE_GFX_VERSION`.",,"{'url': 'https://api.github.com/repos/ollama/ollama/issues/13186/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}",https://api.github.com/repos/ollama/ollama/issues/13186/timeline,,,False,"{'url': 'https://api.github.com/repos/ollama/ollama/pulls/13186', 'html_url': 'https://github.com/ollama/ollama/pull/13186', 'diff_url': 'https://github.com/ollama/ollama/pull/13186.diff', 'patch_url': 'https://github.com/ollama/ollama/pull/13186.patch', 'merged_at': None}"
https://api.github.com/repos/ollama/ollama/issues/13185,https://api.github.com/repos/ollama/ollama,https://api.github.com/repos/ollama/ollama/issues/13185/labels{/name},https://api.github.com/repos/ollama/ollama/issues/13185/comments,https://api.github.com/repos/ollama/ollama/issues/13185/events,https://github.com/ollama/ollama/pull/13185,3650214007,PR_kwDOJ0Z1Ps60v-Zz,13185,docs: Fix typos,"{'login': 'Eason023', 'id': 75676459, 'node_id': 'MDQ6VXNlcjc1Njc2NDU5', 'avatar_url': 'https://avatars.githubusercontent.com/u/75676459?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/Eason023', 'html_url': 'https://github.com/Eason023', 'followers_url': 'https://api.github.com/users/Eason023/followers', 'following_url': 'https://api.github.com/users/Eason023/following{/other_user}', 'gists_url': 'https://api.github.com/users/Eason023/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/Eason023/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/Eason023/subscriptions', 'organizations_url': 'https://api.github.com/users/Eason023/orgs', 'repos_url': 'https://api.github.com/users/Eason023/repos', 'events_url': 'https://api.github.com/users/Eason023/events{/privacy}', 'received_events_url': 'https://api.github.com/users/Eason023/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}",[],open,False,,[],,0,2025-11-21T04:58:35Z,2025-11-23T03:44:56Z,,NONE,,,,,"## Summary
This changes Ollama to fix several small typos and wording inconsistent issues in the API and troubleshooting documentation.

## Testing
Docs only; no code changes.",,"{'url': 'https://api.github.com/repos/ollama/ollama/issues/13185/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}",https://api.github.com/repos/ollama/ollama/issues/13185/timeline,,,False,"{'url': 'https://api.github.com/repos/ollama/ollama/pulls/13185', 'html_url': 'https://github.com/ollama/ollama/pull/13185', 'diff_url': 'https://github.com/ollama/ollama/pull/13185.diff', 'patch_url': 'https://github.com/ollama/ollama/pull/13185.patch', 'merged_at': None}"
https://api.github.com/repos/ollama/ollama/issues/13184,https://api.github.com/repos/ollama/ollama,https://api.github.com/repos/ollama/ollama/issues/13184/labels{/name},https://api.github.com/repos/ollama/ollama/issues/13184/comments,https://api.github.com/repos/ollama/ollama/issues/13184/events,https://github.com/ollama/ollama/issues/13184,3649877554,I_kwDOJ0Z1Ps7ZjLYy,13184,Formatted Outputs's Schemas,"{'login': 'KansaiUser', 'id': 67377117, 'node_id': 'MDQ6VXNlcjY3Mzc3MTE3', 'avatar_url': 'https://avatars.githubusercontent.com/u/67377117?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/KansaiUser', 'html_url': 'https://github.com/KansaiUser', 'followers_url': 'https://api.github.com/users/KansaiUser/followers', 'following_url': 'https://api.github.com/users/KansaiUser/following{/other_user}', 'gists_url': 'https://api.github.com/users/KansaiUser/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/KansaiUser/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/KansaiUser/subscriptions', 'organizations_url': 'https://api.github.com/users/KansaiUser/orgs', 'repos_url': 'https://api.github.com/users/KansaiUser/repos', 'events_url': 'https://api.github.com/users/KansaiUser/events{/privacy}', 'received_events_url': 'https://api.github.com/users/KansaiUser/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}",[],open,False,,[],,1,2025-11-21T01:57:38Z,2025-11-21T15:05:01Z,,NONE,,,"{'total': 0, 'completed': 0, 'percent_completed': 0}","{'blocked_by': 0, 'total_blocked_by': 0, 'blocking': 0, 'total_blocking': 0}","I would like to ask for clarification about Structured Outputs (here in the [docs](https://github.com/ollama/ollama/blob/feb18cd710dec1e4754ea56124238a11eb3cb90a/docs/api.md#structured-outputs-1))

It is written

> Structured outputs
> Structured outputs are supported by providing a JSON schema in the format parameter. The model will generate a response that matches the schema. See the [Chat request (Structured outputs)](https://github.com/ollama/ollama/blob/feb18cd710dec1e4754ea56124238a11eb3cb90a/docs/api.md#chat-request-structured-outputs) example below.

And then we have an example in 

Chat request (Structured outputs)
Request

```
curl -X POST http://localhost:11434/api/chat -H ""Content-Type: application/json"" -d '{
  ""model"": ""llama3.1"",
  ""messages"": [{""role"": ""user"", ""content"": ""Ollama is 22 years old and busy saving the world. Return a JSON object with the age and availability.""}],
  ""stream"": false,
  ""format"": {
    ""type"": ""object"",
    ""properties"": {
      ""age"": {
        ""type"": ""integer""
      },
      ""available"": {
        ""type"": ""boolean""
      }
    },
    ""required"": [
      ""age"",
      ""available""
    ]
  },
  ""options"": {
    ""temperature"": 0
  }
}'
```
is this schema (type, properties, required)  the only one accepted? or is it the one checked and recommended?

Or can the user just provide any type of well formed JSON here?  in order for it to work

Note: I have used successfully the schema above, but in my team there is an ongoing discussion if other schemas are possible.

Note: I know that you can just put ` ""format"": ""json"",` too, but my question is about schemas",,"{'url': 'https://api.github.com/repos/ollama/ollama/issues/13184/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}",https://api.github.com/repos/ollama/ollama/issues/13184/timeline,,,,
https://api.github.com/repos/ollama/ollama/issues/13183,https://api.github.com/repos/ollama/ollama,https://api.github.com/repos/ollama/ollama/issues/13183/labels{/name},https://api.github.com/repos/ollama/ollama/issues/13183/comments,https://api.github.com/repos/ollama/ollama/issues/13183/events,https://github.com/ollama/ollama/issues/13183,3649461010,I_kwDOJ0Z1Ps7ZhlsS,13183,How to use reranker models in Ollama?,"{'login': 'pironev', 'id': 44299477, 'node_id': 'MDQ6VXNlcjQ0Mjk5NDc3', 'avatar_url': 'https://avatars.githubusercontent.com/u/44299477?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/pironev', 'html_url': 'https://github.com/pironev', 'followers_url': 'https://api.github.com/users/pironev/followers', 'following_url': 'https://api.github.com/users/pironev/following{/other_user}', 'gists_url': 'https://api.github.com/users/pironev/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/pironev/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/pironev/subscriptions', 'organizations_url': 'https://api.github.com/users/pironev/orgs', 'repos_url': 'https://api.github.com/users/pironev/repos', 'events_url': 'https://api.github.com/users/pironev/events{/privacy}', 'received_events_url': 'https://api.github.com/users/pironev/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}","[{'id': 5667396200, 'node_id': 'LA_kwDOJ0Z1Ps8AAAABUc2aaA', 'url': 'https://api.github.com/repos/ollama/ollama/labels/feature%20request', 'name': 'feature request', 'color': 'a2eeef', 'default': False, 'description': 'New feature or request'}]",closed,False,,[],,2,2025-11-20T22:53:23Z,2025-11-21T21:46:25Z,2025-11-21T21:46:24Z,NONE,,,"{'total': 0, 'completed': 0, 'percent_completed': 0}","{'blocked_by': 0, 'total_blocked_by': 0, 'blocking': 0, 'total_blocking': 0}","I've seen a lot of reranker models on the Ollama models page. 
How do I use these because **_http://localhost:11434/v1/rerank_** doesn't work?","{'login': 'pdevine', 'id': 75239, 'node_id': 'MDQ6VXNlcjc1MjM5', 'avatar_url': 'https://avatars.githubusercontent.com/u/75239?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/pdevine', 'html_url': 'https://github.com/pdevine', 'followers_url': 'https://api.github.com/users/pdevine/followers', 'following_url': 'https://api.github.com/users/pdevine/following{/other_user}', 'gists_url': 'https://api.github.com/users/pdevine/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/pdevine/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/pdevine/subscriptions', 'organizations_url': 'https://api.github.com/users/pdevine/orgs', 'repos_url': 'https://api.github.com/users/pdevine/repos', 'events_url': 'https://api.github.com/users/pdevine/events{/privacy}', 'received_events_url': 'https://api.github.com/users/pdevine/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}","{'url': 'https://api.github.com/repos/ollama/ollama/issues/13183/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}",https://api.github.com/repos/ollama/ollama/issues/13183/timeline,,duplicate,,
https://api.github.com/repos/ollama/ollama/issues/13182,https://api.github.com/repos/ollama/ollama,https://api.github.com/repos/ollama/ollama/issues/13182/labels{/name},https://api.github.com/repos/ollama/ollama/issues/13182/comments,https://api.github.com/repos/ollama/ollama/issues/13182/events,https://github.com/ollama/ollama/pull/13182,3649422649,PR_kwDOJ0Z1Ps60tTjh,13182,Deepseek v3 family parser,"{'login': 'gr4ceG', 'id': 88872231, 'node_id': 'MDQ6VXNlcjg4ODcyMjMx', 'avatar_url': 'https://avatars.githubusercontent.com/u/88872231?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/gr4ceG', 'html_url': 'https://github.com/gr4ceG', 'followers_url': 'https://api.github.com/users/gr4ceG/followers', 'following_url': 'https://api.github.com/users/gr4ceG/following{/other_user}', 'gists_url': 'https://api.github.com/users/gr4ceG/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/gr4ceG/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/gr4ceG/subscriptions', 'organizations_url': 'https://api.github.com/users/gr4ceG/orgs', 'repos_url': 'https://api.github.com/users/gr4ceG/repos', 'events_url': 'https://api.github.com/users/gr4ceG/events{/privacy}', 'received_events_url': 'https://api.github.com/users/gr4ceG/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}",[],open,False,,[],,0,2025-11-20T22:42:39Z,2025-11-22T00:51:58Z,,CONTRIBUTOR,,,,,,,"{'url': 'https://api.github.com/repos/ollama/ollama/issues/13182/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}",https://api.github.com/repos/ollama/ollama/issues/13182/timeline,,,True,"{'url': 'https://api.github.com/repos/ollama/ollama/pulls/13182', 'html_url': 'https://github.com/ollama/ollama/pull/13182', 'diff_url': 'https://github.com/ollama/ollama/pull/13182.diff', 'patch_url': 'https://github.com/ollama/ollama/pull/13182.patch', 'merged_at': None}"
https://api.github.com/repos/ollama/ollama/issues/13181,https://api.github.com/repos/ollama/ollama,https://api.github.com/repos/ollama/ollama/issues/13181/labels{/name},https://api.github.com/repos/ollama/ollama/issues/13181/comments,https://api.github.com/repos/ollama/ollama/issues/13181/events,https://github.com/ollama/ollama/pull/13181,3649344120,PR_kwDOJ0Z1Ps60tBtt,13181,docs: clarify num_ctx parameter description,"{'login': 'ssam18', 'id': 97642706, 'node_id': 'U_kgDOBdHo0g', 'avatar_url': 'https://avatars.githubusercontent.com/u/97642706?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/ssam18', 'html_url': 'https://github.com/ssam18', 'followers_url': 'https://api.github.com/users/ssam18/followers', 'following_url': 'https://api.github.com/users/ssam18/following{/other_user}', 'gists_url': 'https://api.github.com/users/ssam18/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/ssam18/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/ssam18/subscriptions', 'organizations_url': 'https://api.github.com/users/ssam18/orgs', 'repos_url': 'https://api.github.com/users/ssam18/repos', 'events_url': 'https://api.github.com/users/ssam18/events{/privacy}', 'received_events_url': 'https://api.github.com/users/ssam18/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}",[],open,False,,[],,5,2025-11-20T22:18:58Z,2025-11-21T19:45:48Z,,CONTRIBUTOR,,,,,"## Description

This PR clarifies the description of the `num_ctx` parameter in the Modelfile documentation.

## Problem

The previous description stated: ""Sets the size of the context window used to generate the next token.""

This was misleading because it suggested `num_ctx` only affects the next token generation, when it actually defines the **total token capacity** for the entire conversation (input prompt + generated response).

## Solution

Updated the description to:
> Sets the size of the context window. This is the maximum number of tokens the model can handle at once, including both the input prompt and the generated response.

This clarification helps users understand:
- `num_ctx` limits the **total** conversation size
- It includes both the prompt and the generated tokens
- Why ollama may fail when the combined prompt and response exceed this limit

## Changes

- Updated `docs/modelfile.mdx` with clearer `num_ctx` description

Fixes #12474","{'login': 'ssam18', 'id': 97642706, 'node_id': 'U_kgDOBdHo0g', 'avatar_url': 'https://avatars.githubusercontent.com/u/97642706?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/ssam18', 'html_url': 'https://github.com/ssam18', 'followers_url': 'https://api.github.com/users/ssam18/followers', 'following_url': 'https://api.github.com/users/ssam18/following{/other_user}', 'gists_url': 'https://api.github.com/users/ssam18/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/ssam18/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/ssam18/subscriptions', 'organizations_url': 'https://api.github.com/users/ssam18/orgs', 'repos_url': 'https://api.github.com/users/ssam18/repos', 'events_url': 'https://api.github.com/users/ssam18/events{/privacy}', 'received_events_url': 'https://api.github.com/users/ssam18/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}","{'url': 'https://api.github.com/repos/ollama/ollama/issues/13181/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}",https://api.github.com/repos/ollama/ollama/issues/13181/timeline,,,False,"{'url': 'https://api.github.com/repos/ollama/ollama/pulls/13181', 'html_url': 'https://github.com/ollama/ollama/pull/13181', 'diff_url': 'https://github.com/ollama/ollama/pull/13181.diff', 'patch_url': 'https://github.com/ollama/ollama/pull/13181.patch', 'merged_at': None}"
https://api.github.com/repos/ollama/ollama/issues/13180,https://api.github.com/repos/ollama/ollama,https://api.github.com/repos/ollama/ollama/issues/13180/labels{/name},https://api.github.com/repos/ollama/ollama/issues/13180/comments,https://api.github.com/repos/ollama/ollama/issues/13180/events,https://github.com/ollama/ollama/pull/13180,3649142793,PR_kwDOJ0Z1Ps60sV1Y,13180,Deepseek v3 family renderer,"{'login': 'gr4ceG', 'id': 88872231, 'node_id': 'MDQ6VXNlcjg4ODcyMjMx', 'avatar_url': 'https://avatars.githubusercontent.com/u/88872231?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/gr4ceG', 'html_url': 'https://github.com/gr4ceG', 'followers_url': 'https://api.github.com/users/gr4ceG/followers', 'following_url': 'https://api.github.com/users/gr4ceG/following{/other_user}', 'gists_url': 'https://api.github.com/users/gr4ceG/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/gr4ceG/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/gr4ceG/subscriptions', 'organizations_url': 'https://api.github.com/users/gr4ceG/orgs', 'repos_url': 'https://api.github.com/users/gr4ceG/repos', 'events_url': 'https://api.github.com/users/gr4ceG/events{/privacy}', 'received_events_url': 'https://api.github.com/users/gr4ceG/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}",[],open,False,,[],,0,2025-11-20T21:23:19Z,2025-11-20T21:58:39Z,,CONTRIBUTOR,,,,,,,"{'url': 'https://api.github.com/repos/ollama/ollama/issues/13180/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}",https://api.github.com/repos/ollama/ollama/issues/13180/timeline,,,False,"{'url': 'https://api.github.com/repos/ollama/ollama/pulls/13180', 'html_url': 'https://github.com/ollama/ollama/pull/13180', 'diff_url': 'https://github.com/ollama/ollama/pull/13180.diff', 'patch_url': 'https://github.com/ollama/ollama/pull/13180.patch', 'merged_at': None}"
https://api.github.com/repos/ollama/ollama/issues/13179,https://api.github.com/repos/ollama/ollama,https://api.github.com/repos/ollama/ollama/issues/13179/labels{/name},https://api.github.com/repos/ollama/ollama/issues/13179/comments,https://api.github.com/repos/ollama/ollama/issues/13179/events,https://github.com/ollama/ollama/pull/13179,3649107560,PR_kwDOJ0Z1Ps60sOBr,13179,app/ui: fix model capabilities not updating after download completion,"{'login': 'hoyyeva', 'id': 63033505, 'node_id': 'MDQ6VXNlcjYzMDMzNTA1', 'avatar_url': 'https://avatars.githubusercontent.com/u/63033505?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/hoyyeva', 'html_url': 'https://github.com/hoyyeva', 'followers_url': 'https://api.github.com/users/hoyyeva/followers', 'following_url': 'https://api.github.com/users/hoyyeva/following{/other_user}', 'gists_url': 'https://api.github.com/users/hoyyeva/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/hoyyeva/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/hoyyeva/subscriptions', 'organizations_url': 'https://api.github.com/users/hoyyeva/orgs', 'repos_url': 'https://api.github.com/users/hoyyeva/repos', 'events_url': 'https://api.github.com/users/hoyyeva/events{/privacy}', 'received_events_url': 'https://api.github.com/users/hoyyeva/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}",[],open,False,,[],,0,2025-11-20T21:13:03Z,2025-11-20T21:41:46Z,,CONTRIBUTOR,,,,,"This PR partially fixes #12950.

Model capabilities (such as vision support) were not being detected after a model finished downloading during chat. The /api/show endpoint was never called after the download completed, so the UI continued to treat the model as lacking capabilities, resulting in incorrect error messages — for example, indicating that a vision model does not support image input.

This PR adds capability-fetching logic to the chat download completion handler to ensure that model capabilities are properly available after the download.",,"{'url': 'https://api.github.com/repos/ollama/ollama/issues/13179/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}",https://api.github.com/repos/ollama/ollama/issues/13179/timeline,,,False,"{'url': 'https://api.github.com/repos/ollama/ollama/pulls/13179', 'html_url': 'https://github.com/ollama/ollama/pull/13179', 'diff_url': 'https://github.com/ollama/ollama/pull/13179.diff', 'patch_url': 'https://github.com/ollama/ollama/pull/13179.patch', 'merged_at': None}"
https://api.github.com/repos/ollama/ollama/issues/13178,https://api.github.com/repos/ollama/ollama,https://api.github.com/repos/ollama/ollama/issues/13178/labels{/name},https://api.github.com/repos/ollama/ollama/issues/13178/comments,https://api.github.com/repos/ollama/ollama/issues/13178/events,https://github.com/ollama/ollama/issues/13178,3648991074,I_kwDOJ0Z1Ps7Zfy9i,13178,Install script overwrites systemd config,"{'login': 'cwthomas-llu', 'id': 4316465, 'node_id': 'MDQ6VXNlcjQzMTY0NjU=', 'avatar_url': 'https://avatars.githubusercontent.com/u/4316465?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/cwthomas-llu', 'html_url': 'https://github.com/cwthomas-llu', 'followers_url': 'https://api.github.com/users/cwthomas-llu/followers', 'following_url': 'https://api.github.com/users/cwthomas-llu/following{/other_user}', 'gists_url': 'https://api.github.com/users/cwthomas-llu/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/cwthomas-llu/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/cwthomas-llu/subscriptions', 'organizations_url': 'https://api.github.com/users/cwthomas-llu/orgs', 'repos_url': 'https://api.github.com/users/cwthomas-llu/repos', 'events_url': 'https://api.github.com/users/cwthomas-llu/events{/privacy}', 'received_events_url': 'https://api.github.com/users/cwthomas-llu/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}","[{'id': 5667396184, 'node_id': 'LA_kwDOJ0Z1Ps8AAAABUc2aWA', 'url': 'https://api.github.com/repos/ollama/ollama/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': ""Something isn't working""}]",open,False,,[],,2,2025-11-20T20:36:02Z,2025-11-21T20:24:05Z,,NONE,,,"{'total': 0, 'completed': 0, 'percent_completed': 0}","{'blocked_by': 0, 'total_blocked_by': 0, 'blocking': 0, 'total_blocking': 0}","### What is the issue?

To make Ollama work in our environment, we had to modify the systemd script `/etc/systemd/system/ollama.service`. The Ollama documentation states that to update Ollama, just re-run the install script. We did that and it overwrote our existing systemd script. Then, Ollama stopped working in our environment.

I think the install script should skip copying the default systemd script, if one already exists.

### Relevant log output

```shell

```

### OS

_No response_

### GPU

_No response_

### CPU

_No response_

### Ollama version

_No response_",,"{'url': 'https://api.github.com/repos/ollama/ollama/issues/13178/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}",https://api.github.com/repos/ollama/ollama/issues/13178/timeline,,,,
https://api.github.com/repos/ollama/ollama/issues/13177,https://api.github.com/repos/ollama/ollama,https://api.github.com/repos/ollama/ollama/issues/13177/labels{/name},https://api.github.com/repos/ollama/ollama/issues/13177/comments,https://api.github.com/repos/ollama/ollama/issues/13177/events,https://github.com/ollama/ollama/issues/13177,3648868490,I_kwDOJ0Z1Ps7ZfVCK,13177,deepseek-ocr :: Not able to run this model,"{'login': 'amey6992', 'id': 73101803, 'node_id': 'MDQ6VXNlcjczMTAxODAz', 'avatar_url': 'https://avatars.githubusercontent.com/u/73101803?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/amey6992', 'html_url': 'https://github.com/amey6992', 'followers_url': 'https://api.github.com/users/amey6992/followers', 'following_url': 'https://api.github.com/users/amey6992/following{/other_user}', 'gists_url': 'https://api.github.com/users/amey6992/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/amey6992/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/amey6992/subscriptions', 'organizations_url': 'https://api.github.com/users/amey6992/orgs', 'repos_url': 'https://api.github.com/users/amey6992/repos', 'events_url': 'https://api.github.com/users/amey6992/events{/privacy}', 'received_events_url': 'https://api.github.com/users/amey6992/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}","[{'id': 5667396184, 'node_id': 'LA_kwDOJ0Z1Ps8AAAABUc2aWA', 'url': 'https://api.github.com/repos/ollama/ollama/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': ""Something isn't working""}]",closed,False,,[],,2,2025-11-20T20:04:17Z,2025-11-20T20:23:52Z,2025-11-20T20:22:08Z,NONE,,,"{'total': 0, 'completed': 0, 'percent_completed': 0}","{'blocked_by': 0, 'total_blocked_by': 0, 'blocking': 0, 'total_blocking': 0}","### What is the issue?

### Issue with deepseek-ocr ###

- I did pull the model
- When I run it, I get an error
- Error: 500 Internal Server Error: unable to load model: /home/.ollama/models/blobs/sha256-3a18673ff291a1d8de94d490877127899356d33a18028d5f3945bf245c11b02c


<img width=""1854"" height=""847"" alt=""Image"" src=""https://github.com/user-attachments/assets/e16dcd79-215a-46ec-8058-fedc6a3fd09f"" />

free memory is about 160Gbs, the other model llama3.2 worked well

### Relevant log output

```shell
llama_model_loader: loaded meta data with 33 key-value pairs and 631 tensors from /home/.ollama/models/blobs/sha256-3a18673ff291a1d8de94d490877127899356d33a18028d5f3945bf245c11b02c (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:           deepseekocr.attention.head_count u32              = 10
llama_model_loader: - kv   1:        deepseekocr.attention.head_count_kv u32              = 10
llama_model_loader: - kv   2:                    deepseekocr.block_count u32              = 12
llama_model_loader: - kv   3:                 deepseekocr.context_length u32              = 8192
llama_model_loader: - kv   4:               deepseekocr.embedding_length u32              = 1280
llama_model_loader: - kv   5:                   deepseekocr.expert_count u32              = 64
llama_model_loader: - kv   6:              deepseekocr.expert_used_count u32              = 6
llama_model_loader: - kv   7:            deepseekocr.feed_forward_length u32              = 6848
llama_model_loader: - kv   8:                       general.architecture str              = deepseekocr
llama_model_loader: - kv   9:                          general.file_type u32              = 1
llama_model_loader: - kv  10:               general.quantization_version u32              = 2
llama_model_loader: - kv  11:      deepseekocr.leading_dense_block_count u32              = 1
llama_model_loader: - kv  12:                deepseekocr.sam.block_count u32              = 12
llama_model_loader: - kv  13:           deepseekocr.sam.embedding_length u32              = 768
llama_model_loader: - kv  14:   deepseekocr.sam.global_attention_indexes arr[i32,4]       = [2, 5, 8, 11]
llama_model_loader: - kv  15:                 deepseekocr.sam.head_count u32              = 12
llama_model_loader: - kv  16:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  17:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  18:           tokenizer.ggml.add_padding_token bool             = false
llama_model_loader: - kv  19:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 1
llama_model_loader: - kv  21:                      tokenizer.ggml.merges arr[str,127741]  = [""Ġ t"", ""Ġ a"", ""i n"", ""Ġ Ġ"", ""h e...
llama_model_loader: - kv  22:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = default
llama_model_loader: - kv  25:                      tokenizer.ggml.scores arr[f32,129280]  = [0.000000, 1.000000, 2.000000, 3.0000...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,129280]  = [3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.tokens arr[str,129280]  = [""<｜begin▁of▁sentence｜>"", ""<�...
llama_model_loader: - kv  28:             deepseekocr.vision.block_count u32              = 24
llama_model_loader: - kv  29:        deepseekocr.vision.embedding_length u32              = 1024
llama_model_loader: - kv  30:              deepseekocr.vision.head_count u32              = 16
llama_model_loader: - kv  31:              deepseekocr.vision.image_size u32              = 224
llama_model_loader: - kv  32:              deepseekocr.vision.patch_size u32              = 14
llama_model_loader: - type  f32:  360 tensors
llama_model_loader: - type  f16:  151 tensors
llama_model_loader: - type bf16:  120 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = F16
print_info: file size   = 6.22 GiB (16.02 BPW) 
llama_model_load: error loading model: error loading model architecture: unknown model architecture: 'deepseekocr'
llama_model_load_from_file_impl: failed to load model
time=2025-11-21T01:23:08.526+05:30 level=INFO source=sched.go:425 msg=""NewLlamaServer failed"" model=/home/.ollama/models/blobs/sha256-3a18673ff291a1d8de94d490877127899356d33a18028d5f3945bf245c11b02c error=""unable to load model: /home/plus91/.ollama/models/blobs/sha256-3a18673ff291a1d8de94d490877127899356d33a18028d5f3945bf245c11b02c""
```

### OS

Linux

### GPU

_No response_

### CPU

Intel

### Ollama version

0.12.11","{'login': 'rick-github', 'id': 14946854, 'node_id': 'MDQ6VXNlcjE0OTQ2ODU0', 'avatar_url': 'https://avatars.githubusercontent.com/u/14946854?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/rick-github', 'html_url': 'https://github.com/rick-github', 'followers_url': 'https://api.github.com/users/rick-github/followers', 'following_url': 'https://api.github.com/users/rick-github/following{/other_user}', 'gists_url': 'https://api.github.com/users/rick-github/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/rick-github/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/rick-github/subscriptions', 'organizations_url': 'https://api.github.com/users/rick-github/orgs', 'repos_url': 'https://api.github.com/users/rick-github/repos', 'events_url': 'https://api.github.com/users/rick-github/events{/privacy}', 'received_events_url': 'https://api.github.com/users/rick-github/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}","{'url': 'https://api.github.com/repos/ollama/ollama/issues/13177/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}",https://api.github.com/repos/ollama/ollama/issues/13177/timeline,,completed,,
https://api.github.com/repos/ollama/ollama/issues/13176,https://api.github.com/repos/ollama/ollama,https://api.github.com/repos/ollama/ollama/issues/13176/labels{/name},https://api.github.com/repos/ollama/ollama/issues/13176/comments,https://api.github.com/repos/ollama/ollama/issues/13176/events,https://github.com/ollama/ollama/pull/13176,3648302548,PR_kwDOJ0Z1Ps60pbRl,13176,discovery: fix cuda overlap case,"{'login': 'dhiltgen', 'id': 4033016, 'node_id': 'MDQ6VXNlcjQwMzMwMTY=', 'avatar_url': 'https://avatars.githubusercontent.com/u/4033016?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/dhiltgen', 'html_url': 'https://github.com/dhiltgen', 'followers_url': 'https://api.github.com/users/dhiltgen/followers', 'following_url': 'https://api.github.com/users/dhiltgen/following{/other_user}', 'gists_url': 'https://api.github.com/users/dhiltgen/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/dhiltgen/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/dhiltgen/subscriptions', 'organizations_url': 'https://api.github.com/users/dhiltgen/orgs', 'repos_url': 'https://api.github.com/users/dhiltgen/repos', 'events_url': 'https://api.github.com/users/dhiltgen/events{/privacy}', 'received_events_url': 'https://api.github.com/users/dhiltgen/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}",[],closed,False,,[],,0,2025-11-20T17:25:01Z,2025-11-20T20:15:40Z,2025-11-20T20:15:38Z,COLLABORATOR,,,,,"Recent refactoring introduced a regression for filtering cuda overlap to favor newest supported version.

In the context of NVIDIA Jetson Thor, both v12 and v13 enumerate the GPU, but v12 doesn't actually work at runtime.

Fixes #13033 ","{'login': 'dhiltgen', 'id': 4033016, 'node_id': 'MDQ6VXNlcjQwMzMwMTY=', 'avatar_url': 'https://avatars.githubusercontent.com/u/4033016?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/dhiltgen', 'html_url': 'https://github.com/dhiltgen', 'followers_url': 'https://api.github.com/users/dhiltgen/followers', 'following_url': 'https://api.github.com/users/dhiltgen/following{/other_user}', 'gists_url': 'https://api.github.com/users/dhiltgen/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/dhiltgen/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/dhiltgen/subscriptions', 'organizations_url': 'https://api.github.com/users/dhiltgen/orgs', 'repos_url': 'https://api.github.com/users/dhiltgen/repos', 'events_url': 'https://api.github.com/users/dhiltgen/events{/privacy}', 'received_events_url': 'https://api.github.com/users/dhiltgen/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}","{'url': 'https://api.github.com/repos/ollama/ollama/issues/13176/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}",https://api.github.com/repos/ollama/ollama/issues/13176/timeline,,,False,"{'url': 'https://api.github.com/repos/ollama/ollama/pulls/13176', 'html_url': 'https://github.com/ollama/ollama/pull/13176', 'diff_url': 'https://github.com/ollama/ollama/pull/13176.diff', 'patch_url': 'https://github.com/ollama/ollama/pull/13176.patch', 'merged_at': '2025-11-20T20:15:38Z'}"
https://api.github.com/repos/ollama/ollama/issues/13175,https://api.github.com/repos/ollama/ollama,https://api.github.com/repos/ollama/ollama/issues/13175/labels{/name},https://api.github.com/repos/ollama/ollama/issues/13175/comments,https://api.github.com/repos/ollama/ollama/issues/13175/events,https://github.com/ollama/ollama/issues/13175,3648273022,I_kwDOJ0Z1Ps7ZdDp-,13175,Add support for reasoning_content in the OpenAI compliant API,"{'login': 'avfulton', 'id': 181163350, 'node_id': 'U_kgDOCsxVVg', 'avatar_url': 'https://avatars.githubusercontent.com/u/181163350?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/avfulton', 'html_url': 'https://github.com/avfulton', 'followers_url': 'https://api.github.com/users/avfulton/followers', 'following_url': 'https://api.github.com/users/avfulton/following{/other_user}', 'gists_url': 'https://api.github.com/users/avfulton/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/avfulton/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/avfulton/subscriptions', 'organizations_url': 'https://api.github.com/users/avfulton/orgs', 'repos_url': 'https://api.github.com/users/avfulton/repos', 'events_url': 'https://api.github.com/users/avfulton/events{/privacy}', 'received_events_url': 'https://api.github.com/users/avfulton/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}","[{'id': 5667396200, 'node_id': 'LA_kwDOJ0Z1Ps8AAAABUc2aaA', 'url': 'https://api.github.com/repos/ollama/ollama/labels/feature%20request', 'name': 'feature request', 'color': 'a2eeef', 'default': False, 'description': 'New feature or request'}]",open,False,,[],,0,2025-11-20T17:17:30Z,2025-11-20T17:17:30Z,,NONE,,,"{'total': 0, 'completed': 0, 'percent_completed': 0}","{'blocked_by': 0, 'total_blocked_by': 0, 'blocking': 0, 'total_blocking': 0}","It would be useful to add a response format parameter for reasoning models that allows toggling between ""reasoning"" and ""reasoning_content"" in the OpenAI style API response.",,"{'url': 'https://api.github.com/repos/ollama/ollama/issues/13175/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}",https://api.github.com/repos/ollama/ollama/issues/13175/timeline,,,,
https://api.github.com/repos/ollama/ollama/issues/13174,https://api.github.com/repos/ollama/ollama,https://api.github.com/repos/ollama/ollama/issues/13174/labels{/name},https://api.github.com/repos/ollama/ollama/issues/13174/comments,https://api.github.com/repos/ollama/ollama/issues/13174/events,https://github.com/ollama/ollama/pull/13174,3647929348,PR_kwDOJ0Z1Ps60oPUE,13174,app/cmd: update ollama help to navigate to ollama doc,"{'login': 'hoyyeva', 'id': 63033505, 'node_id': 'MDQ6VXNlcjYzMDMzNTA1', 'avatar_url': 'https://avatars.githubusercontent.com/u/63033505?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/hoyyeva', 'html_url': 'https://github.com/hoyyeva', 'followers_url': 'https://api.github.com/users/hoyyeva/followers', 'following_url': 'https://api.github.com/users/hoyyeva/following{/other_user}', 'gists_url': 'https://api.github.com/users/hoyyeva/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/hoyyeva/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/hoyyeva/subscriptions', 'organizations_url': 'https://api.github.com/users/hoyyeva/orgs', 'repos_url': 'https://api.github.com/users/hoyyeva/repos', 'events_url': 'https://api.github.com/users/hoyyeva/events{/privacy}', 'received_events_url': 'https://api.github.com/users/hoyyeva/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}",[],closed,False,,[],,0,2025-11-20T15:59:05Z,2025-11-20T21:30:37Z,2025-11-20T21:30:35Z,CONTRIBUTOR,,,,,,"{'login': 'hoyyeva', 'id': 63033505, 'node_id': 'MDQ6VXNlcjYzMDMzNTA1', 'avatar_url': 'https://avatars.githubusercontent.com/u/63033505?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/hoyyeva', 'html_url': 'https://github.com/hoyyeva', 'followers_url': 'https://api.github.com/users/hoyyeva/followers', 'following_url': 'https://api.github.com/users/hoyyeva/following{/other_user}', 'gists_url': 'https://api.github.com/users/hoyyeva/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/hoyyeva/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/hoyyeva/subscriptions', 'organizations_url': 'https://api.github.com/users/hoyyeva/orgs', 'repos_url': 'https://api.github.com/users/hoyyeva/repos', 'events_url': 'https://api.github.com/users/hoyyeva/events{/privacy}', 'received_events_url': 'https://api.github.com/users/hoyyeva/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}","{'url': 'https://api.github.com/repos/ollama/ollama/issues/13174/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}",https://api.github.com/repos/ollama/ollama/issues/13174/timeline,,,False,"{'url': 'https://api.github.com/repos/ollama/ollama/pulls/13174', 'html_url': 'https://github.com/ollama/ollama/pull/13174', 'diff_url': 'https://github.com/ollama/ollama/pull/13174.diff', 'patch_url': 'https://github.com/ollama/ollama/pull/13174.patch', 'merged_at': '2025-11-20T21:30:35Z'}"
https://api.github.com/repos/ollama/ollama/issues/13173,https://api.github.com/repos/ollama/ollama,https://api.github.com/repos/ollama/ollama/issues/13173/labels{/name},https://api.github.com/repos/ollama/ollama/issues/13173/comments,https://api.github.com/repos/ollama/ollama/issues/13173/events,https://github.com/ollama/ollama/issues/13173,3647925885,I_kwDOJ0Z1Ps7Zbu59,13173,New AMD memory detection routines ignores unified memory on AMD APU,"{'login': 'rjmalagon', 'id': 13302853, 'node_id': 'MDQ6VXNlcjEzMzAyODUz', 'avatar_url': 'https://avatars.githubusercontent.com/u/13302853?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/rjmalagon', 'html_url': 'https://github.com/rjmalagon', 'followers_url': 'https://api.github.com/users/rjmalagon/followers', 'following_url': 'https://api.github.com/users/rjmalagon/following{/other_user}', 'gists_url': 'https://api.github.com/users/rjmalagon/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/rjmalagon/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/rjmalagon/subscriptions', 'organizations_url': 'https://api.github.com/users/rjmalagon/orgs', 'repos_url': 'https://api.github.com/users/rjmalagon/repos', 'events_url': 'https://api.github.com/users/rjmalagon/events{/privacy}', 'received_events_url': 'https://api.github.com/users/rjmalagon/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}","[{'id': 5667396184, 'node_id': 'LA_kwDOJ0Z1Ps8AAAABUc2aWA', 'url': 'https://api.github.com/repos/ollama/ollama/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': ""Something isn't working""}, {'id': 6433346500, 'node_id': 'LA_kwDOJ0Z1Ps8AAAABf3UTxA', 'url': 'https://api.github.com/repos/ollama/ollama/labels/amd', 'name': 'amd', 'color': '000000', 'default': False, 'description': 'Issues relating to AMD GPUs and ROCm'}]",open,False,,[],,7,2025-11-20T15:58:11Z,2025-11-24T16:43:47Z,,NONE,,,"{'total': 0, 'completed': 0, 'percent_completed': 0}","{'blocked_by': 0, 'total_blocked_by': 0, 'blocking': 0, 'total_blocking': 0}","### What is the issue?

After https://github.com/ollama/ollama/pull/12871 , Ollama counts strict VRAM on AMD APUs even when the unified RAM is used by ROCM and Vulkan runtimes. Tested on Radenon 680M (gfx1030) on ROCM 6.4.4 

### Relevant log output

```shell
before patches
´´´
time=2025-11-20T15:56:06.961Z level=DEBUG source=runner.go:175 msg=""adjusting filtering IDs"" FilterID=0 new_ID=0
time=2025-11-20T15:56:06.961Z level=DEBUG source=runner.go:40 msg=""GPU bootstrap discovery took"" duration=6.603711351s
time=2025-11-20T15:56:06.961Z level=INFO source=types.go:42 msg=""inference compute"" id=0 filter_id=0 library=ROCm compute=gfx1030 name=ROCm0 description=""AMD Radeon 680M"" libdirs=ollama,rocm_v6 driver=60443.48 pci_id=0000:e7:00.0 type=iGPU total=""96.0 GiB"" available=""90.6 GiB""
´´´
after patches
´´´
time=2025-11-20T15:45:28.686Z level=INFO source=types.go:42 msg=""inference compute"" id=0 filter_id=0 library=ROCm compute=gfx1030 name=ROCm0 description=""AMD Radeon 680M"" libdirs=ollama,rocm_v6 driver=60443.48 pci_id=0000:e7:00.0 type=iGPU total=""512.0 MiB"" available=""496.1 MiB""
time=2025-11-20T15:45:28.686Z level=INFO source=routes.go:1638 msg=""entering low vram mode"" ""total vram""=""512.0 MiB"" threshold=""20.0 GiB""
´´´
```

### OS

_No response_

### GPU

_No response_

### CPU

_No response_

### Ollama version

_No response_",,"{'url': 'https://api.github.com/repos/ollama/ollama/issues/13173/reactions', 'total_count': 4, '+1': 4, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}",https://api.github.com/repos/ollama/ollama/issues/13173/timeline,,,,
https://api.github.com/repos/ollama/ollama/issues/13172,https://api.github.com/repos/ollama/ollama,https://api.github.com/repos/ollama/ollama/issues/13172/labels{/name},https://api.github.com/repos/ollama/ollama/issues/13172/comments,https://api.github.com/repos/ollama/ollama/issues/13172/events,https://github.com/ollama/ollama/issues/13172,3647731519,I_kwDOJ0Z1Ps7Za_c_,13172,Version stuck at 0.13.0,"{'login': 'CraftingBuilds', 'id': 214755301, 'node_id': 'U_kgDODMzn5Q', 'avatar_url': 'https://avatars.githubusercontent.com/u/214755301?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/CraftingBuilds', 'html_url': 'https://github.com/CraftingBuilds', 'followers_url': 'https://api.github.com/users/CraftingBuilds/followers', 'following_url': 'https://api.github.com/users/CraftingBuilds/following{/other_user}', 'gists_url': 'https://api.github.com/users/CraftingBuilds/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/CraftingBuilds/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/CraftingBuilds/subscriptions', 'organizations_url': 'https://api.github.com/users/CraftingBuilds/orgs', 'repos_url': 'https://api.github.com/users/CraftingBuilds/repos', 'events_url': 'https://api.github.com/users/CraftingBuilds/events{/privacy}', 'received_events_url': 'https://api.github.com/users/CraftingBuilds/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}","[{'id': 5667396184, 'node_id': 'LA_kwDOJ0Z1Ps8AAAABUc2aWA', 'url': 'https://api.github.com/repos/ollama/ollama/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': ""Something isn't working""}]",closed,False,,[],,1,2025-11-20T15:12:16Z,2025-11-23T22:22:29Z,2025-11-23T22:22:29Z,NONE,,,"{'total': 0, 'completed': 0, 'percent_completed': 0}","{'blocked_by': 0, 'total_blocked_by': 0, 'blocking': 0, 'total_blocking': 0}","### What is the issue?

I am unable to install a version any newer than 0.13.0 even with the install script or manual install...

### Relevant log output

```shell
Konsole KDE
```

### OS
Ubuntu KDE 24.4.0
_No response_

### GPU
no
_No response_

### CPU
yes
_No response_

### Ollama version
0.13.0
_No response_","{'login': 'rick-github', 'id': 14946854, 'node_id': 'MDQ6VXNlcjE0OTQ2ODU0', 'avatar_url': 'https://avatars.githubusercontent.com/u/14946854?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/rick-github', 'html_url': 'https://github.com/rick-github', 'followers_url': 'https://api.github.com/users/rick-github/followers', 'following_url': 'https://api.github.com/users/rick-github/following{/other_user}', 'gists_url': 'https://api.github.com/users/rick-github/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/rick-github/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/rick-github/subscriptions', 'organizations_url': 'https://api.github.com/users/rick-github/orgs', 'repos_url': 'https://api.github.com/users/rick-github/repos', 'events_url': 'https://api.github.com/users/rick-github/events{/privacy}', 'received_events_url': 'https://api.github.com/users/rick-github/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}","{'url': 'https://api.github.com/repos/ollama/ollama/issues/13172/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}",https://api.github.com/repos/ollama/ollama/issues/13172/timeline,,completed,,
https://api.github.com/repos/ollama/ollama/issues/13171,https://api.github.com/repos/ollama/ollama,https://api.github.com/repos/ollama/ollama/issues/13171/labels{/name},https://api.github.com/repos/ollama/ollama/issues/13171/comments,https://api.github.com/repos/ollama/ollama/issues/13171/events,https://github.com/ollama/ollama/issues/13171,3645633042,I_kwDOJ0Z1Ps7ZS_IS,13171,Error: 500 Internal Server Error: llama runner process has terminated: exit status 2,"{'login': 'MapleZJH', 'id': 34159292, 'node_id': 'MDQ6VXNlcjM0MTU5Mjky', 'avatar_url': 'https://avatars.githubusercontent.com/u/34159292?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/MapleZJH', 'html_url': 'https://github.com/MapleZJH', 'followers_url': 'https://api.github.com/users/MapleZJH/followers', 'following_url': 'https://api.github.com/users/MapleZJH/following{/other_user}', 'gists_url': 'https://api.github.com/users/MapleZJH/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/MapleZJH/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/MapleZJH/subscriptions', 'organizations_url': 'https://api.github.com/users/MapleZJH/orgs', 'repos_url': 'https://api.github.com/users/MapleZJH/repos', 'events_url': 'https://api.github.com/users/MapleZJH/events{/privacy}', 'received_events_url': 'https://api.github.com/users/MapleZJH/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}","[{'id': 5667396184, 'node_id': 'LA_kwDOJ0Z1Ps8AAAABUc2aWA', 'url': 'https://api.github.com/repos/ollama/ollama/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': ""Something isn't working""}]",closed,False,,[],,0,2025-11-20T06:05:54Z,2025-11-21T01:36:32Z,2025-11-21T01:36:32Z,NONE,,,"{'total': 0, 'completed': 0, 'percent_completed': 0}","{'blocked_by': 0, 'total_blocked_by': 0, 'blocking': 0, 'total_blocking': 0}","### What is the issue?

When I choose Airplane mode want to keep my model local, I face this problem:
cmd: ollama run deepseek-r1:32b
Error: 500 Internal Server Error: llama runner process has terminated: exit status 2

I using deepseek-r1:32b and deepseek-r1:70b models, they have been downloaded local already.
The setting of ollama are as follows:
1. Expose ollama to the network: off;
2. Airplane mode: on;
3. system: OLLAMA_HOST: 127.0.0.1;
4. system: OLLAMA_MODEL: D:\LLM\Models_ollama (which is where my models locate)

Ollama version: 0.13.0
CPU: AMD RYZEN AI MAX+ PRO 395w
GPU: Radeon 8060s
GPU MEMORY: 96.0GB

### Relevant log output

```shell
time=2025-11-20T14:01:57.543+08:00 level=INFO source=server.go:1328 msg=""waiting for server to become available"" status=""llm server not responding""
time=2025-11-20T14:01:57.794+08:00 level=INFO source=server.go:1328 msg=""waiting for server to become available"" status=""llm server error""
time=2025-11-20T14:02:00.557+08:00 level=INFO source=sched.go:470 msg=""Load failed"" model=D:\LLM\Models_ollama\blobs\sha256-6150cb382311b69f09cc0f9a1b69fc029cbd742b66bb8ec531aa5ecf5c613e93 error=""llama runner process has terminated: exit status 2""
[GIN] 2025/11/20 - 14:02:00 | 500 |    49.853006s |       127.0.0.1 | POST     ""/api/generate""


time=2025-11-20T13:53:41.540+08:00 level=INFO source=routes.go:1544 msg=""server config"" env=""map[CUDA_VISIBLE_DEVICES: GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:32768 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11434 OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:D:\\LLM\\Models_ollama OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false OLLAMA_VULKAN:false ROCR_VISIBLE_DEVICES:]""
time=2025-11-20T13:53:41.566+08:00 level=INFO source=images.go:522 msg=""total blobs: 11""
time=2025-11-20T13:53:41.567+08:00 level=INFO source=images.go:529 msg=""total unused blobs removed: 0""
time=2025-11-20T13:53:41.568+08:00 level=INFO source=routes.go:1597 msg=""Listening on [::]:11434 (version 0.13.0)""
time=2025-11-20T13:53:41.569+08:00 level=INFO source=runner.go:67 msg=""discovering available GPUs...""
time=2025-11-20T13:53:41.569+08:00 level=INFO source=runner.go:102 msg=""experimental Vulkan support disabled.  To enable, set OLLAMA_VULKAN=1""
time=2025-11-20T13:53:41.608+08:00 level=INFO source=server.go:392 msg=""starting runner"" cmd=""C:\\Users\\ZJH\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 58346""
time=2025-11-20T13:53:43.013+08:00 level=INFO source=server.go:392 msg=""starting runner"" cmd=""C:\\Users\\ZJH\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 49825""
time=2025-11-20T13:55:13.013+08:00 level=INFO source=runner.go:449 msg=""failure during GPU discovery"" OLLAMA_LIBRARY_PATH=""[C:\\Users\\ZJH\\AppData\\Local\\Programs\\Ollama\\lib\\ollama C:\\Users\\ZJH\\AppData\\Local\\Programs\\Ollama\\lib\\ollama\\cuda_v13]"" extra_envs=map[] error=""failed to finish discovery before timeout""
time=2025-11-20T13:55:13.031+08:00 level=INFO source=server.go:392 msg=""starting runner"" cmd=""C:\\Users\\ZJH\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 57508""
time=2025-11-20T13:55:14.647+08:00 level=INFO source=server.go:392 msg=""starting runner"" cmd=""C:\\Users\\ZJH\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 57577""
time=2025-11-20T13:55:17.215+08:00 level=INFO source=types.go:42 msg=""inference compute"" id=0 filter_id=0 library=ROCm compute=gfx1151 name=ROCm0 description=""AMD Radeon(TM) 8060S Graphics"" libdirs=ollama,rocm driver=60241.51 pci_id=0000:c3:00.0 type=iGPU total=""96.0 GiB"" available=""94.7 GiB""
[GIN] 2025/11/20 - 13:55:17 | 200 |            0s |       127.0.0.1 | GET      ""/api/version""
[GIN] 2025/11/20 - 13:58:28 | 200 |            0s |       127.0.0.1 | GET      ""/api/version""
[GIN] 2025/11/20 - 13:58:39 | 200 |            0s |       127.0.0.1 | GET      ""/api/version""
[GIN] 2025/11/20 - 13:58:48 | 200 |            0s |       127.0.0.1 | HEAD     ""/""
[GIN] 2025/11/20 - 13:58:48 | 200 |      1.6596ms |       127.0.0.1 | GET      ""/api/tags""
[GIN] 2025/11/20 - 14:01:10 | 200 |            0s |       127.0.0.1 | HEAD     ""/""
[GIN] 2025/11/20 - 14:01:10 | 200 |     95.0704ms |       127.0.0.1 | POST     ""/api/show""
[GIN] 2025/11/20 - 14:01:10 | 200 |     83.4468ms |       127.0.0.1 | POST     ""/api/show""
time=2025-11-20T14:01:10.908+08:00 level=INFO source=server.go:392 msg=""starting runner"" cmd=""C:\\Users\\ZJH\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 62280""
time=2025-11-20T14:01:11.828+08:00 level=INFO source=cpu_windows.go:148 msg=packages count=1
time=2025-11-20T14:01:11.829+08:00 level=INFO source=cpu_windows.go:195 msg="""" package=0 cores=16 efficiency=0 threads=32
llama_model_loader: loaded meta data with 26 key-value pairs and 771 tensors from D:\LLM\Models_ollama\blobs\sha256-6150cb382311b69f09cc0f9a1b69fc029cbd742b66bb8ec531aa5ecf5c613e93 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Qwen 32B
llama_model_loader: - kv   3:                           general.basename str              = DeepSeek-R1-Distill-Qwen
llama_model_loader: - kv   4:                         general.size_label str              = 32B
llama_model_loader: - kv   5:                          qwen2.block_count u32              = 64
llama_model_loader: - kv   6:                       qwen2.context_length u32              = 131072
llama_model_loader: - kv   7:                     qwen2.embedding_length u32              = 5120
llama_model_loader: - kv   8:                  qwen2.feed_forward_length u32              = 27648
llama_model_loader: - kv   9:                 qwen2.attention.head_count u32              = 40
llama_model_loader: - kv  10:              qwen2.attention.head_count_kv u32              = 8
llama_model_loader: - kv  11:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  12:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                          general.file_type u32              = 15
llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = deepseek-r1-qwen
llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,152064]  = [""!"", ""\"""", ""#"", ""$"", ""%"", ""&"", ""'"", ...
llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  18:                      tokenizer.ggml.merges arr[str,151387]  = [""Ġ Ġ"", ""ĠĠ ĠĠ"", ""i n"", ""Ġ t"",...
llama_model_loader: - kv  19:                tokenizer.ggml.bos_token_id u32              = 151646
llama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 151643
llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  23:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
llama_model_loader: - kv  25:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  321 tensors
llama_model_loader: - type q4_K:  385 tensors
llama_model_loader: - type q6_K:   65 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 18.48 GiB (4.85 BPW) 
load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
load: printing all EOG tokens:
load:   - 151643 ('<｜end▁of▁sentence｜>')
load:   - 151662 ('<|fim_pad|>')
load:   - 151663 ('<|repo_name|>')
load:   - 151664 ('<|file_sep|>')
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 32.76 B
print_info: general.name     = DeepSeek R1 Distill Qwen 32B
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151646 '<｜begin▁of▁sentence｜>'
print_info: EOS token        = 151643 '<｜end▁of▁sentence｜>'
print_info: EOT token        = 151643 '<｜end▁of▁sentence｜>'
print_info: PAD token        = 151643 '<｜end▁of▁sentence｜>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<｜end▁of▁sentence｜>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-11-20T14:01:12.167+08:00 level=INFO source=server.go:392 msg=""starting runner"" cmd=""C:\\Users\\ZJH\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --model D:\\LLM\\Models_ollama\\blobs\\sha256-6150cb382311b69f09cc0f9a1b69fc029cbd742b66bb8ec531aa5ecf5c613e93 --port 62302""
time=2025-11-20T14:01:12.195+08:00 level=INFO source=sched.go:443 msg=""system memory"" total=""31.8 GiB"" free=""20.9 GiB"" free_swap=""15.1 GiB""
time=2025-11-20T14:01:12.195+08:00 level=INFO source=sched.go:450 msg=""gpu memory"" id=0 library=ROCm available=""94.1 GiB"" free=""94.6 GiB"" minimum=""457.0 MiB"" overhead=""0 B""
time=2025-11-20T14:01:12.195+08:00 level=INFO source=server.go:459 msg=""loading model"" ""model layers""=65 requested=-1
time=2025-11-20T14:01:12.196+08:00 level=INFO source=device.go:240 msg=""model weights"" device=ROCm0 size=""18.1 GiB""
time=2025-11-20T14:01:12.196+08:00 level=INFO source=device.go:251 msg=""kv cache"" device=ROCm0 size=""8.0 GiB""
time=2025-11-20T14:01:12.196+08:00 level=INFO source=device.go:262 msg=""compute graph"" device=ROCm0 size=""2.6 GiB""
time=2025-11-20T14:01:12.196+08:00 level=INFO source=device.go:272 msg=""total memory"" size=""28.7 GiB""
time=2025-11-20T14:01:12.822+08:00 level=INFO source=runner.go:963 msg=""starting go runner""
load_backend: loaded CPU backend from C:\Users\ZJH\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-icelake.dll
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 ROCm devices:
  Device 0: AMD Radeon(TM) 8060S Graphics, gfx1151 (0x1151), VMM: no, Wave Size: 32, ID: 0
load_backend: loaded ROCm backend from C:\Users\ZJH\AppData\Local\Programs\Ollama\lib\ollama\rocm\ggml-hip.dll
time=2025-11-20T14:01:12.927+08:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 ROCm.0.NO_VMM=1 ROCm.0.NO_PEER_COPY=1 ROCm.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2025-11-20T14:01:12.929+08:00 level=INFO source=runner.go:999 msg=""Server listening on 127.0.0.1:62302""
time=2025-11-20T14:01:12.941+08:00 level=INFO source=runner.go:893 msg=load request=""{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:false KvSize:32768 KvCacheType: NumThreads:16 GPULayers:65[ID:0 Layers:65(0..64)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:true}""
time=2025-11-20T14:01:12.941+08:00 level=INFO source=server.go:1294 msg=""waiting for llama runner to start responding""
time=2025-11-20T14:01:12.941+08:00 level=INFO source=server.go:1328 msg=""waiting for server to become available"" status=""llm server loading model""
ggml_hip_mgmt_init located ADLX version 1.3
ggml_backend_cuda_device_get_memory device 0000:c3:00.0 utilizing AMD specific memory reporting free: 101567168512 total: 103079215104
llama_model_load_from_file_impl: using device ROCm0 (AMD Radeon(TM) 8060S Graphics) (0000:c3:00.0) - 96862 MiB free
llama_model_loader: loaded meta data with 26 key-value pairs and 771 tensors from D:\LLM\Models_ollama\blobs\sha256-6150cb382311b69f09cc0f9a1b69fc029cbd742b66bb8ec531aa5ecf5c613e93 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Qwen 32B
llama_model_loader: - kv   3:                           general.basename str              = DeepSeek-R1-Distill-Qwen
llama_model_loader: - kv   4:                         general.size_label str              = 32B
llama_model_loader: - kv   5:                          qwen2.block_count u32              = 64
llama_model_loader: - kv   6:                       qwen2.context_length u32              = 131072
llama_model_loader: - kv   7:                     qwen2.embedding_length u32              = 5120
llama_model_loader: - kv   8:                  qwen2.feed_forward_length u32              = 27648
llama_model_loader: - kv   9:                 qwen2.attention.head_count u32              = 40
llama_model_loader: - kv  10:              qwen2.attention.head_count_kv u32              = 8
llama_model_loader: - kv  11:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  12:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                          general.file_type u32              = 15
llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = deepseek-r1-qwen
llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,152064]  = [""!"", ""\"""", ""#"", ""$"", ""%"", ""&"", ""'"", ...
llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  18:                      tokenizer.ggml.merges arr[str,151387]  = [""Ġ Ġ"", ""ĠĠ ĠĠ"", ""i n"", ""Ġ t"",...
llama_model_loader: - kv  19:                tokenizer.ggml.bos_token_id u32              = 151646
llama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 151643
llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  23:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
llama_model_loader: - kv  25:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  321 tensors
llama_model_loader: - type q4_K:  385 tensors
llama_model_loader: - type q6_K:   65 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 18.48 GiB (4.85 BPW) 
load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
load: printing all EOG tokens:
load:   - 151643 ('<｜end▁of▁sentence｜>')
load:   - 151662 ('<|fim_pad|>')
load:   - 151663 ('<|repo_name|>')
load:   - 151664 ('<|file_sep|>')
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 5120
print_info: n_layer          = 64
print_info: n_head           = 40
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 5
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 27648
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = -1
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: model type       = 32B
print_info: model params     = 32.76 B
print_info: general.name     = DeepSeek R1 Distill Qwen 32B
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151646 '<｜begin▁of▁sentence｜>'
print_info: EOS token        = 151643 '<｜end▁of▁sentence｜>'
print_info: EOT token        = 151643 '<｜end▁of▁sentence｜>'
print_info: PAD token        = 151643 '<｜end▁of▁sentence｜>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<｜end▁of▁sentence｜>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 64 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 65/65 layers to GPU
load_tensors:        ROCm0 model buffer size = 18508.35 MiB
load_tensors:   CPU_Mapped model buffer size =   417.66 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 32768
llama_context: n_ctx_per_seq = 32768
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = disabled
llama_context: kv_unified    = false
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (32768) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_context:  ROCm_Host  output buffer size =     0.60 MiB
Exception 0xc0000005 0x1 0x10 0x7ffb26419176
PC=0x7ffb26419176
signal arrived during external code execution

runtime.cgocall(0x7ff6052c88e0, 0xc0003e9c00)
	runtime/cgocall.go:167 +0x3e fp=0xc0003e9bd8 sp=0xc0003e9b70 pc=0x7ff60459243e
github.com/ollama/ollama/llama._Cfunc_llama_init_from_model(0x2a6a1db4020, {0x8000, 0x200, 0x200, 0x1, 0x10, 0x10, 0xffffffff, 0xffffffff, 0xffffffff, ...})
	_cgo_gotypes.go:754 +0x54 fp=0xc0003e9c00 sp=0xc0003e9bd8 pc=0x7ff604962d34
github.com/ollama/ollama/llama.NewContextWithModel.func1(...)
	github.com/ollama/ollama/llama/llama.go:317
github.com/ollama/ollama/llama.NewContextWithModel(0xc00037f9f0, {{0x8000, 0x200, 0x200, 0x1, 0x10, 0x10, 0xffffffff, 0xffffffff, 0xffffffff, ...}})
	github.com/ollama/ollama/llama/llama.go:317 +0x158 fp=0xc0003e9da0 sp=0xc0003e9c00 pc=0x7ff6049672f8
github.com/ollama/ollama/runner/llamarunner.(*Server).loadModel(0xc00032e320, {{0xc0003c0f80, 0x1, 0x1}, 0x41, 0x0, 0x1, {0xc0003c0f78, 0x1, 0x2}, ...}, ...)
	github.com/ollama/ollama/runner/llamarunner/runner.go:845 +0x178 fp=0xc0003e9ee8 sp=0xc0003e9da0 pc=0x7ff604a216d8
github.com/ollama/ollama/runner/llamarunner.(*Server).load.gowrap2()
	github.com/ollama/ollama/runner/llamarunner/runner.go:932 +0x115 fp=0xc0003e9fe0 sp=0xc0003e9ee8 pc=0x7ff604a228f5
runtime.goexit({})
	runtime/asm_amd64.s:1700 +0x1 fp=0xc0003e9fe8 sp=0xc0003e9fe0 pc=0x7ff60459d8e1
created by github.com/ollama/ollama/runner/llamarunner.(*Server).load in goroutine 55
	github.com/ollama/ollama/runner/llamarunner/runner.go:932 +0x88a

goroutine 1 gp=0xc0000021c0 m=nil [IO wait]:
runtime.gopark(0x7ff60459f0e0?, 0x7ff60640ca00?, 0x20?, 0xc0?, 0xc0003ac0cc?)
	runtime/proc.go:435 +0xce fp=0xc00058f648 sp=0xc00058f628 pc=0x7ff60459598e
runtime.netpollblock(0x3d4?, 0x4530406?, 0xf6?)
	runtime/netpoll.go:575 +0xf7 fp=0xc00058f680 sp=0xc00058f648 pc=0x7ff60455bdf7
internal/poll.runtime_pollWait(0x2a6fb7ed110, 0x72)
	runtime/netpoll.go:351 +0x85 fp=0xc00058f6a0 sp=0xc00058f680 pc=0x7ff604594b25
internal/poll.(*pollDesc).wait(0x7ff60462a693?, 0x0?, 0x0)
	internal/poll/fd_poll_runtime.go:84 +0x27 fp=0xc00058f6c8 sp=0xc00058f6a0 pc=0x7ff60462bc87
internal/poll.execIO(0xc0003ac020, 0xc00050f770)
	internal/poll/fd_windows.go:177 +0x105 fp=0xc00058f740 sp=0xc00058f6c8 pc=0x7ff60462d0e5
internal/poll.(*FD).acceptOne(0xc0003ac008, 0x3a0, {0xc0003a41e0?, 0xc00050f7d0?, 0x7ff604634da5?}, 0xc00050f804?)
	internal/poll/fd_windows.go:946 +0x65 fp=0xc00058f7a0 sp=0xc00058f740 pc=0x7ff604631665
internal/poll.(*FD).Accept(0xc0003ac008, 0xc00058f950)
	internal/poll/fd_windows.go:980 +0x1b6 fp=0xc00058f858 sp=0xc00058f7a0 pc=0x7ff604631996
net.(*netFD).accept(0xc0003ac008)
	net/fd_windows.go:182 +0x4b fp=0xc00058f970 sp=0xc00058f858 pc=0x7ff6046a2f0b
net.(*TCPListener).accept(0xc000308300)
	net/tcpsock_posix.go:159 +0x1b fp=0xc00058f9c0 sp=0xc00058f970 pc=0x7ff6046b8f5b
net.(*TCPListener).Accept(0xc000308300)
	net/tcpsock.go:380 +0x30 fp=0xc00058f9f0 sp=0xc00058f9c0 pc=0x7ff6046b7d10
net/http.(*onceCloseListener).Accept(0xc0003301b0?)
	<autogenerated>:1 +0x24 fp=0xc00058fa08 sp=0xc00058f9f0 pc=0x7ff6048d1184
net/http.(*Server).Serve(0xc000320700, {0x7ff605a81580, 0xc000308300})
	net/http/server.go:3424 +0x30c fp=0xc00058fb38 sp=0xc00058fa08 pc=0x7ff6048a8a4c
github.com/ollama/ollama/runner/llamarunner.Execute({0xc0000da020, 0x4, 0x6})
	github.com/ollama/ollama/runner/llamarunner/runner.go:1000 +0x8f5 fp=0xc00058fd08 sp=0xc00058fb38 pc=0x7ff604a232b5
github.com/ollama/ollama/runner.Execute({0xc0000da010?, 0x0?, 0x0?})
	github.com/ollama/ollama/runner/runner.go:22 +0xd4 fp=0xc00058fd30 sp=0xc00058fd08 pc=0x7ff604ac9714
github.com/ollama/ollama/cmd.NewCLI.func2(0xc000320300?, {0x7ff60589ad9d?, 0x4?, 0x7ff60589ada1?})
	github.com/ollama/ollama/cmd/cmd.go:1841 +0x45 fp=0xc00058fd58 sp=0xc00058fd30 pc=0x7ff605259145
github.com/spf13/cobra.(*Command).execute(0xc000235508, {0xc0003080c0, 0x4, 0x4})
	github.com/spf13/cobra@v1.7.0/command.go:940 +0x85c fp=0xc00058fe78 sp=0xc00058fd58 pc=0x7ff60471d9dc
github.com/spf13/cobra.(*Command).ExecuteC(0xc000166908)
	github.com/spf13/cobra@v1.7.0/command.go:1068 +0x3a5 fp=0xc00058ff30 sp=0xc00058fe78 pc=0x7ff60471e225
github.com/spf13/cobra.(*Command).Execute(...)
	github.com/spf13/cobra@v1.7.0/command.go:992
github.com/spf13/cobra.(*Command).ExecuteContext(...)
	github.com/spf13/cobra@v1.7.0/command.go:985
main.main()
	github.com/ollama/ollama/main.go:12 +0x4d fp=0xc00058ff50 sp=0xc00058ff30 pc=0x7ff605259c2d
runtime.main()
	runtime/proc.go:283 +0x27d fp=0xc00058ffe0 sp=0xc00058ff50 pc=0x7ff604564ddd
runtime.goexit({})
	runtime/asm_amd64.s:1700 +0x1 fp=0xc00058ffe8 sp=0xc00058ffe0 pc=0x7ff60459d8e1

goroutine 2 gp=0xc0000028c0 m=nil [force gc (idle)]:
runtime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)
	runtime/proc.go:435 +0xce fp=0xc0000a7fa8 sp=0xc0000a7f88 pc=0x7ff60459598e
runtime.goparkunlock(...)
	runtime/proc.go:441
runtime.forcegchelper()
	runtime/proc.go:348 +0xb8 fp=0xc0000a7fe0 sp=0xc0000a7fa8 pc=0x7ff6045650f8
runtime.goexit({})
	runtime/asm_amd64.s:1700 +0x1 fp=0xc0000a7fe8 sp=0xc0000a7fe0 pc=0x7ff60459d8e1
created by runtime.init.7 in goroutine 1
	runtime/proc.go:336 +0x1a

goroutine 3 gp=0xc000002c40 m=nil [GC sweep wait]:
runtime.gopark(0x1?, 0x0?, 0x0?, 0x0?, 0x0?)
	runtime/proc.go:435 +0xce fp=0xc0000a9f80 sp=0xc0000a9f60 pc=0x7ff60459598e
runtime.goparkunlock(...)
	runtime/proc.go:441
runtime.bgsweep(0xc0000b6000)
	runtime/mgcsweep.go:316 +0xdf fp=0xc0000a9fc8 sp=0xc0000a9f80 pc=0x7ff60454debf
runtime.gcenable.gowrap1()
	runtime/mgc.go:204 +0x25 fp=0xc0000a9fe0 sp=0xc0000a9fc8 pc=0x7ff604542285
runtime.goexit({})
	runtime/asm_amd64.s:1700 +0x1 fp=0xc0000a9fe8 sp=0xc0000a9fe0 pc=0x7ff60459d8e1
created by runtime.gcenable in goroutine 1
	runtime/mgc.go:204 +0x66

goroutine 4 gp=0xc000002e00 m=nil [GC scavenge wait]:
runtime.gopark(0x10000?, 0x7ff605a6de10?, 0x0?, 0x0?, 0x0?)
	runtime/proc.go:435 +0xce fp=0xc0000bdf78 sp=0xc0000bdf58 pc=0x7ff60459598e
runtime.goparkunlock(...)
	runtime/proc.go:441
runtime.(*scavengerState).park(0x7ff6064333c0)
	runtime/mgcscavenge.go:425 +0x49 fp=0xc0000bdfa8 sp=0xc0000bdf78 pc=0x7ff60454b909
runtime.bgscavenge(0xc0000b6000)
	runtime/mgcscavenge.go:658 +0x59 fp=0xc0000bdfc8 sp=0xc0000bdfa8 pc=0x7ff60454be99
runtime.gcenable.gowrap2()
	runtime/mgc.go:205 +0x25 fp=0xc0000bdfe0 sp=0xc0000bdfc8 pc=0x7ff604542225
runtime.goexit({})
	runtime/asm_amd64.s:1700 +0x1 fp=0xc0000bdfe8 sp=0xc0000bdfe0 pc=0x7ff60459d8e1
created by runtime.gcenable in goroutine 1
	runtime/mgc.go:205 +0xa5

goroutine 5 gp=0xc000003340 m=nil [finalizer wait]:
runtime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)
	runtime/proc.go:435 +0xce fp=0xc0000bfe30 sp=0xc0000bfe10 pc=0x7ff60459598e
runtime.runfinq()
	runtime/mfinal.go:196 +0x107 fp=0xc0000bffe0 sp=0xc0000bfe30 pc=0x7ff604541207
runtime.goexit({})
	runtime/asm_amd64.s:1700 +0x1 fp=0xc0000bffe8 sp=0xc0000bffe0 pc=0x7ff60459d8e1
created by runtime.createfing in goroutine 1
	runtime/mfinal.go:166 +0x3d

goroutine 6 gp=0xc000003dc0 m=nil [chan receive]:
runtime.gopark(0xc0002295e0?, 0xc000694000?, 0x60?, 0xbf?, 0x7ff60468be48?)
	runtime/proc.go:435 +0xce fp=0xc0000abf18 sp=0xc0000abef8 pc=0x7ff60459598e
runtime.chanrecv(0xc000038460, 0x0, 0x1)
	runtime/chan.go:664 +0x445 fp=0xc0000abf90 sp=0xc0000abf18 pc=0x7ff604532d45
runtime.chanrecv1(0x7ff604564f40?, 0xc0000abf76?)
	runtime/chan.go:506 +0x12 fp=0xc0000abfb8 sp=0xc0000abf90 pc=0x7ff6045328d2
runtime.unique_runtime_registerUniqueMapCleanup.func2(...)
	runtime/mgc.go:1796
runtime.unique_runtime_registerUniqueMapCleanup.gowrap1()
	runtime/mgc.go:1799 +0x2f fp=0xc0000abfe0 sp=0xc0000abfb8 pc=0x7ff6045454af
runtime.goexit({})
	runtime/asm_amd64.s:1700 +0x1 fp=0xc0000abfe8 sp=0xc0000abfe0 pc=0x7ff60459d8e1
created by unique.runtime_registerUniqueMapCleanup in goroutine 1
	runtime/mgc.go:1794 +0x85

goroutine 7 gp=0xc0004181c0 m=nil [GC worker (idle)]:
runtime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)
	runtime/proc.go:435 +0xce fp=0xc0000b9f38 sp=0xc0000b9f18 pc=0x7ff60459598e
runtime.gcBgMarkWorker(0xc000039880)
	runtime/mgc.go:1423 +0xe9 fp=0xc0000b9fc8 sp=0xc0000b9f38 pc=0x7ff6045447a9
runtime.gcBgMarkStartWorkers.gowrap1()
	runtime/mgc.go:1339 +0x25 fp=0xc0000b9fe0 sp=0xc0000b9fc8 pc=0x7ff604544685
runtime.goexit({})
	runtime/asm_amd64.s:1700 +0x1 fp=0xc0000b9fe8 sp=0xc0000b9fe0 pc=0x7ff60459d8e1
created by runtime.gcBgMarkStartWorkers in goroutine 1
	runtime/mgc.go:1339 +0x105

goroutine 8 gp=0xc000418380 m=nil [GC worker (idle)]:
runtime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)
	runtime/proc.go:435 +0xce fp=0xc0000bbf38 sp=0xc0000bbf18 pc=0x7ff60459598e
runtime.gcBgMarkWorker(0xc000039880)
	runtime/mgc.go:1423 +0xe9 fp=0xc0000bbfc8 sp=0xc0000bbf38 pc=0x7ff6045447a9
runtime.gcBgMarkStartWorkers.gowrap1()
	runtime/mgc.go:1339 +0x25 fp=0xc0000bbfe0 sp=0xc0000bbfc8 pc=0x7ff604544685
runtime.goexit({})
	runtime/asm_amd64.s:1700 +0x1 fp=0xc0000bbfe8 sp=0xc0000bbfe0 pc=0x7ff60459d8e1
created by runtime.gcBgMarkStartWorkers in goroutine 1
	runtime/mgc.go:1339 +0x105

goroutine 9 gp=0xc000418540 m=nil [GC worker (idle)]:
runtime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)
	runtime/proc.go:435 +0xce fp=0xc000475f38 sp=0xc000475f18 pc=0x7ff60459598e
runtime.gcBgMarkWorker(0xc000039880)
	runtime/mgc.go:1423 +0xe9 fp=0xc000475fc8 sp=0xc000475f38 pc=0x7ff6045447a9
runtime.gcBgMarkStartWorkers.gowrap1()
	runtime/mgc.go:1339 +0x25 fp=0xc000475fe0 sp=0xc000475fc8 pc=0x7ff604544685
runtime.goexit({})
	runtime/asm_amd64.s:1700 +0x1 fp=0xc000475fe8 sp=0xc000475fe0 pc=0x7ff60459d8e1
created by runtime.gcBgMarkStartWorkers in goroutine 1
	runtime/mgc.go:1339 +0x105

goroutine 10 gp=0xc000418700 m=nil [GC worker (idle)]:
runtime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)
	runtime/proc.go:435 +0xce fp=0xc000477f38 sp=0xc000477f18 pc=0x7ff60459598e
runtime.gcBgMarkWorker(0xc000039880)
	runtime/mgc.go:1423 +0xe9 fp=0xc000477fc8 sp=0xc000477f38 pc=0x7ff6045447a9
runtime.gcBgMarkStartWorkers.gowrap1()
	runtime/mgc.go:1339 +0x25 fp=0xc000477fe0 sp=0xc000477fc8 pc=0x7ff604544685
runtime.goexit({})
	runtime/asm_amd64.s:1700 +0x1 fp=0xc000477fe8 sp=0xc000477fe0 pc=0x7ff60459d8e1
created by runtime.gcBgMarkStartWorkers in goroutine 1
	runtime/mgc.go:1339 +0x105

goroutine 18 gp=0xc0001061c0 m=nil [GC worker (idle)]:
runtime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)
	runtime/proc.go:435 +0xce fp=0xc000471f38 sp=0xc000471f18 pc=0x7ff60459598e
runtime.gcBgMarkWorker(0xc000039880)
	runtime/mgc.go:1423 +0xe9 fp=0xc000471fc8 sp=0xc000471f38 pc=0x7ff6045447a9
runtime.gcBgMarkStartWorkers.gowrap1()
	runtime/mgc.go:1339 +0x25 fp=0xc000471fe0 sp=0xc000471fc8 pc=0x7ff604544685
runtime.goexit({})
	runtime/asm_amd64.s:1700 +0x1 fp=0xc000471fe8 sp=0xc000471fe0 pc=0x7ff60459d8e1
created by runtime.gcBgMarkStartWorkers in goroutine 1
	runtime/mgc.go:1339 +0x105

goroutine 34 gp=0xc000484000 m=nil [GC worker (idle)]:
runtime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)
	runtime/proc.go:435 +0xce fp=0xc00048bf38 sp=0xc00048bf18 pc=0x7ff60459598e
runtime.gcBgMarkWorker(0xc000039880)
	runtime/mgc.go:1423 +0xe9 fp=0xc00048bfc8 sp=0xc00048bf38 pc=0x7ff6045447a9
runtime.gcBgMarkStartWorkers.gowrap1()
	runtime/mgc.go:1339 +0x25 fp=0xc00048bfe0 sp=0xc00048bfc8 pc=0x7ff604544685
runtime.goexit({})
	runtime/asm_amd64.s:1700 +0x1 fp=0xc00048bfe8 sp=0xc00048bfe0 pc=0x7ff60459d8e1
created by runtime.gcBgMarkStartWorkers in goroutine 1
	runtime/mgc.go:1339 +0x105

goroutine 11 gp=0xc0004188c0 m=nil [GC worker (idle)]:
runtime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)
	runtime/proc.go:435 +0xce fp=0xc000487f38 sp=0xc000487f18 pc=0x7ff60459598e
runtime.gcBgMarkWorker(0xc000039880)
	runtime/mgc.go:1423 +0xe9 fp=0xc000487fc8 sp=0xc000487f38 pc=0x7ff6045447a9
runtime.gcBgMarkStartWorkers.gowrap1()
	runtime/mgc.go:1339 +0x25 fp=0xc000487fe0 sp=0xc000487fc8 pc=0x7ff604544685
runtime.goexit({})
	runtime/asm_amd64.s:1700 +0x1 fp=0xc000487fe8 sp=0xc000487fe0 pc=0x7ff60459d8e1
created by runtime.gcBgMarkStartWorkers in goroutine 1
	runtime/mgc.go:1339 +0x105

goroutine 35 gp=0xc0004841c0 m=nil [GC worker (idle)]:
runtime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)
	runtime/proc.go:435 +0xce fp=0xc00048df38 sp=0xc00048df18 pc=0x7ff60459598e
runtime.gcBgMarkWorker(0xc000039880)
	runtime/mgc.go:1423 +0xe9 fp=0xc00048dfc8 sp=0xc00048df38 pc=0x7ff6045447a9
runtime.gcBgMarkStartWorkers.gowrap1()
	runtime/mgc.go:1339 +0x25 fp=0xc00048dfe0 sp=0xc00048dfc8 pc=0x7ff604544685
runtime.goexit({})
	runtime/asm_amd64.s:1700 +0x1 fp=0xc00048dfe8 sp=0xc00048dfe0 pc=0x7ff60459d8e1
created by runtime.gcBgMarkStartWorkers in goroutine 1
	runtime/mgc.go:1339 +0x105

goroutine 12 gp=0xc000418a80 m=nil [GC worker (idle)]:
runtime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)
	runtime/proc.go:435 +0xce fp=0xc000489f38 sp=0xc000489f18 pc=0x7ff60459598e
runtime.gcBgMarkWorker(0xc000039880)
	runtime/mgc.go:1423 +0xe9 fp=0xc000489fc8 sp=0xc000489f38 pc=0x7ff6045447a9
runtime.gcBgMarkStartWorkers.gowrap1()
	runtime/mgc.go:1339 +0x25 fp=0xc000489fe0 sp=0xc000489fc8 pc=0x7ff604544685
runtime.goexit({})
	runtime/asm_amd64.s:1700 +0x1 fp=0xc000489fe8 sp=0xc000489fe0 pc=0x7ff60459d8e1
created by runtime.gcBgMarkStartWorkers in goroutine 1
	runtime/mgc.go:1339 +0x105

goroutine 19 gp=0xc000106380 m=nil [GC worker (idle)]:
runtime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)
	runtime/proc.go:435 +0xce fp=0xc000473f38 sp=0xc000473f18 pc=0x7ff60459598e
runtime.gcBgMarkWorker(0xc000039880)
	runtime/mgc.go:1423 +0xe9 fp=0xc000473fc8 sp=0xc000473f38 pc=0x7ff6045447a9
runtime.gcBgMarkStartWorkers.gowrap1()
	runtime/mgc.go:1339 +0x25 fp=0xc000473fe0 sp=0xc000473fc8 pc=0x7ff604544685
runtime.goexit({})
	runtime/asm_amd64.s:1700 +0x1 fp=0xc000473fe8 sp=0xc000473fe0 pc=0x7ff60459d8e1
created by runtime.gcBgMarkStartWorkers in goroutine 1
	runtime/mgc.go:1339 +0x105

goroutine 36 gp=0xc000484380 m=nil [GC worker (idle)]:
runtime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)
	runtime/proc.go:435 +0xce fp=0xc000493f38 sp=0xc000493f18 pc=0x7ff60459598e
runtime.gcBgMarkWorker(0xc000039880)
	runtime/mgc.go:1423 +0xe9 fp=0xc000493fc8 sp=0xc000493f38 pc=0x7ff6045447a9
runtime.gcBgMarkStartWorkers.gowrap1()
	runtime/mgc.go:1339 +0x25 fp=0xc000493fe0 sp=0xc000493fc8 pc=0x7ff604544685
runtime.goexit({})
	runtime/asm_amd64.s:1700 +0x1 fp=0xc000493fe8 sp=0xc000493fe0 pc=0x7ff60459d8e1
created by runtime.gcBgMarkStartWorkers in goroutine 1
	runtime/mgc.go:1339 +0x105

goroutine 13 gp=0xc000418c40 m=nil [GC worker (idle)]:
runtime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)
	runtime/proc.go:435 +0xce fp=0xc00048ff38 sp=0xc00048ff18 pc=0x7ff60459598e
runtime.gcBgMarkWorker(0xc000039880)
	runtime/mgc.go:1423 +0xe9 fp=0xc00048ffc8 sp=0xc00048ff38 pc=0x7ff6045447a9
runtime.gcBgMarkStartWorkers.gowrap1()
	runtime/mgc.go:1339 +0x25 fp=0xc00048ffe0 sp=0xc00048ffc8 pc=0x7ff604544685
runtime.goexit({})
	runtime/asm_amd64.s:1700 +0x1 fp=0xc00048ffe8 sp=0xc00048ffe0 pc=0x7ff60459d8e1
created by runtime.gcBgMarkStartWorkers in goroutine 1
	runtime/mgc.go:1339 +0x105

goroutine 20 gp=0xc000106540 m=nil [GC worker (idle)]:
runtime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)
	runtime/proc.go:435 +0xce fp=0xc000115f38 sp=0xc000115f18 pc=0x7ff60459598e
runtime.gcBgMarkWorker(0xc000039880)
	runtime/mgc.go:1423 +0xe9 fp=0xc000115fc8 sp=0xc000115f38 pc=0x7ff6045447a9
runtime.gcBgMarkStartWorkers.gowrap1()
	runtime/mgc.go:1339 +0x25 fp=0xc000115fe0 sp=0xc000115fc8 pc=0x7ff604544685
runtime.goexit({})
	runtime/asm_amd64.s:1700 +0x1 fp=0xc000115fe8 sp=0xc000115fe0 pc=0x7ff60459d8e1
created by runtime.gcBgMarkStartWorkers in goroutine 1
	runtime/mgc.go:1339 +0x105

goroutine 21 gp=0xc000106700 m=nil [GC worker (idle)]:
runtime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)
	runtime/proc.go:435 +0xce fp=0xc000117f38 sp=0xc000117f18 pc=0x7ff60459598e
runtime.gcBgMarkWorker(0xc000039880)
	runtime/mgc.go:1423 +0xe9 fp=0xc000117fc8 sp=0xc000117f38 pc=0x7ff6045447a9
runtime.gcBgMarkStartWorkers.gowrap1()
	runtime/mgc.go:1339 +0x25 fp=0xc000117fe0 sp=0xc000117fc8 pc=0x7ff604544685
runtime.goexit({})
	runtime/asm_amd64.s:1700 +0x1 fp=0xc000117fe8 sp=0xc000117fe0 pc=0x7ff60459d8e1
created by runtime.gcBgMarkStartWorkers in goroutine 1
	runtime/mgc.go:1339 +0x105

goroutine 37 gp=0xc000484540 m=nil [GC worker (idle)]:
runtime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)
	runtime/proc.go:435 +0xce fp=0xc000495f38 sp=0xc000495f18 pc=0x7ff60459598e
runtime.gcBgMarkWorker(0xc000039880)
	runtime/mgc.go:1423 +0xe9 fp=0xc000495fc8 sp=0xc000495f38 pc=0x7ff6045447a9
runtime.gcBgMarkStartWorkers.gowrap1()
	runtime/mgc.go:1339 +0x25 fp=0xc000495fe0 sp=0xc000495fc8 pc=0x7ff604544685
runtime.goexit({})
	runtime/asm_amd64.s:1700 +0x1 fp=0xc000495fe8 sp=0xc000495fe0 pc=0x7ff60459d8e1
created by runtime.gcBgMarkStartWorkers in goroutine 1
	runtime/mgc.go:1339 +0x105

goroutine 14 gp=0xc000418e00 m=nil [GC worker (idle)]:
runtime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)
	runtime/proc.go:435 +0xce fp=0xc000491f38 sp=0xc000491f18 pc=0x7ff60459598e
runtime.gcBgMarkWorker(0xc000039880)
	runtime/mgc.go:1423 +0xe9 fp=0xc000491fc8 sp=0xc000491f38 pc=0x7ff6045447a9
runtime.gcBgMarkStartWorkers.gowrap1()
	runtime/mgc.go:1339 +0x25 fp=0xc000491fe0 sp=0xc000491fc8 pc=0x7ff604544685
runtime.goexit({})
	runtime/asm_amd64.s:1700 +0x1 fp=0xc000491fe8 sp=0xc000491fe0 pc=0x7ff60459d8e1
created by runtime.gcBgMarkStartWorkers in goroutine 1
	runtime/mgc.go:1339 +0x105

goroutine 22 gp=0xc0001068c0 m=nil [GC worker (idle)]:
runtime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)
	runtime/proc.go:435 +0xce fp=0xc000111f38 sp=0xc000111f18 pc=0x7ff60459598e
runtime.gcBgMarkWorker(0xc000039880)
	runtime/mgc.go:1423 +0xe9 fp=0xc000111fc8 sp=0xc000111f38 pc=0x7ff6045447a9
runtime.gcBgMarkStartWorkers.gowrap1()
	runtime/mgc.go:1339 +0x25 fp=0xc000111fe0 sp=0xc000111fc8 pc=0x7ff604544685
runtime.goexit({})
	runtime/asm_amd64.s:1700 +0x1 fp=0xc000111fe8 sp=0xc000111fe0 pc=0x7ff60459d8e1
created by runtime.gcBgMarkStartWorkers in goroutine 1
	runtime/mgc.go:1339 +0x105

goroutine 23 gp=0xc000106a80 m=nil [GC worker (idle)]:
runtime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)
	runtime/proc.go:435 +0xce fp=0xc000113f38 sp=0xc000113f18 pc=0x7ff60459598e
runtime.gcBgMarkWorker(0xc000039880)
	runtime/mgc.go:1423 +0xe9 fp=0xc000113fc8 sp=0xc000113f38 pc=0x7ff6045447a9
runtime.gcBgMarkStartWorkers.gowrap1()
	runtime/mgc.go:1339 +0x25 fp=0xc000113fe0 sp=0xc000113fc8 pc=0x7ff604544685
runtime.goexit({})
	runtime/asm_amd64.s:1700 +0x1 fp=0xc000113fe8 sp=0xc000113fe0 pc=0x7ff60459d8e1
created by runtime.gcBgMarkStartWorkers in goroutine 1
	runtime/mgc.go:1339 +0x105

goroutine 38 gp=0xc000484700 m=nil [GC worker (idle)]:
runtime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)
	runtime/proc.go:435 +0xce fp=0xc00049bf38 sp=0xc00049bf18 pc=0x7ff60459598e
runtime.gcBgMarkWorker(0xc000039880)
	runtime/mgc.go:1423 +0xe9 fp=0xc00049bfc8 sp=0xc00049bf38 pc=0x7ff6045447a9
runtime.gcBgMarkStartWorkers.gowrap1()
	runtime/mgc.go:1339 +0x25 fp=0xc00049bfe0 sp=0xc00049bfc8 pc=0x7ff604544685
runtime.goexit({})
	runtime/asm_amd64.s:1700 +0x1 fp=0xc00049bfe8 sp=0xc00049bfe0 pc=0x7ff60459d8e1
created by runtime.gcBgMarkStartWorkers in goroutine 1
	runtime/mgc.go:1339 +0x105

goroutine 15 gp=0xc000418fc0 m=nil [GC worker (idle)]:
runtime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)
	runtime/proc.go:435 +0xce fp=0xc000497f38 sp=0xc000497f18 pc=0x7ff60459598e
runtime.gcBgMarkWorker(0xc000039880)
	runtime/mgc.go:1423 +0xe9 fp=0xc000497fc8 sp=0xc000497f38 pc=0x7ff6045447a9
runtime.gcBgMarkStartWorkers.gowrap1()
	runtime/mgc.go:1339 +0x25 fp=0xc000497fe0 sp=0xc000497fc8 pc=0x7ff604544685
runtime.goexit({})
	runtime/asm_amd64.s:1700 +0x1 fp=0xc000497fe8 sp=0xc000497fe0 pc=0x7ff60459d8e1
created by runtime.gcBgMarkStartWorkers in goroutine 1
	runtime/mgc.go:1339 +0x105

goroutine 16 gp=0xc000419180 m=nil [GC worker (idle)]:
runtime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)
	runtime/proc.go:435 +0xce fp=0xc000499f38 sp=0xc000499f18 pc=0x7ff60459598e
runtime.gcBgMarkWorker(0xc000039880)
	runtime/mgc.go:1423 +0xe9 fp=0xc000499fc8 sp=0xc000499f38 pc=0x7ff6045447a9
runtime.gcBgMarkStartWorkers.gowrap1()
	runtime/mgc.go:1339 +0x25 fp=0xc000499fe0 sp=0xc000499fc8 pc=0x7ff604544685
runtime.goexit({})
	runtime/asm_amd64.s:1700 +0x1 fp=0xc000499fe8 sp=0xc000499fe0 pc=0x7ff60459d8e1
created by runtime.gcBgMarkStartWorkers in goroutine 1
	runtime/mgc.go:1339 +0x105

goroutine 24 gp=0xc000106c40 m=nil [GC worker (idle)]:
runtime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)
	runtime/proc.go:435 +0xce fp=0xc00011df38 sp=0xc00011df18 pc=0x7ff60459598e
runtime.gcBgMarkWorker(0xc000039880)
	runtime/mgc.go:1423 +0xe9 fp=0xc00011dfc8 sp=0xc00011df38 pc=0x7ff6045447a9
runtime.gcBgMarkStartWorkers.gowrap1()
	runtime/mgc.go:1339 +0x25 fp=0xc00011dfe0 sp=0xc00011dfc8 pc=0x7ff604544685
runtime.goexit({})
	runtime/asm_amd64.s:1700 +0x1 fp=0xc00011dfe8 sp=0xc00011dfe0 pc=0x7ff60459d8e1
created by runtime.gcBgMarkStartWorkers in goroutine 1
	runtime/mgc.go:1339 +0x105

goroutine 39 gp=0xc0004848c0 m=nil [GC worker (idle)]:
runtime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)
	runtime/proc.go:435 +0xce fp=0xc00049df38 sp=0xc00049df18 pc=0x7ff60459598e
runtime.gcBgMarkWorker(0xc000039880)
	runtime/mgc.go:1423 +0xe9 fp=0xc00049dfc8 sp=0xc00049df38 pc=0x7ff6045447a9
runtime.gcBgMarkStartWorkers.gowrap1()
	runtime/mgc.go:1339 +0x25 fp=0xc00049dfe0 sp=0xc00049dfc8 pc=0x7ff604544685
runtime.goexit({})
	runtime/asm_amd64.s:1700 +0x1 fp=0xc00049dfe8 sp=0xc00049dfe0 pc=0x7ff60459d8e1
created by runtime.gcBgMarkStartWorkers in goroutine 1
	runtime/mgc.go:1339 +0x105

goroutine 50 gp=0xc000419340 m=nil [GC worker (idle)]:
runtime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)
	runtime/proc.go:435 +0xce fp=0xc000119f38 sp=0xc000119f18 pc=0x7ff60459598e
runtime.gcBgMarkWorker(0xc000039880)
	runtime/mgc.go:1423 +0xe9 fp=0xc000119fc8 sp=0xc000119f38 pc=0x7ff6045447a9
runtime.gcBgMarkStartWorkers.gowrap1()
	runtime/mgc.go:1339 +0x25 fp=0xc000119fe0 sp=0xc000119fc8 pc=0x7ff604544685
runtime.goexit({})
	runtime/asm_amd64.s:1700 +0x1 fp=0xc000119fe8 sp=0xc000119fe0 pc=0x7ff60459d8e1
created by runtime.gcBgMarkStartWorkers in goroutine 1
	runtime/mgc.go:1339 +0x105

goroutine 51 gp=0xc000419500 m=nil [GC worker (idle)]:
runtime.gopark(0x4eb00b91ba40?, 0x1?, 0x0?, 0x0?, 0x0?)
	runtime/proc.go:435 +0xce fp=0xc00011bf38 sp=0xc00011bf18 pc=0x7ff60459598e
runtime.gcBgMarkWorker(0xc000039880)
	runtime/mgc.go:1423 +0xe9 fp=0xc00011bfc8 sp=0xc00011bf38 pc=0x7ff6045447a9
runtime.gcBgMarkStartWorkers.gowrap1()
	runtime/mgc.go:1339 +0x25 fp=0xc00011bfe0 sp=0xc00011bfc8 pc=0x7ff604544685
runtime.goexit({})
	runtime/asm_amd64.s:1700 +0x1 fp=0xc00011bfe8 sp=0xc00011bfe0 pc=0x7ff60459d8e1
created by runtime.gcBgMarkStartWorkers in goroutine 1
	runtime/mgc.go:1339 +0x105

goroutine 25 gp=0xc000106e00 m=nil [GC worker (idle)]:
runtime.gopark(0x4eb00b8c05b4?, 0x0?, 0x0?, 0x0?, 0x0?)
	runtime/proc.go:435 +0xce fp=0xc00011ff38 sp=0xc00011ff18 pc=0x7ff60459598e
runtime.gcBgMarkWorker(0xc000039880)
	runtime/mgc.go:1423 +0xe9 fp=0xc00011ffc8 sp=0xc00011ff38 pc=0x7ff6045447a9
runtime.gcBgMarkStartWorkers.gowrap1()
	runtime/mgc.go:1339 +0x25 fp=0xc00011ffe0 sp=0xc00011ffc8 pc=0x7ff604544685
runtime.goexit({})
	runtime/asm_amd64.s:1700 +0x1 fp=0xc00011ffe8 sp=0xc00011ffe0 pc=0x7ff60459d8e1
created by runtime.gcBgMarkStartWorkers in goroutine 1
	runtime/mgc.go:1339 +0x105

goroutine 40 gp=0xc000484a80 m=nil [GC worker (idle)]:
runtime.gopark(0x7ff606481fa0?, 0x1?, 0x84?, 0x52?, 0x0?)
	runtime/proc.go:435 +0xce fp=0xc0004a3f38 sp=0xc0004a3f18 pc=0x7ff60459598e
runtime.gcBgMarkWorker(0xc000039880)
	runtime/mgc.go:1423 +0xe9 fp=0xc0004a3fc8 sp=0xc0004a3f38 pc=0x7ff6045447a9
runtime.gcBgMarkStartWorkers.gowrap1()
	runtime/mgc.go:1339 +0x25 fp=0xc0004a3fe0 sp=0xc0004a3fc8 pc=0x7ff604544685
runtime.goexit({})
	runtime/asm_amd64.s:1700 +0x1 fp=0xc0004a3fe8 sp=0xc0004a3fe0 pc=0x7ff60459d8e1
created by runtime.gcBgMarkStartWorkers in goroutine 1
	runtime/mgc.go:1339 +0x105

goroutine 52 gp=0xc0004196c0 m=nil [GC worker (idle)]:
runtime.gopark(0x4eb00b91ba40?, 0x1?, 0x64?, 0x1b?, 0x0?)
	runtime/proc.go:435 +0xce fp=0xc00049ff38 sp=0xc00049ff18 pc=0x7ff60459598e
runtime.gcBgMarkWorker(0xc000039880)
	runtime/mgc.go:1423 +0xe9 fp=0xc00049ffc8 sp=0xc00049ff38 pc=0x7ff6045447a9
runtime.gcBgMarkStartWorkers.gowrap1()
	runtime/mgc.go:1339 +0x25 fp=0xc00049ffe0 sp=0xc00049ffc8 pc=0x7ff604544685
runtime.goexit({})
	runtime/asm_amd64.s:1700 +0x1 fp=0xc00049ffe8 sp=0xc00049ffe0 pc=0x7ff60459d8e1
created by runtime.gcBgMarkStartWorkers in goroutine 1
	runtime/mgc.go:1339 +0x105

goroutine 26 gp=0xc000106fc0 m=nil [GC worker (idle)]:
runtime.gopark(0x7ff606481fa0?, 0x1?, 0x84?, 0x52?, 0x0?)
	runtime/proc.go:435 +0xce fp=0xc000127f38 sp=0xc000127f18 pc=0x7ff60459598e
runtime.gcBgMarkWorker(0xc000039880)
	runtime/mgc.go:1423 +0xe9 fp=0xc000127fc8 sp=0xc000127f38 pc=0x7ff6045447a9
runtime.gcBgMarkStartWorkers.gowrap1()
	runtime/mgc.go:1339 +0x25 fp=0xc000127fe0 sp=0xc000127fc8 pc=0x7ff604544685
runtime.goexit({})
	runtime/asm_amd64.s:1700 +0x1 fp=0xc000127fe8 sp=0xc000127fe0 pc=0x7ff60459d8e1
created by runtime.gcBgMarkStartWorkers in goroutine 1
	runtime/mgc.go:1339 +0x105

goroutine 41 gp=0xc000484c40 m=nil [GC worker (idle)]:
runtime.gopark(0x4eb00b91ba40?, 0x0?, 0x0?, 0x0?, 0x0?)
	runtime/proc.go:435 +0xce fp=0xc0004a5f38 sp=0xc0004a5f18 pc=0x7ff60459598e
runtime.gcBgMarkWorker(0xc000039880)
	runtime/mgc.go:1423 +0xe9 fp=0xc0004a5fc8 sp=0xc0004a5f38 pc=0x7ff6045447a9
runtime.gcBgMarkStartWorkers.gowrap1()
	runtime/mgc.go:1339 +0x25 fp=0xc0004a5fe0 sp=0xc0004a5fc8 pc=0x7ff604544685
runtime.goexit({})
	runtime/asm_amd64.s:1700 +0x1 fp=0xc0004a5fe8 sp=0xc0004a5fe0 pc=0x7ff60459d8e1
created by runtime.gcBgMarkStartWorkers in goroutine 1
	runtime/mgc.go:1339 +0x105

goroutine 53 gp=0xc000419880 m=nil [GC worker (idle)]:
runtime.gopark(0x4eb00b91ba40?, 0x1?, 0x84?, 0x52?, 0x0?)
	runtime/proc.go:435 +0xce fp=0xc0004a1f38 sp=0xc0004a1f18 pc=0x7ff60459598e
runtime.gcBgMarkWorker(0xc000039880)
	runtime/mgc.go:1423 +0xe9 fp=0xc0004a1fc8 sp=0xc0004a1f38 pc=0x7ff6045447a9
runtime.gcBgMarkStartWorkers.gowrap1()
	runtime/mgc.go:1339 +0x25 fp=0xc0004a1fe0 sp=0xc0004a1fc8 pc=0x7ff604544685
runtime.goexit({})
	runtime/asm_amd64.s:1700 +0x1 fp=0xc0004a1fe8 sp=0xc0004a1fe0 pc=0x7ff60459d8e1
created by runtime.gcBgMarkStartWorkers in goroutine 1
	runtime/mgc.go:1339 +0x105

goroutine 27 gp=0xc000107180 m=nil [GC worker (idle)]:
runtime.gopark(0x4eb00b8c05b4?, 0x1?, 0x90?, 0x1e?, 0x0?)
	runtime/proc.go:435 +0xce fp=0xc000129f38 sp=0xc000129f18 pc=0x7ff60459598e
runtime.gcBgMarkWorker(0xc000039880)
	runtime/mgc.go:1423 +0xe9 fp=0xc000129fc8 sp=0xc000129f38 pc=0x7ff6045447a9
runtime.gcBgMarkStartWorkers.gowrap1()
	runtime/mgc.go:1339 +0x25 fp=0xc000129fe0 sp=0xc000129fc8 pc=0x7ff604544685
runtime.goexit({})
	runtime/asm_amd64.s:1700 +0x1 fp=0xc000129fe8 sp=0xc000129fe0 pc=0x7ff60459d8e1
created by runtime.gcBgMarkStartWorkers in goroutine 1
	runtime/mgc.go:1339 +0x105

goroutine 54 gp=0xc000506380 m=nil [sync.WaitGroup.Wait]:
runtime.gopark(0x0?, 0x0?, 0xc0?, 0x83?, 0x0?)
	runtime/proc.go:435 +0xce fp=0xc000123e20 sp=0xc000123e00 pc=0x7ff60459598e
runtime.goparkunlock(...)
	runtime/proc.go:441
runtime.semacquire1(0xc00032e340, 0x0, 0x1, 0x0, 0x18)
	runtime/sema.go:188 +0x22f fp=0xc000123e88 sp=0xc000123e20 pc=0x7ff60457750f
sync.runtime_SemacquireWaitGroup(0x0?)
	runtime/sema.go:110 +0x25 fp=0xc000123ec0 sp=0xc000123e88 pc=0x7ff604596f85
sync.(*WaitGroup).Wait(0x0?)
	sync/waitgroup.go:118 +0x48 fp=0xc000123ee8 sp=0xc000123ec0 pc=0x7ff6045ab7a8
github.com/ollama/ollama/runner/llamarunner.(*Server).run(0xc00032e320, {0x7ff605a83b80, 0xc0003a60f0})
	github.com/ollama/ollama/runner/llamarunner/runner.go:359 +0x4b fp=0xc000123fb8 sp=0xc000123ee8 pc=0x7ff604a1e08b
github.com/ollama/ollama/runner/llamarunner.Execute.gowrap1()
	github.com/ollama/ollama/runner/llamarunner/runner.go:979 +0x28 fp=0xc000123fe0 sp=0xc000123fb8 pc=0x7ff604a23528
runtime.goexit({})
	runtime/asm_amd64.s:1700 +0x1 fp=0xc000123fe8 sp=0xc000123fe0 pc=0x7ff60459d8e1
created by github.com/ollama/ollama/runner/llamarunner.Execute in goroutine 1
	github.com/ollama/ollama/runner/llamarunner/runner.go:979 +0x4c5

goroutine 55 gp=0xc000506540 m=nil [IO wait]:
runtime.gopark(0x0?, 0xc0003ac2a0?, 0x48?, 0xc3?, 0xc0003ac34c?)
	runtime/proc.go:435 +0xce fp=0xc00058b8c8 sp=0xc00058b8a8 pc=0x7ff60459598e
runtime.netpollblock(0x3dc?, 0x4530406?, 0xf6?)
	runtime/netpoll.go:575 +0xf7 fp=0xc00058b900 sp=0xc00058b8c8 pc=0x7ff60455bdf7
internal/poll.runtime_pollWait(0x2a6fb7ecff8, 0x72)
	runtime/netpoll.go:351 +0x85 fp=0xc00058b920 sp=0xc00058b900 pc=0x7ff604594b25
internal/poll.(*pollDesc).wait(0x7ff60475c9b7?, 0xc00058b970?, 0x0)
	internal/poll/fd_poll_runtime.go:84 +0x27 fp=0xc00058b948 sp=0xc00058b920 pc=0x7ff60462bc87
internal/poll.execIO(0xc0003ac2a0, 0x7ff605912278)
	internal/poll/fd_windows.go:177 +0x105 fp=0xc00058b9c0 sp=0xc00058b948 pc=0x7ff60462d0e5
internal/poll.(*FD).Read(0xc0003ac288, {0xc0003ce000, 0x1000, 0x1000})
	internal/poll/fd_windows.go:438 +0x29b fp=0xc00058ba60 sp=0xc00058b9c0 pc=0x7ff60462ddbb
net.(*netFD).Read(0xc0003ac288, {0xc0003ce000?, 0xc00058bad0?, 0x7ff60462c145?})
	net/fd_posix.go:55 +0x25 fp=0xc00058baa8 sp=0xc00058ba60 pc=0x7ff6046a1025
net.(*conn).Read(0xc000688058, {0xc0003ce000?, 0x0?, 0x0?})
	net/net.go:194 +0x45 fp=0xc00058baf0 sp=0xc00058baa8 pc=0x7ff6046b0505
net/http.(*connReader).Read(0xc00032c630, {0xc0003ce000, 0x1000, 0x1000})
	net/http/server.go:798 +0x159 fp=0xc00058bb40 sp=0xc00058baf0 pc=0x7ff60489d8f9
bufio.(*Reader).fill(0xc0000c24e0)
	bufio/bufio.go:113 +0x103 fp=0xc00058bb78 sp=0xc00058bb40 pc=0x7ff6046c6d43
bufio.(*Reader).Peek(0xc0000c24e0, 0x4)
	bufio/bufio.go:152 +0x53 fp=0xc00058bb98 sp=0xc00058bb78 pc=0x7ff6046c6e73
net/http.(*conn).serve(0xc0003301b0, {0x7ff605a83b48, 0xc00032c540})
	net/http/server.go:2137 +0x785 fp=0xc00058bfb8 sp=0xc00058bb98 pc=0x7ff6048a36e5
net/http.(*Server).Serve.gowrap3()
	net/http/server.go:3454 +0x28 fp=0xc00058bfe0 sp=0xc00058bfb8 pc=0x7ff6048a8e48
runtime.goexit({})
	runtime/asm_amd64.s:1700 +0x1 fp=0xc00058bfe8 sp=0xc00058bfe0 pc=0x7ff60459d8e1
created by net/http.(*Server).Serve in goroutine 1
	net/http/server.go:3454 +0x485
rax     0x0
rbx     0x2a6a1ff2e48
rcx     0x2a6a1ff2e48
rdx     0x2a6a1ff2e48
rdi     0x2a6a1ff2e18
rsi     0x0
rbp     0x2a6a1ff2e48
rsp     0xaf50d2e400
r8      0xfffffffd00000000
r9      0x2a6a1ff2df8
r10     0x5f
r11     0xc000012d
r12     0x0
r13     0x0
r14     0x2a6a1ff2e18
r15     0xaf50d2e5f0
rip     0x7ffb26419176
rflags  0x10246
cs      0x33
fs      0x53
gs      0x2b
time=2025-11-20T14:01:57.543+08:00 level=INFO source=server.go:1328 msg=""waiting for server to become available"" status=""llm server not responding""
time=2025-11-20T14:01:57.794+08:00 level=INFO source=server.go:1328 msg=""waiting for server to become available"" status=""llm server error""
time=2025-11-20T14:02:00.557+08:00 level=INFO source=sched.go:470 msg=""Load failed"" model=D:\LLM\Models_ollama\blobs\sha256-6150cb382311b69f09cc0f9a1b69fc029cbd742b66bb8ec531aa5ecf5c613e93 error=""llama runner process has terminated: exit status 2""
[GIN] 2025/11/20 - 14:02:00 | 500 |    49.853006s |       127.0.0.1 | POST     ""/api/generate""
```

### OS

Windows

### GPU

AMD

### CPU

AMD

### Ollama version

0.13.0","{'login': 'MapleZJH', 'id': 34159292, 'node_id': 'MDQ6VXNlcjM0MTU5Mjky', 'avatar_url': 'https://avatars.githubusercontent.com/u/34159292?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/MapleZJH', 'html_url': 'https://github.com/MapleZJH', 'followers_url': 'https://api.github.com/users/MapleZJH/followers', 'following_url': 'https://api.github.com/users/MapleZJH/following{/other_user}', 'gists_url': 'https://api.github.com/users/MapleZJH/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/MapleZJH/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/MapleZJH/subscriptions', 'organizations_url': 'https://api.github.com/users/MapleZJH/orgs', 'repos_url': 'https://api.github.com/users/MapleZJH/repos', 'events_url': 'https://api.github.com/users/MapleZJH/events{/privacy}', 'received_events_url': 'https://api.github.com/users/MapleZJH/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}","{'url': 'https://api.github.com/repos/ollama/ollama/issues/13171/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}",https://api.github.com/repos/ollama/ollama/issues/13171/timeline,,completed,,
https://api.github.com/repos/ollama/ollama/issues/13170,https://api.github.com/repos/ollama/ollama,https://api.github.com/repos/ollama/ollama/issues/13170/labels{/name},https://api.github.com/repos/ollama/ollama/issues/13170/comments,https://api.github.com/repos/ollama/ollama/issues/13170/events,https://github.com/ollama/ollama/issues/13170,3645626120,I_kwDOJ0Z1Ps7ZS9cI,13170,Failed to concurrent call embed api,"{'login': 'ultradawn', 'id': 12838392, 'node_id': 'MDQ6VXNlcjEyODM4Mzky', 'avatar_url': 'https://avatars.githubusercontent.com/u/12838392?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/ultradawn', 'html_url': 'https://github.com/ultradawn', 'followers_url': 'https://api.github.com/users/ultradawn/followers', 'following_url': 'https://api.github.com/users/ultradawn/following{/other_user}', 'gists_url': 'https://api.github.com/users/ultradawn/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/ultradawn/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/ultradawn/subscriptions', 'organizations_url': 'https://api.github.com/users/ultradawn/orgs', 'repos_url': 'https://api.github.com/users/ultradawn/repos', 'events_url': 'https://api.github.com/users/ultradawn/events{/privacy}', 'received_events_url': 'https://api.github.com/users/ultradawn/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}","[{'id': 5667396184, 'node_id': 'LA_kwDOJ0Z1Ps8AAAABUc2aWA', 'url': 'https://api.github.com/repos/ollama/ollama/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': ""Something isn't working""}]",open,False,"{'login': 'npardal', 'id': 109545900, 'node_id': 'U_kgDOBoeJrA', 'avatar_url': 'https://avatars.githubusercontent.com/u/109545900?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/npardal', 'html_url': 'https://github.com/npardal', 'followers_url': 'https://api.github.com/users/npardal/followers', 'following_url': 'https://api.github.com/users/npardal/following{/other_user}', 'gists_url': 'https://api.github.com/users/npardal/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/npardal/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/npardal/subscriptions', 'organizations_url': 'https://api.github.com/users/npardal/orgs', 'repos_url': 'https://api.github.com/users/npardal/repos', 'events_url': 'https://api.github.com/users/npardal/events{/privacy}', 'received_events_url': 'https://api.github.com/users/npardal/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}","[{'login': 'npardal', 'id': 109545900, 'node_id': 'U_kgDOBoeJrA', 'avatar_url': 'https://avatars.githubusercontent.com/u/109545900?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/npardal', 'html_url': 'https://github.com/npardal', 'followers_url': 'https://api.github.com/users/npardal/followers', 'following_url': 'https://api.github.com/users/npardal/following{/other_user}', 'gists_url': 'https://api.github.com/users/npardal/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/npardal/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/npardal/subscriptions', 'organizations_url': 'https://api.github.com/users/npardal/orgs', 'repos_url': 'https://api.github.com/users/npardal/repos', 'events_url': 'https://api.github.com/users/npardal/events{/privacy}', 'received_events_url': 'https://api.github.com/users/npardal/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}]",,2,2025-11-20T06:02:45Z,2025-11-21T21:51:29Z,,NONE,,,"{'total': 0, 'completed': 0, 'percent_completed': 0}","{'blocked_by': 0, 'total_blocked_by': 0, 'blocking': 0, 'total_blocking': 0}","### What is the issue?

[ollama.error.rtf](https://github.com/user-attachments/files/23645321/ollama.error.rtf)

Got 500 error when concurrent call embed api. refer to the detail into attached file
-------------
[GIN] 2025/11/20 - 13:56:55 | 500 |  1.274575125s |       127.0.0.1 | POST     ""/api/embed""
[GIN] 2025/11/20 - 13:56:55 | 500 |  1.264958458s |       127.0.0.1 | POST     ""/api/embed""

### Relevant log output

```shell

```

### OS

macOS

### GPU

Apple

### CPU

Apple

### Ollama version

0.13",,"{'url': 'https://api.github.com/repos/ollama/ollama/issues/13170/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}",https://api.github.com/repos/ollama/ollama/issues/13170/timeline,,,,
https://api.github.com/repos/ollama/ollama/issues/13169,https://api.github.com/repos/ollama/ollama,https://api.github.com/repos/ollama/ollama/issues/13169/labels{/name},https://api.github.com/repos/ollama/ollama/issues/13169/comments,https://api.github.com/repos/ollama/ollama/issues/13169/events,https://github.com/ollama/ollama/issues/13169,3645470088,I_kwDOJ0Z1Ps7ZSXWI,13169,Ollama 0.13.0 docker fails with cuda on ARM,"{'login': 'audunmg', 'id': 3478347, 'node_id': 'MDQ6VXNlcjM0NzgzNDc=', 'avatar_url': 'https://avatars.githubusercontent.com/u/3478347?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/audunmg', 'html_url': 'https://github.com/audunmg', 'followers_url': 'https://api.github.com/users/audunmg/followers', 'following_url': 'https://api.github.com/users/audunmg/following{/other_user}', 'gists_url': 'https://api.github.com/users/audunmg/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/audunmg/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/audunmg/subscriptions', 'organizations_url': 'https://api.github.com/users/audunmg/orgs', 'repos_url': 'https://api.github.com/users/audunmg/repos', 'events_url': 'https://api.github.com/users/audunmg/events{/privacy}', 'received_events_url': 'https://api.github.com/users/audunmg/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}","[{'id': 5667396184, 'node_id': 'LA_kwDOJ0Z1Ps8AAAABUc2aWA', 'url': 'https://api.github.com/repos/ollama/ollama/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': ""Something isn't working""}, {'id': 6430601766, 'node_id': 'LA_kwDOJ0Z1Ps8AAAABf0syJg', 'url': 'https://api.github.com/repos/ollama/ollama/labels/nvidia', 'name': 'nvidia', 'color': '8CDB00', 'default': False, 'description': 'Issues relating to Nvidia GPUs and CUDA'}, {'id': 6677677816, 'node_id': 'LA_kwDOJ0Z1Ps8AAAABjgVG-A', 'url': 'https://api.github.com/repos/ollama/ollama/labels/docker', 'name': 'docker', 'color': '0052CC', 'default': False, 'description': 'Issues relating to using ollama in containers'}]",open,False,"{'login': 'dhiltgen', 'id': 4033016, 'node_id': 'MDQ6VXNlcjQwMzMwMTY=', 'avatar_url': 'https://avatars.githubusercontent.com/u/4033016?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/dhiltgen', 'html_url': 'https://github.com/dhiltgen', 'followers_url': 'https://api.github.com/users/dhiltgen/followers', 'following_url': 'https://api.github.com/users/dhiltgen/following{/other_user}', 'gists_url': 'https://api.github.com/users/dhiltgen/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/dhiltgen/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/dhiltgen/subscriptions', 'organizations_url': 'https://api.github.com/users/dhiltgen/orgs', 'repos_url': 'https://api.github.com/users/dhiltgen/repos', 'events_url': 'https://api.github.com/users/dhiltgen/events{/privacy}', 'received_events_url': 'https://api.github.com/users/dhiltgen/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}","[{'login': 'dhiltgen', 'id': 4033016, 'node_id': 'MDQ6VXNlcjQwMzMwMTY=', 'avatar_url': 'https://avatars.githubusercontent.com/u/4033016?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/dhiltgen', 'html_url': 'https://github.com/dhiltgen', 'followers_url': 'https://api.github.com/users/dhiltgen/followers', 'following_url': 'https://api.github.com/users/dhiltgen/following{/other_user}', 'gists_url': 'https://api.github.com/users/dhiltgen/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/dhiltgen/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/dhiltgen/subscriptions', 'organizations_url': 'https://api.github.com/users/dhiltgen/orgs', 'repos_url': 'https://api.github.com/users/dhiltgen/repos', 'events_url': 'https://api.github.com/users/dhiltgen/events{/privacy}', 'received_events_url': 'https://api.github.com/users/dhiltgen/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}]",,3,2025-11-20T05:04:12Z,2025-11-22T02:21:08Z,,NONE,,,"{'total': 0, 'completed': 0, 'percent_completed': 0}","{'blocked_by': 0, 'total_blocked_by': 0, 'blocking': 0, 'total_blocking': 0}","### What is the issue?

Ollama 0.13.0 fails with 500 error when running on ARM64/Ampere

Downgrading to 0.12.9 fixes the issue.

```
nvidia-smi 
Thu Nov 20 13:49:55 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.247.01             Driver Version: 535.247.01   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA RTX A5000               On  | 00000002:01:00.0 Off |                  Off |
|  0%   36C    P8              14W / 230W |      1MiB / 24564MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA RTX A5000               On  | 00000003:01:00.0 Off |                  Off |
|  0%   37C    P8              16W / 230W |      1MiB / 24564MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|  No running processes found                                                           |
+---------------------------------------------------------------------------------------+
```

### Relevant log output

```shell
ollama  | //ml/backend/ggml/ggml/src/ggml-cuda/template-instances/../mma.cuh:445: ERROR: CUDA kernel mma has no device code compatible with CUDA arch 720. ggml-cuda.cu was compiled for: __CUDA_ARCH_LIST__
... (repeated)
ollama  | //ml/backend/ggml/ggml/src/ggml-cuda/template-instances/../mma.cuh:445: ERROR: CUDA kernel mma has no device code compatible with CUDA arch 720. ggml-cuda.cu was compiled for: __CUDA_ARCH_LIST__
ollama  | ggml_cuda_compute_forward: ROPE failed
ollama  | CUDA error: unspecified launch failure
ollama  |   current device: 0, in function ggml_cuda_compute_forward at //ml/backend/ggml/ggml/src/ggml-cuda/ggml-cuda.cu:2672
ollama  |   err
ollama  | //ml/backend/ggml/ggml/src/ggml-cuda/ggml-cuda.cu:88: CUDA error
ollama  | /usr/lib/ollama/libggml-base.so(+0x23050)[0xffff48033050]
ollama  | /usr/lib/ollama/libggml-base.so(ggml_print_backtrace+0x268)[0xffff4803302c]
ollama  | /usr/lib/ollama/libggml-base.so(ggml_abort+0xe0)[0xffff48031fc0]
ollama  | /usr/lib/ollama/cuda_jetpack5/libggml-cuda.so(+0xd6cd4)[0xffff004d6cd4]
ollama  | /usr/lib/ollama/cuda_jetpack5/libggml-cuda.so(+0xe3b74)[0xffff004e3b74]
ollama  | /usr/lib/ollama/cuda_jetpack5/libggml-cuda.so(+0xe4df4)[0xffff004e4df4]
ollama  | /usr/bin/ollama(+0xf339ec)[0xaaaab49b39ec]
ollama  | /usr/bin/ollama(+0xec4730)[0xaaaab4944730]
ollama  | /usr/bin/ollama(+0x370b4c)[0xaaaab3df0b4c]
ollama  | SIGABRT: abort
ollama  | PC=0xffff917c7608 m=26 sigcode=18446744073709551610
ollama  | signal arrived during cgo execution
ollama  | 
ollama  | goroutine 965 gp=0x40004f7880 m=26 mp=0x4000204008 [syscall]:
ollama  | runtime.cgocall(0xaaaab4944708, 0x40000bda78)
ollama  | 	runtime/cgocall.go:167 +0x44 fp=0x40000bda30 sp=0x40000bd9f0 pc=0xaaaab3de5954
ollama  | github.com/ollama/ollama/ml/backend/ggml._Cfunc_ggml_backend_sched_graph_compute_async(0xffff18000ba0, 0xfff8f000d8d0)
ollama  | 	_cgo_gotypes.go:963 +0x34 fp=0x40000bda70 sp=0x40000bda30 pc=0xaaaab41baa74
ollama  | github.com/ollama/ollama/ml/backend/ggml.(*Context).ComputeWithNotify.func2(...)
ollama  | 	github.com/ollama/ollama/ml/backend/ggml/ggml.go:825
ollama  | github.com/ollama/ollama/ml/backend/ggml.(*Context).ComputeWithNotify(0x4000416080, 0x4000ca38f0?, {0x4000e03580, 0x1, 0x2?})
ollama  | 	github.com/ollama/ollama/ml/backend/ggml/ggml.go:825 +0x1a8 fp=0x40000bdb50 sp=0x40000bda70 pc=0xaaaab41c47b8
ollama  | github.com/ollama/ollama/runner/ollamarunner.(*Server).computeBatch(0x400017c000, {0x0, {0xaaaab51268f0, 0x4000416080}, {0xaaaab5130e80, 0x40002e7c38}, {0x4000209800, 0xb, 0x10}, {{0xaaaab5130e80, ...}, ...}, ...})
ollama  | 	github.com/ollama/ollama/runner/ollamarunner/runner.go:723 +0x70c fp=0x40000bded0 sp=0x40000bdb50 pc=0xaaaab426b91c
ollama  | github.com/ollama/ollama/runner/ollamarunner.(*Server).run.gowrap1()
ollama  | 	github.com/ollama/ollama/runner/ollamarunner/runner.go:458 +0x5c fp=0x40000bdfd0 sp=0x40000bded0 pc=0xaaaab4269c8c
ollama  | runtime.goexit({})
ollama  | 	runtime/asm_arm64.s:1223 +0x4 fp=0x40000bdfd0 sp=0x40000bdfd0 pc=0xaaaab3df0d54
ollama  | created by github.com/ollama/ollama/runner/ollamarunner.(*Server).run in goroutine 66
ollama  | 	github.com/ollama/ollama/runner/ollamarunner/runner.go:458 +0x22c
... 
ollama  | goroutine 952 gp=0x40004f76c0 m=nil [chan receive]:
ollama  | runtime.gopark(0x736501?, 0x795465726977a0c4?, 0xf8?, 0xfa?, 0xaaaab41436a0?)
ollama  | 	runtime/proc.go:435 +0xc8 fp=0x40004dfaa0 sp=0x40004dfa80 pc=0xaaaab3de8e68
ollama  | runtime.chanrecv(0x4000132150, 0x0, 0x1)
ollama  | 	runtime/chan.go:664 +0x42c fp=0x40004dfb20 sp=0x40004dfaa0 pc=0xaaaab3d84bec
ollama  | runtime.chanrecv1(0xaaaab4c4e1fc?, 0x2c?)
ollama  | 	runtime/chan.go:506 +0x14 fp=0x40004dfb50 sp=0x40004dfb20 pc=0xaaaab3d84784
ollama  | github.com/ollama/ollama/runner/ollamarunner.(*Server).computeBatch(0x400017c000, {0x1, {0xaaaab51268f0, 0x4000466040}, {0xaaaab5130e80, 0x40003fd0f8}, {0x4001858008, 0x1, 0x1}, {{0xaaaab5130e80, ...}, ...}, ...})
ollama  | 	github.com/ollama/ollama/runner/ollamarunner/runner.go:651 +0x130 fp=0x40004dfed0 sp=0x40004dfb50 pc=0xaaaab426b340
ollama  | github.com/ollama/ollama/runner/ollamarunner.(*Server).run.gowrap1()
ollama  | 	github.com/ollama/ollama/runner/ollamarunner/runner.go:458 +0x5c fp=0x40004dffd0 sp=0x40004dfed0 pc=0xaaaab4269c8c
ollama  | runtime.goexit({})
ollama  | 	runtime/asm_arm64.s:1223 +0x4 fp=0x40004dffd0 sp=0x40004dffd0 pc=0xaaaab3df0d54
ollama  | created by github.com/ollama/ollama/runner/ollamarunner.(*Server).run in goroutine 66
ollama  | 	github.com/ollama/ollama/runner/ollamarunner/runner.go:458 +0x22c
ollama  | 
ollama  | r0      0x0
ollama  | r1      0x13a
ollama  | r2      0x6
ollama  | r3      0xfff9217af140
ollama  | r4      0xffff91d7eb50
ollama  | r5      0x1
ollama  | r6      0x20
ollama  | r7      0xfff9217ad8e0
ollama  | r8      0x83
ollama  | r9      0x0
ollama  | r10     0xa
ollama  | r11     0x101010101010101
ollama  | r12     0xfff9217ad970
ollama  | r13     0x0
ollama  | r14     0x0
ollama  | r15     0x2cd
ollama  | r16     0x0
ollama  | r17     0x0
ollama  | r18     0x2c8
ollama  | r19     0x13a
ollama  | r20     0xfff9217af140
ollama  | r21     0x6
ollama  | r22     0xa70
ollama  | r23     0xffff40605268
ollama  | r24     0xffff1857d4e8
ollama  | r25     0xffff0ffec000
ollama  | r26     0xaaaac5bed210
ollama  | r27     0xffff0ffeca58
ollama  | r28     0xfff9217ae530
ollama  | r29     0xfff9217ad870
ollama  | lr      0xffff917c75f4
ollama  | sp      0xfff9217ad860
ollama  | pc      0xffff917c7608
ollama  | fault   0x0
ollama  | time=2025-11-20T04:40:34.280Z level=ERROR source=server.go:1539 msg=""post predict"" error=""Post \""http://127.0.0.1:33547/completion\"": EOF""
ollama  | [GIN] 2025/11/20 - 04:40:34 | 500 |         7m47s |      172.21.0.1 | POST     ""/api/chat""
ollama  | [GIN] 2025/11/20 - 04:40:34 | 500 |         12m0s |      172.21.0.1 | POST     ""/api/chat""
ollama  | time=2025-11-20T04:40:34.375Z level=ERROR source=server.go:265 msg=""llama runner terminated"" error=""exit status 2""
```

### OS

Docker

### GPU

Nvidia

### CPU

Other

### Ollama version

0.13.0",,"{'url': 'https://api.github.com/repos/ollama/ollama/issues/13169/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}",https://api.github.com/repos/ollama/ollama/issues/13169/timeline,,,,
https://api.github.com/repos/ollama/ollama/issues/13168,https://api.github.com/repos/ollama/ollama,https://api.github.com/repos/ollama/ollama/issues/13168/labels{/name},https://api.github.com/repos/ollama/ollama/issues/13168/comments,https://api.github.com/repos/ollama/ollama/issues/13168/events,https://github.com/ollama/ollama/issues/13168,3645341156,I_kwDOJ0Z1Ps7ZR33k,13168,deepseek-ocr模型对话重复,"{'login': 'Adam-xuya', 'id': 222342442, 'node_id': 'U_kgDODUCtKg', 'avatar_url': 'https://avatars.githubusercontent.com/u/222342442?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/Adam-xuya', 'html_url': 'https://github.com/Adam-xuya', 'followers_url': 'https://api.github.com/users/Adam-xuya/followers', 'following_url': 'https://api.github.com/users/Adam-xuya/following{/other_user}', 'gists_url': 'https://api.github.com/users/Adam-xuya/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/Adam-xuya/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/Adam-xuya/subscriptions', 'organizations_url': 'https://api.github.com/users/Adam-xuya/orgs', 'repos_url': 'https://api.github.com/users/Adam-xuya/repos', 'events_url': 'https://api.github.com/users/Adam-xuya/events{/privacy}', 'received_events_url': 'https://api.github.com/users/Adam-xuya/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}","[{'id': 5667396184, 'node_id': 'LA_kwDOJ0Z1Ps8AAAABUc2aWA', 'url': 'https://api.github.com/repos/ollama/ollama/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': ""Something isn't working""}]",open,False,,[],,4,2025-11-20T03:58:12Z,2025-11-21T12:56:36Z,,NONE,,,"{'total': 0, 'completed': 0, 'percent_completed': 0}","{'blocked_by': 0, 'total_blocked_by': 0, 'blocking': 0, 'total_blocking': 0}","### What is the issue?

发送消息，不管是啥，回复的消息始终无限重复

<img width=""1500"" height=""1472"" alt=""Image"" src=""https://github.com/user-attachments/assets/2ecc82a4-acc2-4cd1-bd3b-1390a6d3a45f"" />

<img width=""2642"" height=""470"" alt=""Image"" src=""https://github.com/user-attachments/assets/7a269c68-22de-4e32-a0b6-c55fe32d8131"" />

### Relevant log output

```shell

```

### OS

mac os 26.1

### GPU

_No response_

Apple M1 Max

_No response_

### Ollama version

0.13",,"{'url': 'https://api.github.com/repos/ollama/ollama/issues/13168/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}",https://api.github.com/repos/ollama/ollama/issues/13168/timeline,,,,
https://api.github.com/repos/ollama/ollama/issues/13167,https://api.github.com/repos/ollama/ollama,https://api.github.com/repos/ollama/ollama/issues/13167/labels{/name},https://api.github.com/repos/ollama/ollama/issues/13167/comments,https://api.github.com/repos/ollama/ollama/issues/13167/events,https://github.com/ollama/ollama/issues/13167,3645226044,I_kwDOJ0Z1Ps7ZRbw8,13167,download.go: no resumable download support — failed transfers restart from zero,"{'login': 'dimzon', 'id': 908946, 'node_id': 'MDQ6VXNlcjkwODk0Ng==', 'avatar_url': 'https://avatars.githubusercontent.com/u/908946?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/dimzon', 'html_url': 'https://github.com/dimzon', 'followers_url': 'https://api.github.com/users/dimzon/followers', 'following_url': 'https://api.github.com/users/dimzon/following{/other_user}', 'gists_url': 'https://api.github.com/users/dimzon/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/dimzon/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/dimzon/subscriptions', 'organizations_url': 'https://api.github.com/users/dimzon/orgs', 'repos_url': 'https://api.github.com/users/dimzon/repos', 'events_url': 'https://api.github.com/users/dimzon/events{/privacy}', 'received_events_url': 'https://api.github.com/users/dimzon/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}",[],open,False,,[],,1,2025-11-20T02:56:08Z,2025-11-20T09:17:41Z,,NONE,,,"{'total': 0, 'completed': 0, 'percent_completed': 0}","{'blocked_by': 0, 'total_blocked_by': 0, 'blocking': 0, 'total_blocking': 0}","The current implementation of `pull` in `download.go` does **not support resumable downloads**. When a download stalls or fails (e.g. due to network timeout, proxy interruption, or connection reset), retrying `ollama pull <model>` **restarts the entire download from scratch**, even if the majority of layers have already been partially or fully fetched.

This behavior is observed in:
- Unstable network conditions
- High-latency or metered connections
- Proxy/tunnel setups (e.g. `ssh -R` + `privoxy`)
- Large models (e.g. 7B+, GGUF, or quantized variants)

## Example log:
```
time=2025-11-20T05:39:39.801+03:00 level=INFO source=download.go:374 msg=""7462734796d6 part 11 stalled; retrying. If this persists, press ctrl-c to exit, then 'ollama pull' to find a faster connection.""
```

After interruption and re-run:
```
pulling manifest
pulling 7462734796d6... 0 B / ? MB
```
→ **No attempt to resume. No use of `Range` headers. No partial file reuse.**

### Root Cause

- Downloaded layer data is stored in `~/.ollama/tmp/` but **deleted on failure**
- No use of **partial file persistence** or **checksum validation** to resume
- HTTP client does not send `Range: bytes=...` on retry
- No retry logic with exponential backoff or connection resilience
- No support for `--retry`, `--timeout`, `--proxy`, or `--header` flags

## ✅ Expected Behavior

`ollama pull` should:
1. **Persist partial downloads** in `~/.ollama/tmp/` until completion
2. On restart, **verify existing partial data** (via size or chunk hash)
3. Use `Range: bytes=N-` HTTP header to **resume from server**
4. Retry with backoff on transient errors (408, 429, 502, 503, connection reset)
5. Support resumable transfers **without full re-download**

This is standard in:
- `docker pull` (uses content-addressable layers, resumes on reconnect)
- `skopeo copy` (resumable, retry-aware)
- `curl`, `wget`, `aria2c` (support `--continue-at`, `--retry`)

## 🛠 Proposed Solution

1. **Modify download logic** to:
   -• Keep .part files in `~/.ollama/tmp/` after failed download
   -• On retry, check `Content-Length` and send `Range: bytes=<current_size>-`
   -• Fall back to full download only if server doesn’t support `206 Partial Content`

2. **Add retry resilience**:
```go
client := &http.Client{
    Transport: &http.Transport{
        MaxIdleConns:        10,
        IdleConnTimeout:     30 * time.Second,
        TLSHandshakeTimeout: 10 * time.Second,
    },
    CheckRedirect: func(req *http.Request, via []*http.Request) error {
        return http.ErrUseLastResponse
    },
}
```
And wrap download loop with:
```go
for attempt := 0; attempt < maxRetries; attempt  {
    if err := downloadChunk(url, offset); err != nil {
        time.Sleep(backoff(attempt))
        continue
    }
    break
}
```

3. **Optional CLI flags** (future):
- `--retry <n>`
- `--retry-delay <seconds>`
- `--proxy <url>`
- `--header <key: value>`
- `--output <path>`

## 🧪 Reproduction Steps

1. Run: `ollama pull aya:8b`
2. Interrupt download (Ctrl+C) after 30–50%
3. Re-run: `ollama pull aya:8b`
4. Observe: download starts from 0%, all data re-fetched

## 🧩 Environment

- Ollama version: `0.1.26` (or latest)
- OS: Linux x86_64
- Network: unstable, high RTT, behind proxy
- Model: `aya:8b`, `llama3:8b`, etc.
- Cache dir: `~/.ollama/tmp/`

## 📌 Impact

Without resumable downloads, `ollama` is **not viable** for:
- Users with limited bandwidth
- Remote/cloud setups over SSH tunnels
- Automated workflows in unstable environments
- Large model distribution in production

This is not a QoL improvement — it's a **core reliability requirement**.",,"{'url': 'https://api.github.com/repos/ollama/ollama/issues/13167/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}",https://api.github.com/repos/ollama/ollama/issues/13167/timeline,,,,
https://api.github.com/repos/ollama/ollama/issues/13166,https://api.github.com/repos/ollama/ollama,https://api.github.com/repos/ollama/ollama/issues/13166/labels{/name},https://api.github.com/repos/ollama/ollama/issues/13166/comments,https://api.github.com/repos/ollama/ollama/issues/13166/events,https://github.com/ollama/ollama/pull/13166,3644866158,PR_kwDOJ0Z1Ps60d58f,13166,deepseek2: upgrade to run v3+ models,"{'login': 'mxyng', 'id': 2372640, 'node_id': 'MDQ6VXNlcjIzNzI2NDA=', 'avatar_url': 'https://avatars.githubusercontent.com/u/2372640?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/mxyng', 'html_url': 'https://github.com/mxyng', 'followers_url': 'https://api.github.com/users/mxyng/followers', 'following_url': 'https://api.github.com/users/mxyng/following{/other_user}', 'gists_url': 'https://api.github.com/users/mxyng/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/mxyng/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/mxyng/subscriptions', 'organizations_url': 'https://api.github.com/users/mxyng/orgs', 'repos_url': 'https://api.github.com/users/mxyng/repos', 'events_url': 'https://api.github.com/users/mxyng/events{/privacy}', 'received_events_url': 'https://api.github.com/users/mxyng/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}",[],closed,False,,[],,0,2025-11-20T00:16:45Z,2025-11-20T01:05:41Z,2025-11-20T01:05:39Z,CONTRIBUTOR,,,,,the check for mla omits v3 and r1 which should not return unsupported. instead check the tokenizer for compatibility,"{'login': 'mxyng', 'id': 2372640, 'node_id': 'MDQ6VXNlcjIzNzI2NDA=', 'avatar_url': 'https://avatars.githubusercontent.com/u/2372640?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/mxyng', 'html_url': 'https://github.com/mxyng', 'followers_url': 'https://api.github.com/users/mxyng/followers', 'following_url': 'https://api.github.com/users/mxyng/following{/other_user}', 'gists_url': 'https://api.github.com/users/mxyng/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/mxyng/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/mxyng/subscriptions', 'organizations_url': 'https://api.github.com/users/mxyng/orgs', 'repos_url': 'https://api.github.com/users/mxyng/repos', 'events_url': 'https://api.github.com/users/mxyng/events{/privacy}', 'received_events_url': 'https://api.github.com/users/mxyng/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}","{'url': 'https://api.github.com/repos/ollama/ollama/issues/13166/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}",https://api.github.com/repos/ollama/ollama/issues/13166/timeline,,,False,"{'url': 'https://api.github.com/repos/ollama/ollama/pulls/13166', 'html_url': 'https://github.com/ollama/ollama/pull/13166', 'diff_url': 'https://github.com/ollama/ollama/pull/13166.diff', 'patch_url': 'https://github.com/ollama/ollama/pull/13166.patch', 'merged_at': '2025-11-20T01:05:39Z'}"
https://api.github.com/repos/ollama/ollama/issues/13165,https://api.github.com/repos/ollama/ollama,https://api.github.com/repos/ollama/ollama/issues/13165/labels{/name},https://api.github.com/repos/ollama/ollama/issues/13165/comments,https://api.github.com/repos/ollama/ollama/issues/13165/events,https://github.com/ollama/ollama/pull/13165,3644849247,PR_kwDOJ0Z1Ps60d2cC,13165,Support overriding tensor-split from config,"{'login': 'seulement55', 'id': 30664075, 'node_id': 'MDQ6VXNlcjMwNjY0MDc1', 'avatar_url': 'https://avatars.githubusercontent.com/u/30664075?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/seulement55', 'html_url': 'https://github.com/seulement55', 'followers_url': 'https://api.github.com/users/seulement55/followers', 'following_url': 'https://api.github.com/users/seulement55/following{/other_user}', 'gists_url': 'https://api.github.com/users/seulement55/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/seulement55/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/seulement55/subscriptions', 'organizations_url': 'https://api.github.com/users/seulement55/orgs', 'repos_url': 'https://api.github.com/users/seulement55/repos', 'events_url': 'https://api.github.com/users/seulement55/events{/privacy}', 'received_events_url': 'https://api.github.com/users/seulement55/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}",[],open,False,,[],,0,2025-11-20T00:08:34Z,2025-11-20T00:08:34Z,,NONE,,,,,"Resolves #12010
Resolves #10172
Resolves #7047

---

When it comes to automatic determination of how to optimally split a model that doesn't fit into VRAM of a single GPU, ollama does a rather poor job. Up until 0.11.4, the binary could be wrapped with a shell script and the layer split parameters overridden to manually optimize the layer split across GPUs, but this went away with 0.11.5.

How bad is the problem? For example, for llama 3.x 70b models and variants thereof (e.g. hermes 4), ollama offloads a total of 19/81 layers to my 4x22GB GPUs with context set to maximum 128K. With the manual override configured to 18,21,21,21 81/81 layers are offloaded, still with the context maxed out at 128K, without OOMs, tested up to full 128K of context.

While evolving better default heuristics is appreciated, it seems very unrealistic to expect that they will ever be perfect for every model, and when they are not, there is currently no way to override the split to optimize it. hermes4:70b 19/81 with layers offloaded is completely unusable, but with 81/81 in VRAM across the 4 GPUs, it is very quick.

This patch introduces a clean feature to optionally explicitly override the layer split on a model-by-model basis using overrides stored in an .ini file (defaults to `~/.ollama.ini`). If the file is present (config file path/name can be specified using `OLLAMA_OVERRIDE_CONFIG` if anyone deems the default unsuitable for any reason), it will be used. If it is not, or the requested model doesn't have an override config specified in the .ini file, default heuristic will be used.
Example config block in the .ini file:

```ini
[hermes4:70b]
tensor-split=18,21,21,21
```

Test cases are also included for regression testing against future updates.",,"{'url': 'https://api.github.com/repos/ollama/ollama/issues/13165/reactions', 'total_count': 1, '+1': 1, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}",https://api.github.com/repos/ollama/ollama/issues/13165/timeline,,,False,"{'url': 'https://api.github.com/repos/ollama/ollama/pulls/13165', 'html_url': 'https://github.com/ollama/ollama/pull/13165', 'diff_url': 'https://github.com/ollama/ollama/pull/13165.diff', 'patch_url': 'https://github.com/ollama/ollama/pull/13165.patch', 'merged_at': None}"
https://api.github.com/repos/ollama/ollama/issues/13164,https://api.github.com/repos/ollama/ollama,https://api.github.com/repos/ollama/ollama/issues/13164/labels{/name},https://api.github.com/repos/ollama/ollama/issues/13164/comments,https://api.github.com/repos/ollama/ollama/issues/13164/events,https://github.com/ollama/ollama/pull/13164,3644718772,PR_kwDOJ0Z1Ps60daz4,13164,app: open app instead of always navigating to / on connect,"{'login': 'jmorganca', 'id': 251292, 'node_id': 'MDQ6VXNlcjI1MTI5Mg==', 'avatar_url': 'https://avatars.githubusercontent.com/u/251292?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/jmorganca', 'html_url': 'https://github.com/jmorganca', 'followers_url': 'https://api.github.com/users/jmorganca/followers', 'following_url': 'https://api.github.com/users/jmorganca/following{/other_user}', 'gists_url': 'https://api.github.com/users/jmorganca/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/jmorganca/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/jmorganca/subscriptions', 'organizations_url': 'https://api.github.com/users/jmorganca/orgs', 'repos_url': 'https://api.github.com/users/jmorganca/repos', 'events_url': 'https://api.github.com/users/jmorganca/events{/privacy}', 'received_events_url': 'https://api.github.com/users/jmorganca/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}",[],closed,False,,[],,0,2025-11-19T23:00:10Z,2025-11-20T20:59:19Z,2025-11-20T20:59:18Z,MEMBER,,,,,,"{'login': 'jmorganca', 'id': 251292, 'node_id': 'MDQ6VXNlcjI1MTI5Mg==', 'avatar_url': 'https://avatars.githubusercontent.com/u/251292?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/jmorganca', 'html_url': 'https://github.com/jmorganca', 'followers_url': 'https://api.github.com/users/jmorganca/followers', 'following_url': 'https://api.github.com/users/jmorganca/following{/other_user}', 'gists_url': 'https://api.github.com/users/jmorganca/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/jmorganca/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/jmorganca/subscriptions', 'organizations_url': 'https://api.github.com/users/jmorganca/orgs', 'repos_url': 'https://api.github.com/users/jmorganca/repos', 'events_url': 'https://api.github.com/users/jmorganca/events{/privacy}', 'received_events_url': 'https://api.github.com/users/jmorganca/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}","{'url': 'https://api.github.com/repos/ollama/ollama/issues/13164/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}",https://api.github.com/repos/ollama/ollama/issues/13164/timeline,,,False,"{'url': 'https://api.github.com/repos/ollama/ollama/pulls/13164', 'html_url': 'https://github.com/ollama/ollama/pull/13164', 'diff_url': 'https://github.com/ollama/ollama/pull/13164.diff', 'patch_url': 'https://github.com/ollama/ollama/pull/13164.patch', 'merged_at': '2025-11-20T20:59:18Z'}"
https://api.github.com/repos/ollama/ollama/issues/13163,https://api.github.com/repos/ollama/ollama,https://api.github.com/repos/ollama/ollama/issues/13163/labels{/name},https://api.github.com/repos/ollama/ollama/issues/13163/comments,https://api.github.com/repos/ollama/ollama/issues/13163/events,https://github.com/ollama/ollama/issues/13163,3644403166,I_kwDOJ0Z1Ps7ZOS3e,13163,Ollama 0.12.11 Not Using GPU on RTX 5070 Ti (Blackwell/CC 12.0),"{'login': 'deparko', 'id': 4720544, 'node_id': 'MDQ6VXNlcjQ3MjA1NDQ=', 'avatar_url': 'https://avatars.githubusercontent.com/u/4720544?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/deparko', 'html_url': 'https://github.com/deparko', 'followers_url': 'https://api.github.com/users/deparko/followers', 'following_url': 'https://api.github.com/users/deparko/following{/other_user}', 'gists_url': 'https://api.github.com/users/deparko/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/deparko/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/deparko/subscriptions', 'organizations_url': 'https://api.github.com/users/deparko/orgs', 'repos_url': 'https://api.github.com/users/deparko/repos', 'events_url': 'https://api.github.com/users/deparko/events{/privacy}', 'received_events_url': 'https://api.github.com/users/deparko/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}","[{'id': 5667396184, 'node_id': 'LA_kwDOJ0Z1Ps8AAAABUc2aWA', 'url': 'https://api.github.com/repos/ollama/ollama/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': ""Something isn't working""}, {'id': 6430601766, 'node_id': 'LA_kwDOJ0Z1Ps8AAAABf0syJg', 'url': 'https://api.github.com/repos/ollama/ollama/labels/nvidia', 'name': 'nvidia', 'color': '8CDB00', 'default': False, 'description': 'Issues relating to Nvidia GPUs and CUDA'}]",open,False,"{'login': 'dhiltgen', 'id': 4033016, 'node_id': 'MDQ6VXNlcjQwMzMwMTY=', 'avatar_url': 'https://avatars.githubusercontent.com/u/4033016?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/dhiltgen', 'html_url': 'https://github.com/dhiltgen', 'followers_url': 'https://api.github.com/users/dhiltgen/followers', 'following_url': 'https://api.github.com/users/dhiltgen/following{/other_user}', 'gists_url': 'https://api.github.com/users/dhiltgen/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/dhiltgen/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/dhiltgen/subscriptions', 'organizations_url': 'https://api.github.com/users/dhiltgen/orgs', 'repos_url': 'https://api.github.com/users/dhiltgen/repos', 'events_url': 'https://api.github.com/users/dhiltgen/events{/privacy}', 'received_events_url': 'https://api.github.com/users/dhiltgen/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}","[{'login': 'dhiltgen', 'id': 4033016, 'node_id': 'MDQ6VXNlcjQwMzMwMTY=', 'avatar_url': 'https://avatars.githubusercontent.com/u/4033016?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/dhiltgen', 'html_url': 'https://github.com/dhiltgen', 'followers_url': 'https://api.github.com/users/dhiltgen/followers', 'following_url': 'https://api.github.com/users/dhiltgen/following{/other_user}', 'gists_url': 'https://api.github.com/users/dhiltgen/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/dhiltgen/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/dhiltgen/subscriptions', 'organizations_url': 'https://api.github.com/users/dhiltgen/orgs', 'repos_url': 'https://api.github.com/users/dhiltgen/repos', 'events_url': 'https://api.github.com/users/dhiltgen/events{/privacy}', 'received_events_url': 'https://api.github.com/users/dhiltgen/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}]",,8,2025-11-19T20:59:14Z,2025-11-22T04:11:31Z,,NONE,,,"{'total': 0, 'completed': 0, 'percent_completed': 0}","{'blocked_by': 0, 'total_blocked_by': 0, 'blocking': 0, 'total_blocking': 0}","### What is the issue?


### Description

Ollama 0.12.11 fails to detect and use the GPU for local models on NVIDIA GeForce RTX 5070 Ti (Blackwell architecture, Compute Capability 12.0). The GPU is functional and accessible, but Ollama immediately falls back to CPU-only mode without error messages.

**Critical**: This worked before November 17, 2025, indicating a regression or compatibility issue with Blackwell architecture.

### Environment

- **OS**: Ubuntu 25.04 (GNU/Linux 6.14.0-35-generic x86_64)
- **GPU**: NVIDIA GeForce RTX 5070 Ti (16GB VRAM)
- **GPU Compute Capability**: 12.0 (Blackwell architecture)
- **GPU Driver**: 580.95.05
- **CUDA Runtime**: 12.2.140
- **Ollama Version**: 0.12.11 (latest, clean install)
- **Installation Method**: Standalone binary via systemd service

### Steps to Reproduce

1. Install Ollama 0.12.11 on system with RTX 5070 Ti
2. Configure minimal systemd override:
   ```ini
   [Service]
   Environment=OLLAMA_MODELS=/mnt/shared/ollama-models/models
   Environment=CUDA_VISIBLE_DEVICES=0
   ```
3. Start Ollama service: `sudo systemctl start ollama.service`
4. Load a model: `ollama run llama3.1:8b`
5. Check GPU usage: `ollama ps` or `curl http://localhost:11434/api/ps`

### Expected Behavior

- Ollama should detect GPU and initialize CUDA backend
- Models should offload layers to GPU
- `ollama ps` should show non-zero `size_vram`
- Logs should show: `ggml_cuda_init: found 1 CUDA devices` and `load_backend: loaded CUDA backend`

### Actual Behavior

- Ollama discovers GPU but immediately falls back to CPU
- All models show `size_vram: 0 MB`
- Logs show:
  ```
  msg=""discovering available GPUs...""
  msg=""inference compute"" id=cpu library=cpu
  msg=""entering low vram mode"" ""total vram""=""0 B""
  ```
- No error messages (silent fallback)
- Models run on CPU (slow performance: ~60+ seconds for simple queries)

### Evidence It Previously Worked

**Logs from November 17, 2025** (when GPU was working):
```
ggml_cuda_init: found 1 CUDA devices
load_backend: loaded CUDA backend from /usr/local/lib/ollama/cuda_v13/libggml-cuda.so
device=GPU for model weights and KV cache
offloaded 41/41 layers to GPU
```

**After system reboot on November 18, 2025**: GPU detection stopped working.

### Troubleshooting Attempted

1. ✅ Set environment variables (`OLLAMA_NUM_GPU=1`, `CUDA_VISIBLE_DEVICES=0`)
2. ✅ Reinstalled Ollama binary (v0.12.11 from GitHub releases)
3. ✅ Manual CUDA library path configuration (`LD_LIBRARY_PATH`)
4. ✅ Created symlinks for CUDA libraries
5. ✅ **Clean install**: Complete removal of all Ollama files/configs + fresh install
6. ✅ Minimal configuration (removed all manual overrides, let Ollama auto-discover)

**Result**: All attempts show identical behavior - GPU discovery runs but immediately falls back to CPU within ~13ms.

### GPU Verification

GPU is functional and accessible:
```bash
$ nvidia-smi
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 580.95.05              Driver Version: 580.95.05      CUDA Version: 13.0     |
+-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|   0  NVIDIA GeForce RTX 5070 Ti     Off |   00000000:01:00.0 Off |                  N/A |
|  0%   40C    P8             18W /  300W |   13051MiB /  16303MiB |      0%      Default |
+-----------------------------------------+------------------------+----------------------+
```

Other services successfully use GPU:
- RAG service uses GPU for embeddings/reranking (NomicEmbedder + BGE Reranker on CUDA)
- gnome-shell uses GPU for graphics

### Ollama Service Logs

```bash
$ journalctl -u ollama.service --since '10 seconds ago' | grep -i -E 'gpu|cuda|vram|compute|discovering'
Nov 18 22:49:19 Tatami ollama[31412]: msg=""discovering available GPUs...""
Nov 18 22:49:19 Tatami ollama[31412]: msg=""inference compute"" id=cpu library=cpu
Nov 18 22:49:19 Tatami ollama[31412]: msg=""entering low vram mode"" ""total vram""=""0 B""
```

### Model Status

```bash
$ curl -s http://localhost:11434/api/ps | jq
[
  {
    ""name"": ""llama3.1:8b"",
    ""model"": ""llama3.1:8b"",
    ""size"": 4630000000,
    ""size_vram"": 0,  # <-- Should be non-zero
    ""context_length"": 4096
  }
]
```

### Hypothesis

**Ollama 0.12.11 may not support Compute Capability 12.0 (Blackwell architecture) yet.**

The RTX 5070 Ti is very new hardware, and Ollama's bundled CUDA runners may not include kernels compiled for CC 12.0. When CUDA backend initialization fails, Ollama gracefully falls back to CPU without error messages.

### Questions

1. Does Ollama 0.12.11 support Compute Capability 12.0 (Blackwell)?
2. Are there any debug flags to get more verbose CUDA initialization logs?
3. Is there a known issue or workaround for RTX 50-series GPUs?
4. Should I try rolling back to an older Ollama version that worked before Nov 17?

### Additional Context

- **Models tested**: `llama3.1:8b`, `qwen3:14b`, `qwen:14b` - all show same behavior
- **Cloud models**: Work fine (authenticated with Ollama Cloud)
- **Service configuration**: Minimal systemd override (no manual library paths)

### Related

- This may be related to CUDA compute capability support
- Similar issues may exist for other RTX 50-series GPUs (Blackwell architecture)

### Relevant log output

```shell
### Ollama Service Logs


Nov 19 13:20:52 Tatami systemd[1]: Started ollama.service - Ollama Service.
Nov 19 13:20:52 Tatami ollama[84614]: time=2025-11-19T13:20:52.536-08:00 level=INFO source=routes.go:1544 msg=""server config"" env=""map[CUDA_VISIBLE_DEVICES:0 GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11434 OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/mnt/shared/ollama-models/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false OLLAMA_VULKAN:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]""
Nov 19 13:20:52 Tatami ollama[84614]: time=2025-11-19T13:20:52.539-08:00 level=INFO source=images.go:522 msg=""total blobs: 73""
Nov 19 13:20:52 Tatami ollama[84614]: time=2025-11-19T13:20:52.539-08:00 level=INFO source=images.go:529 msg=""total unused blobs removed: 0""
Nov 19 13:20:52 Tatami ollama[84614]: time=2025-11-19T13:20:52.540-08:00 level=INFO source=routes.go:1597 msg=""Listening on [::]:11434 (version 0.12.11)""
Nov 19 13:20:52 Tatami ollama[84614]: time=2025-11-19T13:20:52.540-08:00 level=INFO source=runner.go:67 msg=""discovering available GPUs...""
Nov 19 13:20:52 Tatami ollama[84614]: time=2025-11-19T13:20:52.540-08:00 level=INFO source=server.go:392 msg=""starting runner"" cmd=""/usr/local/bin/ollama runner --ollama-engine --port 35681""
Nov 19 13:20:52 Tatami ollama[84614]: time=2025-11-19T13:20:52.555-08:00 level=INFO source=types.go:60 msg=""inference compute"" id=cpu library=cpu compute="""" name=cpu description=cpu libdirs=ollama driver="""" pci_id="""" type="""" total=""122.6 GiB"" available=""109.1 GiB""
Nov 19 13:20:52 Tatami ollama[84614]: time=2025-11-19T13:20:52.555-08:00 level=INFO source=routes.go:1638 msg=""entering low vram mode"" ""total vram""=""0 B"" threshold=""20.0 GiB""
Nov 19 13:20:52 Tatami systemd[1]: Started ollama.service - Ollama Service.
Nov 19 13:20:52 Tatami ollama[84614]: time=2025-11-19T13:20:52.536-08:00 level=INFO source=routes.go:1544 msg=""server config"" env=""map[CUDA_VISIBLE_DEVICES:0 GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11434 OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/mnt/shared/ollama-models/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false OLLAMA_VULKAN:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]""
Nov 19 13:20:52 Tatami ollama[84614]: time=2025-11-19T13:20:52.539-08:00 level=INFO source=images.go:522 msg=""total blobs: 73""
Nov 19 13:20:52 Tatami ollama[84614]: time=2025-11-19T13:20:52.539-08:00 level=INFO source=images.go:529 msg=""total unused blobs removed: 0""
Nov 19 13:20:52 Tatami ollama[84614]: time=2025-11-19T13:20:52.540-08:00 level=INFO source=routes.go:1597 msg=""Listening on [::]:11434 (version 0.12.11)""
Nov 19 13:20:52 Tatami ollama[84614]: time=2025-11-19T13:20:52.540-08:00 level=INFO source=runner.go:67 msg=""discovering available GPUs...""
Nov 19 13:20:52 Tatami ollama[84614]: time=2025-11-19T13:20:52.540-08:00 level=INFO source=server.go:392 msg=""starting runner"" cmd=""/usr/local/bin/ollama runner --ollama-engine --port 35681""
Nov 19 13:20:52 Tatami ollama[84614]: time=2025-11-19T13:20:52.555-08:00 level=INFO source=types.go:60 msg=""inference compute"" id=cpu library=cpu compute="""" name=cpu description=cpu libdirs=ollama driver="""" pci_id="""" type="""" total=""122.6 GiB"" available=""109.1 GiB""
Nov 19 13:20:52 Tatami ollama[84614]: time=2025-11-19T13:20:52.555-08:00 level=INFO source=routes.go:1638 msg=""entering low vram mode"" ""total vram""=""0 B"" threshold=""20.0 GiB""
Nov 19 13:20:58 Tatami ollama[84614]: llama_model_loader: loaded meta data with 29 key-value pairs and 292 tensors from /mnt/shared/ollama-models/models/blobs/sha256-667b0c1932bc6ffc593ed1d03f895bf2dc8dc6df21db3042284a6f4416b06a29 (version GGUF V3 (latest))
Nov 19 13:20:58 Tatami ollama[84614]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Nov 19 13:20:58 Tatami ollama[84614]: llama_model_loader: - kv   0:                       general.architecture str              = llama
Nov 19 13:20:58 Tatami ollama[84614]: llama_model_loader: - kv   1:                               general.type str              = model
Nov 19 13:20:58 Tatami ollama[84614]: llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 8B Instruct
Nov 19 13:20:58 Tatami ollama[84614]: llama_model_loader: - kv   3:                           general.finetune str              = Instruct
Nov 19 13:20:58 Tatami ollama[84614]: llama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1
Nov 19 13:20:58 Tatami ollama[84614]: llama_model_loader: - kv   5:                         general.size_label str              = 8B
Nov 19 13:20:58 Tatami ollama[84614]: llama_model_loader: - kv   6:                            general.license str              = llama3.1
Nov 19 13:20:58 Tatami ollama[84614]: llama_model_loader: - kv   7:                               general.tags arr[str,6]       = [""facebook"", ""meta"", ""pytorch"", ""llam...
Nov 19 13:20:58 Tatami ollama[84614]: llama_model_loader: - kv   8:                          general.languages arr[str,8]       = [""en"", ""de"", ""fr"", ""it"", ""pt"", ""hi"", ...
Nov 19 13:20:58 Tatami ollama[84614]: llama_model_loader: - kv   9:                          llama.block_count u32              = 32
Nov 19 13:20:58 Tatami ollama[84614]: llama_model_loader: - kv  10:                       llama.context_length u32              = 131072
Nov 19 13:20:58 Tatami ollama[84614]: llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096
Nov 19 13:20:58 Tatami ollama[84614]: llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 14336
Nov 19 13:20:58 Tatami ollama[84614]: llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 32
Nov 19 13:20:58 Tatami ollama[84614]: llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8
Nov 19 13:20:58 Tatami ollama[84614]: llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000
Nov 19 13:20:58 Tatami ollama[84614]: llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
Nov 19 13:20:58 Tatami ollama[84614]: llama_model_loader: - kv  17:                          general.file_type u32              = 15
Nov 19 13:20:58 Tatami ollama[84614]: llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256
Nov 19 13:20:58 Tatami ollama[84614]: llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128
Nov 19 13:20:58 Tatami ollama[84614]: llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2
Nov 19 13:20:58 Tatami ollama[84614]: llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe
Nov 19 13:20:58 Tatami ollama[84614]: llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = [""!"", ""\"""", ""#"", ""$"", ""%"", ""&"", ""'"", ...
Nov 19 13:20:58 Tatami ollama[84614]: llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Nov 19 13:20:58 Tatami ollama[84614]: llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = [""Ġ Ġ"", ""Ġ ĠĠĠ"", ""ĠĠ ĠĠ"", ""...
Nov 19 13:20:58 Tatami ollama[84614]: llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000
Nov 19 13:20:58 Tatami ollama[84614]: llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009
Nov 19 13:20:58 Tatami ollama[84614]: llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
Nov 19 13:20:58 Tatami ollama[84614]: llama_model_loader: - kv  28:               general.quantization_version u32              = 2
Nov 19 13:20:58 Tatami ollama[84614]: llama_model_loader: - type  f32:   66 tensors
Nov 19 13:20:58 Tatami ollama[84614]: llama_model_loader: - type q4_K:  193 tensors
Nov 19 13:20:58 Tatami ollama[84614]: llama_model_loader: - type q6_K:   33 tensors
Nov 19 13:20:58 Tatami ollama[84614]: print_info: file format = GGUF V3 (latest)
Nov 19 13:20:58 Tatami ollama[84614]: print_info: file type   = Q4_K - Medium
Nov 19 13:20:58 Tatami ollama[84614]: print_info: file size   = 4.58 GiB (4.89 BPW)
Nov 19 13:20:58 Tatami ollama[84614]: load: printing all EOG tokens:
Nov 19 13:20:58 Tatami ollama[84614]: load:   - 128001 ('<|end_of_text|>')
Nov 19 13:20:58 Tatami ollama[84614]: load:   - 128008 ('<|eom_id|>')
Nov 19 13:20:58 Tatami ollama[84614]: load:   - 128009 ('<|eot_id|>')
Nov 19 13:20:58 Tatami ollama[84614]: load: special tokens cache size = 256
Nov 19 13:20:58 Tatami ollama[84614]: load: token to piece cache size = 0.7999 MB
Nov 19 13:20:58 Tatami ollama[84614]: print_info: arch             = llama
Nov 19 13:20:58 Tatami ollama[84614]: print_info: vocab_only       = 1
Nov 19 13:20:58 Tatami ollama[84614]: print_info: model type       = ?B
Nov 19 13:20:58 Tatami ollama[84614]: print_info: model params     = 8.03 B
Nov 19 13:20:58 Tatami ollama[84614]: print_info: general.name     = Meta Llama 3.1 8B Instruct
Nov 19 13:20:58 Tatami ollama[84614]: print_info: vocab type       = BPE
Nov 19 13:20:58 Tatami ollama[84614]: print_info: n_vocab          = 128256
Nov 19 13:20:58 Tatami ollama[84614]: print_info: n_merges         = 280147
Nov 19 13:20:58 Tatami ollama[84614]: print_info: BOS token        = 128000 '<|begin_of_text|>'
Nov 19 13:20:58 Tatami ollama[84614]: print_info: EOS token        = 128009 '<|eot_id|>'
Nov 19 13:20:58 Tatami ollama[84614]: print_info: EOT token        = 128009 '<|eot_id|>'
Nov 19 13:20:58 Tatami ollama[84614]: print_info: EOM token        = 128008 '<|eom_id|>'
Nov 19 13:20:58 Tatami ollama[84614]: print_info: LF token         = 198 'Ċ'
Nov 19 13:20:58 Tatami ollama[84614]: print_info: EOG token        = 128001 '<|end_of_text|>'
Nov 19 13:20:58 Tatami ollama[84614]: print_info: EOG token        = 128008 '<|eom_id|>'
Nov 19 13:20:58 Tatami ollama[84614]: print_info: EOG token        = 128009 '<|eot_id|>'
Nov 19 13:20:58 Tatami ollama[84614]: print_info: max token length = 256
Nov 19 13:20:58 Tatami ollama[84614]: llama_model_load: vocab only - skipping tensors
Nov 19 13:20:58 Tatami ollama[84614]: time=2025-11-19T13:20:58.394-08:00 level=INFO source=server.go:392 msg=""starting runner"" cmd=""/usr/local/bin/ollama runner --model /mnt/shared/ollama-models/models/blobs/sha256-667b0c1932bc6ffc593ed1d03f895bf2dc8dc6df21db3042284a6f4416b06a29 --port 41617""
Nov 19 13:20:58 Tatami ollama[84614]: time=2025-11-19T13:20:58.394-08:00 level=INFO source=sched.go:443 msg=""system memory"" total=""122.6 GiB"" free=""109.1 GiB"" free_swap=""8.0 GiB""
Nov 19 13:20:58 Tatami ollama[84614]: time=2025-11-19T13:20:58.394-08:00 level=INFO source=server.go:459 msg=""loading model"" ""model layers""=33 requested=-1
Nov 19 13:20:58 Tatami ollama[84614]: time=2025-11-19T13:20:58.395-08:00 level=INFO source=device.go:245 msg=""model weights"" device=CPU size=""4.3 GiB""
Nov 19 13:20:58 Tatami ollama[84614]: time=2025-11-19T13:20:58.395-08:00 level=INFO source=device.go:256 msg=""kv cache"" device=CPU size=""512.0 MiB""
Nov 19 13:20:58 Tatami ollama[84614]: time=2025-11-19T13:20:58.395-08:00 level=INFO source=device.go:272 msg=""total memory"" size=""4.8 GiB""
Nov 19 13:20:58 Tatami ollama[84614]: time=2025-11-19T13:20:58.403-08:00 level=INFO source=runner.go:963 msg=""starting go runner""
Nov 19 13:20:58 Tatami ollama[84614]: time=2025-11-19T13:20:58.403-08:00 level=INFO source=ggml.go:104 msg=system CPU.0.LLAMAFILE=1 compiler=cgo(gcc)
Nov 19 13:20:58 Tatami ollama[84614]: time=2025-11-19T13:20:58.404-08:00 level=INFO source=runner.go:999 msg=""Server listening on 127.0.0.1:41617""
Nov 19 13:20:58 Tatami ollama[84614]: time=2025-11-19T13:20:58.406-08:00 level=INFO source=runner.go:893 msg=load request=""{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:false KvSize:4096 KvCacheType: NumThreads:8 GPULayers:[] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}""
Nov 19 13:20:58 Tatami ollama[84614]: time=2025-11-19T13:20:58.406-08:00 level=INFO source=server.go:1294 msg=""waiting for llama runner to start responding""
Nov 19 13:20:58 Tatami ollama[84614]: time=2025-11-19T13:20:58.406-08:00 level=INFO source=server.go:1328 msg=""waiting for server to become available"" status=""llm server loading model""
Nov 19 13:20:58 Tatami ollama[84614]: llama_model_loader: loaded meta data with 29 key-value pairs and 292 tensors from /mnt/shared/ollama-models/models/blobs/sha256-667b0c1932bc6ffc593ed1d03f895bf2dc8dc6df21db3042284a6f4416b06a29 (version GGUF V3 (latest))
Nov 19 13:20:58 Tatami ollama[84614]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Nov 19 13:20:58 Tatami ollama[84614]: llama_model_loader: - kv   0:                       general.architecture str              = llama
Nov 19 13:20:58 Tatami ollama[84614]: llama_model_loader: - kv   1:                               general.type str              = model
Nov 19 13:20:58 Tatami ollama[84614]: llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 8B Instruct
Nov 19 13:20:58 Tatami ollama[84614]: llama_model_loader: - kv   3:                           general.finetune str              = Instruct
Nov 19 13:20:58 Tatami ollama[84614]: llama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1
Nov 19 13:20:58 Tatami ollama[84614]: llama_model_loader: - kv   5:                         general.size_label str              = 8B
Nov 19 13:20:58 Tatami ollama[84614]: llama_model_loader: - kv   6:                            general.license str              = llama3.1
Nov 19 13:20:58 Tatami ollama[84614]: llama_model_loader: - kv   7:                               general.tags arr[str,6]       = [""facebook"", ""meta"", ""pytorch"", ""llam...
Nov 19 13:20:58 Tatami ollama[84614]: llama_model_loader: - kv   8:                          general.languages arr[str,8]       = [""en"", ""de"", ""fr"", ""it"", ""pt"", ""hi"", ...
Nov 19 13:20:58 Tatami ollama[84614]: llama_model_loader: - kv   9:                          llama.block_count u32              = 32
Nov 19 13:20:58 Tatami ollama[84614]: llama_model_loader: - kv  10:                       llama.context_length u32              = 131072
Nov 19 13:20:58 Tatami ollama[84614]: llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096
Nov 19 13:20:58 Tatami ollama[84614]: llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 14336
Nov 19 13:20:58 Tatami ollama[84614]: llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 32
Nov 19 13:20:58 Tatami ollama[84614]: llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8
Nov 19 13:20:58 Tatami ollama[84614]: llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000
Nov 19 13:20:58 Tatami ollama[84614]: llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
Nov 19 13:20:58 Tatami ollama[84614]: llama_model_loader: - kv  17:                          general.file_type u32              = 15
Nov 19 13:20:58 Tatami ollama[84614]: llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256
Nov 19 13:20:58 Tatami ollama[84614]: llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128
Nov 19 13:20:58 Tatami ollama[84614]: llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2
Nov 19 13:20:58 Tatami ollama[84614]: llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe
Nov 19 13:20:58 Tatami ollama[84614]: llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = [""!"", ""\"""", ""#"", ""$"", ""%"", ""&"", ""'"", ...
Nov 19 13:20:58 Tatami ollama[84614]: llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Nov 19 13:20:58 Tatami ollama[84614]: llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = [""Ġ Ġ"", ""Ġ ĠĠĠ"", ""ĠĠ ĠĠ"", ""...
Nov 19 13:20:58 Tatami ollama[84614]: llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000
Nov 19 13:20:58 Tatami ollama[84614]: llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009
Nov 19 13:20:58 Tatami ollama[84614]: llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
Nov 19 13:20:58 Tatami ollama[84614]: llama_model_loader: - kv  28:               general.quantization_version u32              = 2
Nov 19 13:20:58 Tatami ollama[84614]: llama_model_loader: - type  f32:   66 tensors
Nov 19 13:20:58 Tatami ollama[84614]: llama_model_loader: - type q4_K:  193 tensors
Nov 19 13:20:58 Tatami ollama[84614]: llama_model_loader: - type q6_K:   33 tensors
Nov 19 13:20:58 Tatami ollama[84614]: print_info: file format = GGUF V3 (latest)
Nov 19 13:20:58 Tatami ollama[84614]: print_info: file type   = Q4_K - Medium
Nov 19 13:20:58 Tatami ollama[84614]: print_info: file size   = 4.58 GiB (4.89 BPW)
Nov 19 13:20:58 Tatami ollama[84614]: load: printing all EOG tokens:
Nov 19 13:20:58 Tatami ollama[84614]: load:   - 128001 ('<|end_of_text|>')
Nov 19 13:20:58 Tatami ollama[84614]: load:   - 128008 ('<|eom_id|>')
Nov 19 13:20:58 Tatami ollama[84614]: load:   - 128009 ('<|eot_id|>')
Nov 19 13:20:58 Tatami ollama[84614]: load: special tokens cache size = 256
Nov 19 13:20:58 Tatami ollama[84614]: load: token to piece cache size = 0.7999 MB
Nov 19 13:20:58 Tatami ollama[84614]: print_info: arch             = llama
Nov 19 13:20:58 Tatami ollama[84614]: print_info: vocab_only       = 0
Nov 19 13:20:58 Tatami ollama[84614]: print_info: n_ctx_train      = 131072
Nov 19 13:20:58 Tatami ollama[84614]: print_info: n_embd           = 4096
Nov 19 13:20:58 Tatami ollama[84614]: print_info: n_layer          = 32
Nov 19 13:20:58 Tatami ollama[84614]: print_info: n_head           = 32
Nov 19 13:20:58 Tatami ollama[84614]: print_info: n_head_kv        = 8
Nov 19 13:20:58 Tatami ollama[84614]: print_info: n_rot            = 128
Nov 19 13:20:58 Tatami ollama[84614]: print_info: n_swa            = 0
Nov 19 13:20:58 Tatami ollama[84614]: print_info: is_swa_any       = 0
Nov 19 13:20:58 Tatami ollama[84614]: print_info: n_embd_head_k    = 128
Nov 19 13:20:58 Tatami ollama[84614]: print_info: n_embd_head_v    = 128
Nov 19 13:20:58 Tatami ollama[84614]: print_info: n_gqa            = 4
Nov 19 13:20:58 Tatami ollama[84614]: print_info: n_embd_k_gqa     = 1024
Nov 19 13:20:58 Tatami ollama[84614]: print_info: n_embd_v_gqa     = 1024
Nov 19 13:20:58 Tatami ollama[84614]: print_info: f_norm_eps       = 0.0e+00
Nov 19 13:20:58 Tatami ollama[84614]: print_info: f_norm_rms_eps   = 1.0e-05
Nov 19 13:20:58 Tatami ollama[84614]: print_info: f_clamp_kqv      = 0.0e+00
Nov 19 13:20:58 Tatami ollama[84614]: print_info: f_max_alibi_bias = 0.0e+00
Nov 19 13:20:58 Tatami ollama[84614]: print_info: f_logit_scale    = 0.0e+00
Nov 19 13:20:58 Tatami ollama[84614]: print_info: f_attn_scale     = 0.0e+00
Nov 19 13:20:58 Tatami ollama[84614]: print_info: n_ff             = 14336
Nov 19 13:20:58 Tatami ollama[84614]: print_info: n_expert         = 0
Nov 19 13:20:58 Tatami ollama[84614]: print_info: n_expert_used    = 0
Nov 19 13:20:58 Tatami ollama[84614]: print_info: causal attn      = 1
Nov 19 13:20:58 Tatami ollama[84614]: print_info: pooling type     = 0
Nov 19 13:20:58 Tatami ollama[84614]: print_info: rope type        = 0
Nov 19 13:20:58 Tatami ollama[84614]: print_info: rope scaling     = linear
Nov 19 13:20:58 Tatami ollama[84614]: print_info: freq_base_train  = 500000.0
Nov 19 13:20:58 Tatami ollama[84614]: print_info: freq_scale_train = 1
Nov 19 13:20:58 Tatami ollama[84614]: print_info: n_ctx_orig_yarn  = 131072
Nov 19 13:20:58 Tatami ollama[84614]: print_info: rope_finetuned   = unknown
Nov 19 13:20:58 Tatami ollama[84614]: print_info: model type       = 8B
Nov 19 13:20:58 Tatami ollama[84614]: print_info: model params     = 8.03 B
Nov 19 13:20:58 Tatami ollama[84614]: print_info: general.name     = Meta Llama 3.1 8B Instruct
Nov 19 13:20:58 Tatami ollama[84614]: print_info: vocab type       = BPE
Nov 19 13:20:58 Tatami ollama[84614]: print_info: n_vocab          = 128256
Nov 19 13:20:58 Tatami ollama[84614]: print_info: n_merges         = 280147
Nov 19 13:20:58 Tatami ollama[84614]: print_info: BOS token        = 128000 '<|begin_of_text|>'
Nov 19 13:20:58 Tatami ollama[84614]: print_info: EOS token        = 128009 '<|eot_id|>'
Nov 19 13:20:58 Tatami ollama[84614]: print_info: EOT token        = 128009 '<|eot_id|>'
Nov 19 13:20:58 Tatami ollama[84614]: print_info: EOM token        = 128008 '<|eom_id|>'
Nov 19 13:20:58 Tatami ollama[84614]: print_info: LF token         = 198 'Ċ'
Nov 19 13:20:58 Tatami ollama[84614]: print_info: EOG token        = 128001 '<|end_of_text|>'
Nov 19 13:20:58 Tatami ollama[84614]: print_info: EOG token        = 128008 '<|eom_id|>'
Nov 19 13:20:58 Tatami ollama[84614]: print_info: EOG token        = 128009 '<|eot_id|>'
Nov 19 13:20:58 Tatami ollama[84614]: print_info: max token length = 256
Nov 19 13:20:58 Tatami ollama[84614]: load_tensors: loading model tensors, this can take a while... (mmap = false)
Nov 19 13:20:58 Tatami ollama[84614]: load_tensors:          CPU model buffer size =  4685.30 MiB
Nov 19 13:21:00 Tatami ollama[84614]: llama_context: constructing llama_context
Nov 19 13:21:00 Tatami ollama[84614]: llama_context: n_seq_max     = 1
Nov 19 13:21:00 Tatami ollama[84614]: llama_context: n_ctx         = 4096
Nov 19 13:21:00 Tatami ollama[84614]: llama_context: n_ctx_per_seq = 4096
Nov 19 13:21:00 Tatami ollama[84614]: llama_context: n_batch       = 512
Nov 19 13:21:00 Tatami ollama[84614]: llama_context: n_ubatch      = 512
Nov 19 13:21:00 Tatami ollama[84614]: llama_context: causal_attn   = 1
Nov 19 13:21:00 Tatami ollama[84614]: llama_context: flash_attn    = disabled
Nov 19 13:21:00 Tatami ollama[84614]: llama_context: kv_unified    = false
Nov 19 13:21:00 Tatami ollama[84614]: llama_context: freq_base     = 500000.0
Nov 19 13:21:00 Tatami ollama[84614]: llama_context: freq_scale    = 1
Nov 19 13:21:00 Tatami ollama[84614]: llama_context: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
Nov 19 13:21:00 Tatami ollama[84614]: llama_context:        CPU  output buffer size =     0.50 MiB
Nov 19 13:21:00 Tatami ollama[84614]: llama_kv_cache:        CPU KV buffer size =   512.00 MiB
Nov 19 13:21:00 Tatami ollama[84614]: llama_kv_cache: size =  512.00 MiB (  4096 cells,  32 layers,  1/1 seqs), K (f16):  256.00 MiB, V (f16):  256.00 MiB
Nov 19 13:21:00 Tatami ollama[84614]: llama_context:        CPU compute buffer size =   300.01 MiB
Nov 19 13:21:00 Tatami ollama[84614]: llama_context: graph nodes  = 1158
Nov 19 13:21:00 Tatami ollama[84614]: llama_context: graph splits = 1
Nov 19 13:21:00 Tatami ollama[84614]: time=2025-11-19T13:21:00.412-08:00 level=INFO source=server.go:1332 msg=""llama runner started in 2.02 seconds""
Nov 19 13:21:00 Tatami ollama[84614]: time=2025-11-19T13:21:00.412-08:00 level=INFO source=sched.go:517 msg=""loaded runners"" count=1
Nov 19 13:21:00 Tatami ollama[84614]: time=2025-11-19T13:21:00.412-08:00 level=INFO source=server.go:1294 msg=""waiting for llama runner to start responding""
Nov 19 13:21:00 Tatami ollama[84614]: time=2025-11-19T13:21:00.412-08:00 level=INFO source=server.go:1332 msg=""llama runner started in 2.02 seconds""
Nov 19 13:21:21 Tatami ollama[84614]: [GIN] 2025/11/19 - 13:21:21 | 200 | 23.341049668s |             ::1 | POST     ""/api/generate""

```

### OS

Linux

### GPU

Nvidia

### CPU

AMD

### Ollama version

0.12.11",,"{'url': 'https://api.github.com/repos/ollama/ollama/issues/13163/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}",https://api.github.com/repos/ollama/ollama/issues/13163/timeline,,,,
https://api.github.com/repos/ollama/ollama/issues/13162,https://api.github.com/repos/ollama/ollama,https://api.github.com/repos/ollama/ollama/issues/13162/labels{/name},https://api.github.com/repos/ollama/ollama/issues/13162/comments,https://api.github.com/repos/ollama/ollama/issues/13162/events,https://github.com/ollama/ollama/pull/13162,3644252013,PR_kwDOJ0Z1Ps60b2tO,13162,nomic-embed-text:v2: model implementation ,"{'login': 'npardal', 'id': 109545900, 'node_id': 'U_kgDOBoeJrA', 'avatar_url': 'https://avatars.githubusercontent.com/u/109545900?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/npardal', 'html_url': 'https://github.com/npardal', 'followers_url': 'https://api.github.com/users/npardal/followers', 'following_url': 'https://api.github.com/users/npardal/following{/other_user}', 'gists_url': 'https://api.github.com/users/npardal/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/npardal/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/npardal/subscriptions', 'organizations_url': 'https://api.github.com/users/npardal/orgs', 'repos_url': 'https://api.github.com/users/npardal/repos', 'events_url': 'https://api.github.com/users/npardal/events{/privacy}', 'received_events_url': 'https://api.github.com/users/npardal/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}",[],open,False,,[],,0,2025-11-19T20:06:48Z,2025-11-20T00:54:45Z,,CONTRIBUTOR,,,,,"[NOT RDY] 
- initial model implementation file for nomic-embed-text:v2 which has MOE ",,"{'url': 'https://api.github.com/repos/ollama/ollama/issues/13162/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}",https://api.github.com/repos/ollama/ollama/issues/13162/timeline,,,True,"{'url': 'https://api.github.com/repos/ollama/ollama/pulls/13162', 'html_url': 'https://github.com/ollama/ollama/pull/13162', 'diff_url': 'https://github.com/ollama/ollama/pull/13162.diff', 'patch_url': 'https://github.com/ollama/ollama/pull/13162.patch', 'merged_at': None}"
https://api.github.com/repos/ollama/ollama/issues/13161,https://api.github.com/repos/ollama/ollama,https://api.github.com/repos/ollama/ollama/issues/13161/labels{/name},https://api.github.com/repos/ollama/ollama/issues/13161/comments,https://api.github.com/repos/ollama/ollama/issues/13161/events,https://github.com/ollama/ollama/pull/13161,3644236318,PR_kwDOJ0Z1Ps60bzWS,13161,app/ui: add gemini-3-pro-preview to featured list,"{'login': 'hoyyeva', 'id': 63033505, 'node_id': 'MDQ6VXNlcjYzMDMzNTA1', 'avatar_url': 'https://avatars.githubusercontent.com/u/63033505?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/hoyyeva', 'html_url': 'https://github.com/hoyyeva', 'followers_url': 'https://api.github.com/users/hoyyeva/followers', 'following_url': 'https://api.github.com/users/hoyyeva/following{/other_user}', 'gists_url': 'https://api.github.com/users/hoyyeva/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/hoyyeva/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/hoyyeva/subscriptions', 'organizations_url': 'https://api.github.com/users/hoyyeva/orgs', 'repos_url': 'https://api.github.com/users/hoyyeva/repos', 'events_url': 'https://api.github.com/users/hoyyeva/events{/privacy}', 'received_events_url': 'https://api.github.com/users/hoyyeva/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}",[],open,False,,[],,0,2025-11-19T20:01:27Z,2025-11-19T20:57:23Z,,CONTRIBUTOR,,,,,,,"{'url': 'https://api.github.com/repos/ollama/ollama/issues/13161/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}",https://api.github.com/repos/ollama/ollama/issues/13161/timeline,,,False,"{'url': 'https://api.github.com/repos/ollama/ollama/pulls/13161', 'html_url': 'https://github.com/ollama/ollama/pull/13161', 'diff_url': 'https://github.com/ollama/ollama/pull/13161.diff', 'patch_url': 'https://github.com/ollama/ollama/pull/13161.patch', 'merged_at': None}"
https://api.github.com/repos/ollama/ollama/issues/13160,https://api.github.com/repos/ollama/ollama,https://api.github.com/repos/ollama/ollama/issues/13160/labels{/name},https://api.github.com/repos/ollama/ollama/issues/13160/comments,https://api.github.com/repos/ollama/ollama/issues/13160/events,https://github.com/ollama/ollama/pull/13160,3644235726,PR_kwDOJ0Z1Ps60bzOY,13160,kvcache: Run tests both with and without PermutedV,"{'login': 'jessegross', 'id': 6468499, 'node_id': 'MDQ6VXNlcjY0Njg0OTk=', 'avatar_url': 'https://avatars.githubusercontent.com/u/6468499?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/jessegross', 'html_url': 'https://github.com/jessegross', 'followers_url': 'https://api.github.com/users/jessegross/followers', 'following_url': 'https://api.github.com/users/jessegross/following{/other_user}', 'gists_url': 'https://api.github.com/users/jessegross/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/jessegross/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/jessegross/subscriptions', 'organizations_url': 'https://api.github.com/users/jessegross/orgs', 'repos_url': 'https://api.github.com/users/jessegross/repos', 'events_url': 'https://api.github.com/users/jessegross/events{/privacy}', 'received_events_url': 'https://api.github.com/users/jessegross/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}",[],closed,False,,[],,0,2025-11-19T20:01:13Z,2025-11-20T00:45:32Z,2025-11-20T00:45:30Z,CONTRIBUTOR,,,,,The causal cache can store data differently depending on what is best for the backend. We should run tests both ways.,"{'login': 'jessegross', 'id': 6468499, 'node_id': 'MDQ6VXNlcjY0Njg0OTk=', 'avatar_url': 'https://avatars.githubusercontent.com/u/6468499?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/jessegross', 'html_url': 'https://github.com/jessegross', 'followers_url': 'https://api.github.com/users/jessegross/followers', 'following_url': 'https://api.github.com/users/jessegross/following{/other_user}', 'gists_url': 'https://api.github.com/users/jessegross/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/jessegross/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/jessegross/subscriptions', 'organizations_url': 'https://api.github.com/users/jessegross/orgs', 'repos_url': 'https://api.github.com/users/jessegross/repos', 'events_url': 'https://api.github.com/users/jessegross/events{/privacy}', 'received_events_url': 'https://api.github.com/users/jessegross/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}","{'url': 'https://api.github.com/repos/ollama/ollama/issues/13160/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}",https://api.github.com/repos/ollama/ollama/issues/13160/timeline,,,False,"{'url': 'https://api.github.com/repos/ollama/ollama/pulls/13160', 'html_url': 'https://github.com/ollama/ollama/pull/13160', 'diff_url': 'https://github.com/ollama/ollama/pull/13160.diff', 'patch_url': 'https://github.com/ollama/ollama/pull/13160.patch', 'merged_at': '2025-11-20T00:45:30Z'}"
https://api.github.com/repos/ollama/ollama/issues/13159,https://api.github.com/repos/ollama/ollama,https://api.github.com/repos/ollama/ollama/issues/13159/labels{/name},https://api.github.com/repos/ollama/ollama/issues/13159/comments,https://api.github.com/repos/ollama/ollama/issues/13159/events,https://github.com/ollama/ollama/pull/13159,3644044422,PR_kwDOJ0Z1Ps60bKNt,13159,app/ui: handle unspecified bind addresses and wait for server in ollama proxy,"{'login': 'hoyyeva', 'id': 63033505, 'node_id': 'MDQ6VXNlcjYzMDMzNTA1', 'avatar_url': 'https://avatars.githubusercontent.com/u/63033505?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/hoyyeva', 'html_url': 'https://github.com/hoyyeva', 'followers_url': 'https://api.github.com/users/hoyyeva/followers', 'following_url': 'https://api.github.com/users/hoyyeva/following{/other_user}', 'gists_url': 'https://api.github.com/users/hoyyeva/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/hoyyeva/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/hoyyeva/subscriptions', 'organizations_url': 'https://api.github.com/users/hoyyeva/orgs', 'repos_url': 'https://api.github.com/users/hoyyeva/repos', 'events_url': 'https://api.github.com/users/hoyyeva/events{/privacy}', 'received_events_url': 'https://api.github.com/users/hoyyeva/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}",[],open,False,,[],,0,2025-11-19T18:58:35Z,2025-11-20T21:41:30Z,,CONTRIBUTOR,,,,,"This PR fixes
- #13105 
- #12851 

**Issue 1: Unspecified bind addresses (#13105)**
When users set OLLAMA_HOST=0.0.0.0 to bind the Ollama server on all interfaces, the UI proxy was trying to connect to 0.0.0.0, which is not a valid connection target. While 0.0.0.0 is valid for server binding, client connections must use a specific address like 127.0.0.1.
**Issue 2: Race condition on startup (#12851)**
The proxy was initialized immediately without waiting for the Ollama server to be ready, causing ""connection refused"" errors when the UI made requests before the server finished starting.

This PR
1. Use envconfig.Host() for host resolution
Replace manual environment variable handling with envconfig.Host()
2. Wait for server on first request
Use sync.Once pattern to:
- Wait for Ollama server to be ready on first proxy request only
- Allow webview to open immediately (no blocking at startup)",,"{'url': 'https://api.github.com/repos/ollama/ollama/issues/13159/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}",https://api.github.com/repos/ollama/ollama/issues/13159/timeline,,,False,"{'url': 'https://api.github.com/repos/ollama/ollama/pulls/13159', 'html_url': 'https://github.com/ollama/ollama/pull/13159', 'diff_url': 'https://github.com/ollama/ollama/pull/13159.diff', 'patch_url': 'https://github.com/ollama/ollama/pull/13159.patch', 'merged_at': None}"
https://api.github.com/repos/ollama/ollama/issues/13158,https://api.github.com/repos/ollama/ollama,https://api.github.com/repos/ollama/ollama/issues/13158/labels{/name},https://api.github.com/repos/ollama/ollama/issues/13158/comments,https://api.github.com/repos/ollama/ollama/issues/13158/events,https://github.com/ollama/ollama/pull/13158,3643517607,PR_kwDOJ0Z1Ps60ZX92,13158,app/ui: handle unspecified bind addresses in ollama proxy,"{'login': 'hoyyeva', 'id': 63033505, 'node_id': 'MDQ6VXNlcjYzMDMzNTA1', 'avatar_url': 'https://avatars.githubusercontent.com/u/63033505?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/hoyyeva', 'html_url': 'https://github.com/hoyyeva', 'followers_url': 'https://api.github.com/users/hoyyeva/followers', 'following_url': 'https://api.github.com/users/hoyyeva/following{/other_user}', 'gists_url': 'https://api.github.com/users/hoyyeva/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/hoyyeva/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/hoyyeva/subscriptions', 'organizations_url': 'https://api.github.com/users/hoyyeva/orgs', 'repos_url': 'https://api.github.com/users/hoyyeva/repos', 'events_url': 'https://api.github.com/users/hoyyeva/events{/privacy}', 'received_events_url': 'https://api.github.com/users/hoyyeva/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}",[],closed,False,,[],,0,2025-11-19T16:28:57Z,2025-11-19T17:36:44Z,2025-11-19T17:36:43Z,CONTRIBUTOR,,,,,"This PR replace manual environment variable handling with `envconfig.Host()` which:
- handles unspecified address normalization 
- returns a properly parsed `*url.URL`
- handle defaults and scheme prefixing autmatically

the problem was that when users set `OLLAMA_HOST=0.0.0.0` or unspecified value, the app proxy was trying to connect to a invalid connection target. 

This PR fixes #13105
","{'login': 'hoyyeva', 'id': 63033505, 'node_id': 'MDQ6VXNlcjYzMDMzNTA1', 'avatar_url': 'https://avatars.githubusercontent.com/u/63033505?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/hoyyeva', 'html_url': 'https://github.com/hoyyeva', 'followers_url': 'https://api.github.com/users/hoyyeva/followers', 'following_url': 'https://api.github.com/users/hoyyeva/following{/other_user}', 'gists_url': 'https://api.github.com/users/hoyyeva/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/hoyyeva/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/hoyyeva/subscriptions', 'organizations_url': 'https://api.github.com/users/hoyyeva/orgs', 'repos_url': 'https://api.github.com/users/hoyyeva/repos', 'events_url': 'https://api.github.com/users/hoyyeva/events{/privacy}', 'received_events_url': 'https://api.github.com/users/hoyyeva/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}","{'url': 'https://api.github.com/repos/ollama/ollama/issues/13158/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}",https://api.github.com/repos/ollama/ollama/issues/13158/timeline,,,False,"{'url': 'https://api.github.com/repos/ollama/ollama/pulls/13158', 'html_url': 'https://github.com/ollama/ollama/pull/13158', 'diff_url': 'https://github.com/ollama/ollama/pull/13158.diff', 'patch_url': 'https://github.com/ollama/ollama/pull/13158.patch', 'merged_at': None}"
https://api.github.com/repos/ollama/ollama/issues/13157,https://api.github.com/repos/ollama/ollama,https://api.github.com/repos/ollama/ollama/issues/13157/labels{/name},https://api.github.com/repos/ollama/ollama/issues/13157/comments,https://api.github.com/repos/ollama/ollama/issues/13157/events,https://github.com/ollama/ollama/issues/13157,3642983186,I_kwDOJ0Z1Ps7ZI4MS,13157,which quantization ollama cloud models use,"{'login': 'MertcanTekin', 'id': 138381309, 'node_id': 'U_kgDOCD-H_Q', 'avatar_url': 'https://avatars.githubusercontent.com/u/138381309?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/MertcanTekin', 'html_url': 'https://github.com/MertcanTekin', 'followers_url': 'https://api.github.com/users/MertcanTekin/followers', 'following_url': 'https://api.github.com/users/MertcanTekin/following{/other_user}', 'gists_url': 'https://api.github.com/users/MertcanTekin/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/MertcanTekin/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/MertcanTekin/subscriptions', 'organizations_url': 'https://api.github.com/users/MertcanTekin/orgs', 'repos_url': 'https://api.github.com/users/MertcanTekin/repos', 'events_url': 'https://api.github.com/users/MertcanTekin/events{/privacy}', 'received_events_url': 'https://api.github.com/users/MertcanTekin/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}","[{'id': 5667396200, 'node_id': 'LA_kwDOJ0Z1Ps8AAAABUc2aaA', 'url': 'https://api.github.com/repos/ollama/ollama/labels/feature%20request', 'name': 'feature request', 'color': 'a2eeef', 'default': False, 'description': 'New feature or request'}]",open,False,,[],,1,2025-11-19T14:25:57Z,2025-11-19T15:42:24Z,,NONE,,,"{'total': 0, 'completed': 0, 'percent_completed': 0}","{'blocked_by': 0, 'total_blocked_by': 0, 'blocking': 0, 'total_blocking': 0}","I'm wondering if I'll get the same performance running Ollama models locally versus on their cloud service. For example, are ollama run gpt-oss:120b-cloud and ollama run gpt-oss:120b the same quantized models, or does the cloud version use less-quantized (higher quality) models?",,"{'url': 'https://api.github.com/repos/ollama/ollama/issues/13157/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}",https://api.github.com/repos/ollama/ollama/issues/13157/timeline,,,,
https://api.github.com/repos/ollama/ollama/issues/13156,https://api.github.com/repos/ollama/ollama,https://api.github.com/repos/ollama/ollama/issues/13156/labels{/name},https://api.github.com/repos/ollama/ollama/issues/13156/comments,https://api.github.com/repos/ollama/ollama/issues/13156/events,https://github.com/ollama/ollama/issues/13156,3642429479,I_kwDOJ0Z1Ps7ZGxAn,13156,Vulkan Backhand on R5 3500u,"{'login': 'leokernel', 'id': 244955552, 'node_id': 'U_kgDODpm5oA', 'avatar_url': 'https://avatars.githubusercontent.com/u/244955552?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/leokernel', 'html_url': 'https://github.com/leokernel', 'followers_url': 'https://api.github.com/users/leokernel/followers', 'following_url': 'https://api.github.com/users/leokernel/following{/other_user}', 'gists_url': 'https://api.github.com/users/leokernel/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/leokernel/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/leokernel/subscriptions', 'organizations_url': 'https://api.github.com/users/leokernel/orgs', 'repos_url': 'https://api.github.com/users/leokernel/repos', 'events_url': 'https://api.github.com/users/leokernel/events{/privacy}', 'received_events_url': 'https://api.github.com/users/leokernel/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}","[{'id': 5667396184, 'node_id': 'LA_kwDOJ0Z1Ps8AAAABUc2aWA', 'url': 'https://api.github.com/repos/ollama/ollama/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': ""Something isn't working""}]",closed,False,,[],,9,2025-11-19T12:05:50Z,2025-11-19T22:48:54Z,2025-11-19T22:48:54Z,NONE,,,"{'total': 0, 'completed': 0, 'percent_completed': 0}","{'blocked_by': 0, 'total_blocked_by': 0, 'blocking': 0, 'total_blocking': 0}","### What is the issue?

Im' trying to run ollama with vulkan backhand on a machine running proxmox ve.
Ollama runs in an lxc priviledged container with the dev/dri/ devices passed trough, vulkaninfo seems to work but ollama always enters low vram mode... i guess the problem is that ollama cant read the vram capacity.
i tried with a dummy x session but it didnt work.

### Relevant log output

```shell
FROM VULKANINFO 
Devices:

========

GPU0:

        apiVersion         = 1.4.305

        driverVersion      = 25.0.7

        vendorID           = 0x1002

        deviceID           = 0x15d8

        deviceType         = PHYSICAL_DEVICE_TYPE_INTEGRATED_GPU

        deviceName         = AMD Radeon Vega 8 Graphics (RADV RAVEN)

        driverID           = DRIVER_ID_MESA_RADV

        driverName         = radv

        driverInfo         = Mesa 25.0.7-0ubuntu0.24.04.2

        conformanceVersion = 1.4.0.0

        deviceUUID         = 00000000-0500-0000-0000-000000000000

        driverUUID         = 414d442d-4d45-5341-2d44-525600000000

GPU1:

        apiVersion         = 1.4.305

        driverVersion      = 0.0.1

        vendorID           = 0x10005

        deviceID           = 0x0000

        deviceType         = PHYSICAL_DEVICE_TYPE_CPU

        deviceName         = llvmpipe (LLVM 20.1.2, 256 bits)

        driverID           = DRIVER_ID_MESA_LLVMPIPE

        driverName         = llvmpipe

        driverInfo         = Mesa 25.0.7-0ubuntu0.24.04.2 (LLVM 20.1.2)

        conformanceVersion = 1.3.1.1

        deviceUUID         = 6d657361-3235-2e30-2e37-2d3075627500

        driverUUID         = 6c6c766d-7069-7065-5555-494400000000

FROM OLLAMA

root@ollama:~# OLLAMA_VULKAN=1 ollama serve

Couldn't find '/root/.ollama/id_ed25519'. Generating new private key.

Your new public key is: 



ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAINj3TMe5hPHV4OAY+uv+f4tQwQhSTIz6qaH7zTEKdxVK



time=2025-11-18T16:00:10.952+01:00 level=INFO source=routes.go:1544 msg=""server config"" env=""map[CUDA_VISIBLE_DEVICES: GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/root/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false OLLAMA_VULKAN:true ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]""

time=2025-11-18T16:00:10.952+01:00 level=INFO source=images.go:522 msg=""total blobs: 0""

time=2025-11-18T16:00:10.952+01:00 level=INFO source=images.go:529 msg=""total unused blobs removed: 0""

time=2025-11-18T16:00:10.952+01:00 level=INFO source=routes.go:1597 msg=""Listening on 127.0.0.1:11434 (version 0.12.11)""

time=2025-11-18T16:00:10.953+01:00 level=INFO source=runner.go:67 msg=""discovering available GPUs...""

time=2025-11-18T16:00:10.954+01:00 level=INFO source=server.go:392 msg=""starting runner"" cmd=""/usr/local/bin/ollama runner --ollama-engine --port 36187""

time=2025-11-18T16:00:10.992+01:00 level=INFO source=server.go:392 msg=""starting runner"" cmd=""/usr/local/bin/ollama runner --ollama-engine --port 36067""

time=2025-11-18T16:00:11.024+01:00 level=INFO source=server.go:392 msg=""starting runner"" cmd=""/usr/local/bin/ollama runner --ollama-engine --port 40965""

time=2025-11-18T16:00:11.122+01:00 level=INFO source=types.go:60 msg=""inference compute"" id=cpu library=cpu compute="""" name=cpu description=cpu libdirs=ollama driver="""" pci_id="""" type="""" total=""3.9 GiB"" available=""3.8 GiB""

time=2025-11-18T16:00:11.122+01:00 level=INFO source=routes.go:1638 msg=""entering low vram mode"" ""total vram""=""0 B"" threshold=""20.0 GiB""
```

### OS

Linux

### GPU

AMD

### CPU

AMD

### Ollama version

0.12.11","{'login': 'leokernel', 'id': 244955552, 'node_id': 'U_kgDODpm5oA', 'avatar_url': 'https://avatars.githubusercontent.com/u/244955552?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/leokernel', 'html_url': 'https://github.com/leokernel', 'followers_url': 'https://api.github.com/users/leokernel/followers', 'following_url': 'https://api.github.com/users/leokernel/following{/other_user}', 'gists_url': 'https://api.github.com/users/leokernel/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/leokernel/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/leokernel/subscriptions', 'organizations_url': 'https://api.github.com/users/leokernel/orgs', 'repos_url': 'https://api.github.com/users/leokernel/repos', 'events_url': 'https://api.github.com/users/leokernel/events{/privacy}', 'received_events_url': 'https://api.github.com/users/leokernel/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}","{'url': 'https://api.github.com/repos/ollama/ollama/issues/13156/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}",https://api.github.com/repos/ollama/ollama/issues/13156/timeline,,completed,,
https://api.github.com/repos/ollama/ollama/issues/13155,https://api.github.com/repos/ollama/ollama,https://api.github.com/repos/ollama/ollama/issues/13155/labels{/name},https://api.github.com/repos/ollama/ollama/issues/13155/comments,https://api.github.com/repos/ollama/ollama/issues/13155/events,https://github.com/ollama/ollama/issues/13155,3642110453,I_kwDOJ0Z1Ps7ZFjH1,13155,不支持添加api key,"{'login': 'fly23423', 'id': 127937333, 'node_id': 'U_kgDOB6ArNQ', 'avatar_url': 'https://avatars.githubusercontent.com/u/127937333?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/fly23423', 'html_url': 'https://github.com/fly23423', 'followers_url': 'https://api.github.com/users/fly23423/followers', 'following_url': 'https://api.github.com/users/fly23423/following{/other_user}', 'gists_url': 'https://api.github.com/users/fly23423/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/fly23423/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/fly23423/subscriptions', 'organizations_url': 'https://api.github.com/users/fly23423/orgs', 'repos_url': 'https://api.github.com/users/fly23423/repos', 'events_url': 'https://api.github.com/users/fly23423/events{/privacy}', 'received_events_url': 'https://api.github.com/users/fly23423/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}","[{'id': 5667396184, 'node_id': 'LA_kwDOJ0Z1Ps8AAAABUc2aWA', 'url': 'https://api.github.com/repos/ollama/ollama/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': ""Something isn't working""}]",closed,False,,[],,0,2025-11-19T10:37:06Z,2025-11-19T11:08:13Z,2025-11-19T11:08:13Z,NONE,,,"{'total': 0, 'completed': 0, 'percent_completed': 0}","{'blocked_by': 0, 'total_blocked_by': 0, 'blocking': 0, 'total_blocking': 0}","### What is the issue?

不支持添加api key

### Relevant log output

```shell

```

### OS

Docker

### GPU

Intel

### CPU

Intel

### Ollama version

0.12.11","{'login': 'rick-github', 'id': 14946854, 'node_id': 'MDQ6VXNlcjE0OTQ2ODU0', 'avatar_url': 'https://avatars.githubusercontent.com/u/14946854?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/rick-github', 'html_url': 'https://github.com/rick-github', 'followers_url': 'https://api.github.com/users/rick-github/followers', 'following_url': 'https://api.github.com/users/rick-github/following{/other_user}', 'gists_url': 'https://api.github.com/users/rick-github/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/rick-github/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/rick-github/subscriptions', 'organizations_url': 'https://api.github.com/users/rick-github/orgs', 'repos_url': 'https://api.github.com/users/rick-github/repos', 'events_url': 'https://api.github.com/users/rick-github/events{/privacy}', 'received_events_url': 'https://api.github.com/users/rick-github/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}","{'url': 'https://api.github.com/repos/ollama/ollama/issues/13155/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}",https://api.github.com/repos/ollama/ollama/issues/13155/timeline,,duplicate,,
https://api.github.com/repos/ollama/ollama/issues/13154,https://api.github.com/repos/ollama/ollama,https://api.github.com/repos/ollama/ollama/issues/13154/labels{/name},https://api.github.com/repos/ollama/ollama/issues/13154/comments,https://api.github.com/repos/ollama/ollama/issues/13154/events,https://github.com/ollama/ollama/issues/13154,3641239814,I_kwDOJ0Z1Ps7ZCOkG,13154,`think` Parameter Not Suppressing Reasoning in qwen3:4b When Set to `False`,"{'login': 'mfaizanhassan', 'id': 95165455, 'node_id': 'U_kgDOBawcDw', 'avatar_url': 'https://avatars.githubusercontent.com/u/95165455?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/mfaizanhassan', 'html_url': 'https://github.com/mfaizanhassan', 'followers_url': 'https://api.github.com/users/mfaizanhassan/followers', 'following_url': 'https://api.github.com/users/mfaizanhassan/following{/other_user}', 'gists_url': 'https://api.github.com/users/mfaizanhassan/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/mfaizanhassan/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/mfaizanhassan/subscriptions', 'organizations_url': 'https://api.github.com/users/mfaizanhassan/orgs', 'repos_url': 'https://api.github.com/users/mfaizanhassan/repos', 'events_url': 'https://api.github.com/users/mfaizanhassan/events{/privacy}', 'received_events_url': 'https://api.github.com/users/mfaizanhassan/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}","[{'id': 5667396184, 'node_id': 'LA_kwDOJ0Z1Ps8AAAABUc2aWA', 'url': 'https://api.github.com/repos/ollama/ollama/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': ""Something isn't working""}]",closed,False,,[],,0,2025-11-19T06:20:55Z,2025-11-19T07:58:54Z,2025-11-19T07:58:54Z,NONE,,,"{'total': 0, 'completed': 0, 'percent_completed': 0}","{'blocked_by': 0, 'total_blocked_by': 0, 'blocking': 0, 'total_blocking': 0}","### What is the issue?


The `think` parameter in Ollama's chat API does not properly suppress reasoning content when explicitly set to `False`. When `think=False` is used, the model still generates extensive reasoning/thinking content instead of providing a direct answer.

### Environment

- **Model**: `qwen3:4b`
- **Ollama API**: Python SDK (`ollama.chat`)
- **Expected Behavior**: Based on [Qwen3 documentation](https://arxiv.org/pdf/2505.09388), the model supports `/think` and `/no_think` directives to control reasoning output

### Steps to Reproduce

The issue can be reproduced by comparing the raw model behavior with Ollama's `think` parameter implementation.

---

## Experiment 1: Raw Qwen Model with `/think` and `/no_think` (Expected Behavior)

Using the Hugging Face transformers library directly with the Qwen3-4B model demonstrates the **correct behavior**:

```python
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

device = ""cuda"" if torch.cuda.is_available() else ""cpu""
tokenizer = AutoTokenizer.from_pretrained(""Qwen/Qwen3-4B"")
model = AutoModelForCausalLM.from_pretrained(
    ""Qwen/Qwen3-4B"",
    torch_dtype=torch.float16,
    device_map=""auto""
)

base_prompt = ""What is 5+5?""

# Test with /think
user_input_think = base_prompt + "" /think""
messages_think = [{""role"": ""user"", ""content"": user_input_think}]
text_think = tokenizer.apply_chat_template(
    messages_think,
    tokenize=False,
    add_generation_prompt=True
)
inputs_think = tokenizer(text_think, return_tensors=""pt"").to(device)
outputs_think = model.generate(**inputs_think, max_new_tokens=512)
response_think = tokenizer.decode(outputs_think[0][len(inputs_think.input_ids[0]):], skip_special_tokens=False)

print(f""With /think: {response_think}"")

# Test with /no_think
user_input_no_think = base_prompt + "" /no_think""
messages_no_think = [{""role"": ""user"", ""content"": user_input_no_think}]
text_no_think = tokenizer.apply_chat_template(
    messages_no_think,
    tokenize=False,
    add_generation_prompt=True
)
inputs_no_think = tokenizer(text_no_think, return_tensors=""pt"").to(device)
outputs_no_think = model.generate(**inputs_no_think, max_new_tokens=512)
response_no_think = tokenizer.decode(outputs_no_think[0][len(inputs_no_think.input_ids[0]):], skip_special_tokens=False)

print(f""With /no_think: {response_no_think}"")
```

**Output:**

**With `/think`:**
```
<think>
Okay, let's see. The user is asking ""What is 5+5?"" That seems straightforward...
[extensive reasoning omitted for brevity]
I think that's all. There's no ambiguity here. The answer is definitely 10.
</think>

The sum of 5 and 5 is **10**.
```

**With `/no_think`:**
```
<think>

</think>

5 + 5 equals 10.
```

### Result
✅ The raw model correctly responds to `/think` and `/no_think` directives:
- `/think`: Generates extensive reasoning inside `<think>` tags
- `/no_think`: Generates empty `<think></think>` tags with direct answer only

---

## Experiment 2: Ollama Chat API with `think` Parameter (Buggy Behavior)

Testing Ollama's `think` parameter with the same model shows **incorrect behavior**:

```python
from ollama import chat
from ollama import ChatResponse

test_cases = [False, None, True]
query = ""What is 5+5?""

for think_value in test_cases:
    print(f""\nTesting with think={think_value}"")

    response: ChatResponse = chat(**{
        'messages': [{'role': 'user', 'content': query}],
        'model': 'qwen3:4b',
        'think': think_value,
        'format': None,
        'options': {},
        'keep_alive': None
    })

    print(f""Content: {response.message.content}"")
    print(f""Thinking: {response.message.thinking}"")
```

**Output:**

### 1. `think=False` ❌ (BUGGY)
```
Content: Okay, the user asked ""What is 5+5?"" That seems straightforward. Let me think about how to approach this.

First, I should confirm what they're really asking. It's a basic math question, so they might be a kid learning addition, or maybe someone testing if I can do simple math...

[extensive reasoning continues in content field]

The result of **5 + 5** is **10**.

Thinking: None
```

**Problem**: The model generates full reasoning content despite `think=False`. The reasoning is embedded in the `content` field instead of being suppressed.

### 2. `think=None` ✅ (Default behavior)
```
Content: The answer to **5 + 5** is **10**.

Thinking: Okay, the user asked ""What is 5+5?"" Hmm, this seems like a very basic arithmetic question...
[reasoning properly separated]
```

**Result**: Thinking content is properly separated into the `thinking` field.

### 3. `think=True` ✅ (Explicit enable)
```
Content: The result of **5 + 5** is **10**.

Thinking: Okay, the user asked ""What is 5+5?"" Hmm, this seems like a super basic math question...
[reasoning properly separated]
```

**Result**: Same as `think=None`, thinking content properly separated.

---

## Expected vs Actual Behavior

### Expected Behavior for `think=False`
Based on how the raw model responds to `/no_think`, Ollama should:
- **Suppress all reasoning/thinking content**
- Return only a direct answer (similar to `/no_think` output)
- `content` field: Direct answer only
- `thinking` field: `None` or empty

### Actual Behavior for `think=False`
Currently, Ollama:
- ❌ **Does NOT suppress reasoning content**
- ❌ Generates full thinking/reasoning process
- ❌ Embeds reasoning in `content` field instead of separating it
- ✅ Sets `thinking` field to `None` (but this doesn't help since reasoning is in `content`)

## Additional Context

- Reference: [Qwen3 Technical Report - Section on reasoning control](https://arxiv.org/pdf/2505.09388)
- The issue is reproducible with `qwen3:4b`
- This bug affects downstream libraries that rely on Ollama's `think` parameter for controlling reasoning output such as Langchain.


### Relevant log output

```shell

```

### OS

Linux

### GPU

Nvidia

### CPU

Intel

### Ollama version

0.12.6","{'login': 'rick-github', 'id': 14946854, 'node_id': 'MDQ6VXNlcjE0OTQ2ODU0', 'avatar_url': 'https://avatars.githubusercontent.com/u/14946854?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/rick-github', 'html_url': 'https://github.com/rick-github', 'followers_url': 'https://api.github.com/users/rick-github/followers', 'following_url': 'https://api.github.com/users/rick-github/following{/other_user}', 'gists_url': 'https://api.github.com/users/rick-github/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/rick-github/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/rick-github/subscriptions', 'organizations_url': 'https://api.github.com/users/rick-github/orgs', 'repos_url': 'https://api.github.com/users/rick-github/repos', 'events_url': 'https://api.github.com/users/rick-github/events{/privacy}', 'received_events_url': 'https://api.github.com/users/rick-github/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}","{'url': 'https://api.github.com/repos/ollama/ollama/issues/13154/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}",https://api.github.com/repos/ollama/ollama/issues/13154/timeline,,duplicate,,
https://api.github.com/repos/ollama/ollama/issues/13153,https://api.github.com/repos/ollama/ollama,https://api.github.com/repos/ollama/ollama/issues/13153/labels{/name},https://api.github.com/repos/ollama/ollama/issues/13153/comments,https://api.github.com/repos/ollama/ollama/issues/13153/events,https://github.com/ollama/ollama/issues/13153,3641186740,I_kwDOJ0Z1Ps7ZCBm0,13153,Vulkan,"{'login': 'Swagatade', 'id': 116741845, 'node_id': 'U_kgDOBvVW1Q', 'avatar_url': 'https://avatars.githubusercontent.com/u/116741845?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/Swagatade', 'html_url': 'https://github.com/Swagatade', 'followers_url': 'https://api.github.com/users/Swagatade/followers', 'following_url': 'https://api.github.com/users/Swagatade/following{/other_user}', 'gists_url': 'https://api.github.com/users/Swagatade/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/Swagatade/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/Swagatade/subscriptions', 'organizations_url': 'https://api.github.com/users/Swagatade/orgs', 'repos_url': 'https://api.github.com/users/Swagatade/repos', 'events_url': 'https://api.github.com/users/Swagatade/events{/privacy}', 'received_events_url': 'https://api.github.com/users/Swagatade/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}","[{'id': 5667396184, 'node_id': 'LA_kwDOJ0Z1Ps8AAAABUc2aWA', 'url': 'https://api.github.com/repos/ollama/ollama/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': ""Something isn't working""}, {'id': 5808482718, 'node_id': 'LA_kwDOJ0Z1Ps8AAAABWjZpng', 'url': 'https://api.github.com/repos/ollama/ollama/labels/performance', 'name': 'performance', 'color': 'A5B5C6', 'default': False, 'description': ''}, {'id': 9653160222, 'node_id': 'LA_kwDOJ0Z1Ps8AAAACP1-JHg', 'url': 'https://api.github.com/repos/ollama/ollama/labels/vulkan', 'name': 'vulkan', 'color': 'A41E22', 'default': False, 'description': ''}]",open,False,,[],,2,2025-11-19T05:58:50Z,2025-11-20T20:56:41Z,,NONE,,,"{'total': 0, 'completed': 0, 'percent_completed': 0}","{'blocked_by': 0, 'total_blocked_by': 0, 'blocking': 0, 'total_blocking': 0}","### What is the issue?

intel ultra processor gpt-oss-safeguard very slow using Vulkan support. But only cpu performence is perfect.



### Relevant log output

```shell
C:\Users\swaga_nsppntr>set OLLAMA_VULKAN=1

C:\Users\swaga_nsppntr>ollama serve
time=2025-11-19T11:14:58.021+05:30 level=INFO source=routes.go:1544 msg=""server config"" env=""map[CUDA_VISIBLE_DEVICES: GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:C:\\Users\\swaga_nsppntr\\.ollama\\models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false OLLAMA_VULKAN:true ROCR_VISIBLE_DEVICES:]""
time=2025-11-19T11:14:58.272+05:30 level=INFO source=images.go:522 msg=""total blobs: 74""
time=2025-11-19T11:14:58.277+05:30 level=INFO source=images.go:529 msg=""total unused blobs removed: 0""
time=2025-11-19T11:14:58.283+05:30 level=INFO source=routes.go:1597 msg=""Listening on 127.0.0.1:11434 (version 0.12.11)""
time=2025-11-19T11:14:58.285+05:30 level=INFO source=runner.go:67 msg=""discovering available GPUs...""
time=2025-11-19T11:14:58.299+05:30 level=INFO source=server.go:392 msg=""starting runner"" cmd=""C:\\Users\\swaga_nsppntr\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 51942""
time=2025-11-19T11:14:59.184+05:30 level=INFO source=server.go:392 msg=""starting runner"" cmd=""C:\\Users\\swaga_nsppntr\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 51954""
time=2025-11-19T11:14:59.964+05:30 level=INFO source=server.go:392 msg=""starting runner"" cmd=""C:\\Users\\swaga_nsppntr\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 51961""
time=2025-11-19T11:15:00.292+05:30 level=INFO source=server.go:392 msg=""starting runner"" cmd=""C:\\Users\\swaga_nsppntr\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 51966""
time=2025-11-19T11:15:00.877+05:30 level=INFO source=types.go:42 msg=""inference compute"" id=8680a064-0400-0000-0002-000000000000 filter_id="""" library=Vulkan compute=0.0 name=Vulkan0 description=""Intel(R) Arc(TM) 130V GPU (16GB)"" libdirs=ollama,vulkan driver=0.0 pci_id="""" type=iGPU total=""27.6 GiB"" available=""26.8 GiB""
[GIN] 2025/11/19 - 11:15:40 | 200 |      2.1217ms |       127.0.0.1 | GET      ""/api/version""
[GIN] 2025/11/19 - 11:15:41 | 200 |     371.946ms |       127.0.0.1 | GET      ""/api/tags""
[GIN] 2025/11/19 - 11:15:41 | 200 |    335.6007ms |       127.0.0.1 | POST     ""/api/show""
[GIN] 2025/11/19 - 11:15:43 | 200 |     34.5837ms |       127.0.0.1 | GET      ""/api/tags""
[GIN] 2025/11/19 - 11:15:44 | 200 |    155.6211ms |       127.0.0.1 | POST     ""/api/show""
[GIN] 2025/11/19 - 11:15:44 | 200 |    116.4347ms |       127.0.0.1 | POST     ""/api/show""
time=2025-11-19T11:15:44.389+05:30 level=INFO source=server.go:392 msg=""starting runner"" cmd=""C:\\Users\\swaga_nsppntr\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 57220""
time=2025-11-19T11:15:45.033+05:30 level=INFO source=cpu_windows.go:148 msg=packages count=1
time=2025-11-19T11:15:45.033+05:30 level=INFO source=cpu_windows.go:164 msg=""efficiency cores detected"" maxEfficiencyClass=1
time=2025-11-19T11:15:45.034+05:30 level=INFO source=cpu_windows.go:195 msg="""" package=0 cores=8 efficiency=4 threads=8
time=2025-11-19T11:15:45.191+05:30 level=INFO source=server.go:209 msg=""enabling flash attention""
time=2025-11-19T11:15:45.194+05:30 level=INFO source=server.go:392 msg=""starting runner"" cmd=""C:\\Users\\swaga_nsppntr\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --model C:\\Users\\swaga_nsppntr\\.ollama\\models\\blobs\\sha256-c4016c9e54d0a9218b5911790579e58284a9ed57c48b7e87607125c6307f9da1 --port 57231""
time=2025-11-19T11:15:45.198+05:30 level=INFO source=sched.go:443 msg=""system memory"" total=""31.5 GiB"" free=""17.0 GiB"" free_swap=""31.9 GiB""
time=2025-11-19T11:15:45.198+05:30 level=INFO source=sched.go:450 msg=""gpu memory"" id=8680a064-0400-0000-0002-000000000000 library=Vulkan available=""26.1 GiB"" free=""26.6 GiB"" minimum=""457.0 MiB"" overhead=""0 B""
time=2025-11-19T11:15:45.198+05:30 level=INFO source=server.go:702 msg=""loading model"" ""model layers""=25 requested=-1
time=2025-11-19T11:15:45.258+05:30 level=INFO source=runner.go:1398 msg=""starting ollama engine""
time=2025-11-19T11:15:45.260+05:30 level=INFO source=runner.go:1433 msg=""Server listening on 127.0.0.1:57231""
time=2025-11-19T11:15:45.267+05:30 level=INFO source=runner.go:1271 msg=load request=""{Operation:fit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:true KvSize:8192 KvCacheType: NumThreads:4 GPULayers:25[ID:8680a064-0400-0000-0002-000000000000 Layers:25(0..24)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}""
time=2025-11-19T11:15:45.322+05:30 level=INFO source=ggml.go:136 msg="""" architecture=gptoss file_type=MXFP4 name="""" description="""" num_tensors=459 num_key_values=32
load_backend: loaded CPU backend from C:\Users\swaga_nsppntr\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-alderlake.dll
ggml_vulkan: Found 1 Vulkan devices:
ggml_vulkan: 0 = Intel(R) Arc(TM) 130V GPU (16GB) (Intel Corporation) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 32 | shared memory: 32768 | int dot: 1 | matrix cores: KHR_coopmat
load_backend: loaded Vulkan backend from C:\Users\swaga_nsppntr\AppData\Local\Programs\Ollama\lib\ollama\vulkan\ggml-vulkan.dll
time=2025-11-19T11:15:45.427+05:30 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 compiler=cgo(clang)
ggml_backend_vk_get_device_memory called: uuid 8680a064-0400-0000-0002-000000000000
ggml_backend_vk_get_device_memory called: luid 0x000000000000ff6b
ggml_dxgi_pdh_init called
DXGI + PDH Initialized. Getting GPU free memory info
[DXGI] Adapter Description: Intel(R) Arc(TM) 130V GPU (16GB), LUID: 0x000000000000FF6B, Dedicated: 0.12 GB, Shared: 27.45 GB
[DXGI] Adapter Description: Microsoft Basic Render Driver, LUID: 0x000000000001038E, Dedicated: 0.00 GB, Shared: 27.45 GB
Integrated GPU (Intel(R) Arc(TM) 130V GPU (16GB)) with LUID 0x000000000000ff6b detected. Shared Total: 29470189240.00 bytes (27.45 GB), Shared Usage: 1070800896.00 bytes (1.00 GB), Dedicated Total: 134217728.00 bytes (0.12 GB), Dedicated Usage: 0.00 bytes (0.00 GB)
ggml_backend_vk_get_device_memory utilizing DXGI + PDH memory reporting free: 28533606072 total: 29604406968
time=2025-11-19T11:15:45.809+05:30 level=INFO source=runner.go:1271 msg=load request=""{Operation:alloc LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:true KvSize:8192 KvCacheType: NumThreads:4 GPULayers:25[ID:8680a064-0400-0000-0002-000000000000 Layers:25(0..24)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}""
ggml_backend_vk_get_device_memory called: uuid 8680a064-0400-0000-0002-000000000000
ggml_backend_vk_get_device_memory called: luid 0x000000000000ff6b
ggml_dxgi_pdh_init called
DXGI + PDH Initialized. Getting GPU free memory info
[DXGI] Adapter Description: Intel(R) Arc(TM) 130V GPU (16GB), LUID: 0x000000000000FF6B, Dedicated: 0.12 GB, Shared: 27.45 GB
[DXGI] Adapter Description: Microsoft Basic Render Driver, LUID: 0x000000000001038E, Dedicated: 0.00 GB, Shared: 27.45 GB
Integrated GPU (Intel(R) Arc(TM) 130V GPU (16GB)) with LUID 0x000000000000ff6b detected. Shared Total: 29470189240.00 bytes (27.45 GB), Shared Usage: 1070800896.00 bytes (1.00 GB), Dedicated Total: 134217728.00 bytes (0.12 GB), Dedicated Usage: 0.00 bytes (0.00 GB)
ggml_backend_vk_get_device_memory utilizing DXGI + PDH memory reporting free: 28533606072 total: 29604406968
time=2025-11-19T11:15:51.463+05:30 level=INFO source=runner.go:1271 msg=load request=""{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:true KvSize:8192 KvCacheType: NumThreads:4 GPULayers:25[ID:8680a064-0400-0000-0002-000000000000 Layers:25(0..24)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}""
time=2025-11-19T11:15:51.463+05:30 level=INFO source=ggml.go:482 msg=""offloading 24 repeating layers to GPU""
time=2025-11-19T11:15:51.463+05:30 level=INFO source=ggml.go:489 msg=""offloading output layer to GPU""
time=2025-11-19T11:15:51.463+05:30 level=INFO source=ggml.go:494 msg=""offloaded 25/25 layers to GPU""
time=2025-11-19T11:15:51.465+05:30 level=INFO source=device.go:240 msg=""model weights"" device=Vulkan0 size=""11.8 GiB""
time=2025-11-19T11:15:51.465+05:30 level=INFO source=device.go:245 msg=""model weights"" device=CPU size=""1.1 GiB""
time=2025-11-19T11:15:51.465+05:30 level=INFO source=device.go:251 msg=""kv cache"" device=Vulkan0 size=""300.0 MiB""
time=2025-11-19T11:15:51.465+05:30 level=INFO source=device.go:262 msg=""compute graph"" device=Vulkan0 size=""110.6 MiB""
time=2025-11-19T11:15:51.467+05:30 level=INFO source=device.go:267 msg=""compute graph"" device=CPU size=""5.6 MiB""
time=2025-11-19T11:15:51.468+05:30 level=INFO source=device.go:272 msg=""total memory"" size=""13.2 GiB""
time=2025-11-19T11:15:51.468+05:30 level=INFO source=sched.go:517 msg=""loaded runners"" count=1
time=2025-11-19T11:15:51.468+05:30 level=INFO source=server.go:1294 msg=""waiting for llama runner to start responding""
time=2025-11-19T11:15:51.469+05:30 level=INFO source=server.go:1328 msg=""waiting for server to become available"" status=""llm server loading model""
```

### OS

Windows

### GPU

Intel

### CPU

Intel

### Ollama version

v0.12.11",,"{'url': 'https://api.github.com/repos/ollama/ollama/issues/13153/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}",https://api.github.com/repos/ollama/ollama/issues/13153/timeline,,,,
https://api.github.com/repos/ollama/ollama/issues/13152,https://api.github.com/repos/ollama/ollama,https://api.github.com/repos/ollama/ollama/issues/13152/labels{/name},https://api.github.com/repos/ollama/ollama/issues/13152/comments,https://api.github.com/repos/ollama/ollama/issues/13152/events,https://github.com/ollama/ollama/issues/13152,3641174317,I_kwDOJ0Z1Ps7ZB-kt,13152,GPT-OSS: 120B doesnt share between CPU/GPU @ CTX over 8192,"{'login': 'Stef1519', 'id': 169326369, 'node_id': 'U_kgDOChe3IQ', 'avatar_url': 'https://avatars.githubusercontent.com/u/169326369?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/Stef1519', 'html_url': 'https://github.com/Stef1519', 'followers_url': 'https://api.github.com/users/Stef1519/followers', 'following_url': 'https://api.github.com/users/Stef1519/following{/other_user}', 'gists_url': 'https://api.github.com/users/Stef1519/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/Stef1519/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/Stef1519/subscriptions', 'organizations_url': 'https://api.github.com/users/Stef1519/orgs', 'repos_url': 'https://api.github.com/users/Stef1519/repos', 'events_url': 'https://api.github.com/users/Stef1519/events{/privacy}', 'received_events_url': 'https://api.github.com/users/Stef1519/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}","[{'id': 5667396184, 'node_id': 'LA_kwDOJ0Z1Ps8AAAABUc2aWA', 'url': 'https://api.github.com/repos/ollama/ollama/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': ""Something isn't working""}, {'id': 6677367769, 'node_id': 'LA_kwDOJ0Z1Ps8AAAABjgCL2Q', 'url': 'https://api.github.com/repos/ollama/ollama/labels/needs%20more%20info', 'name': 'needs more info', 'color': 'BA8041', 'default': False, 'description': 'More information is needed to assist'}]",open,False,,[],,1,2025-11-19T05:53:08Z,2025-11-20T14:21:21Z,,NONE,,,"{'total': 0, 'completed': 0, 'percent_completed': 0}","{'blocked_by': 0, 'total_blocked_by': 0, 'blocking': 0, 'total_blocking': 0}","### What is the issue?

OS: Ubuntu
Ollama Version: 0.12.11
Hardware: Xeon E5 2697 V2, 2 CPUs, 128 GB RAM, 3xP106-090, 1xP106-100, Tesla M10 32 GB, Total 56 GB GPU Memory

CLI & OpenWebUI

Issue: #1  

GPT-OSS:120B happily shares between CPU/GPU when using a CTX Size of 8192 (altrough nearly 50:50 wouldn't be very valid either, considering 56 GB GPU memory):

num_ctx = 8192:
Gpt-oss:120b    a951a23b46a1    67 GB    48%/52% CPU/GPU    8192       4 minutes from now    
Works somehow, responses not very fast, but within one Minute.

num_ctx=32768:
gpt-oss:120b    a951a23b46a1    66 GB    100% CPU     32768      4 minutes from now    


num_ctx=32768 and num_gpu=16:
gpt-oss:120b    a951a23b46a1    68 GB    56%/44% CPU/GPU    32768      4 minutes from now    
No response for ages (probably not at all, was giving up after ten minutes)


Issue #2:

GPT-OSS:20B: Takes ages to initially load, easily five minutes or more (occupying 14 GB GPU memory)

Any clues?
Thank you very much!





### Relevant log output

```shell

```

### OS

_No response_

### GPU

_No response_

### CPU

_No response_

### Ollama version

_No response_",,"{'url': 'https://api.github.com/repos/ollama/ollama/issues/13152/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}",https://api.github.com/repos/ollama/ollama/issues/13152/timeline,,,,
https://api.github.com/repos/ollama/ollama/issues/13151,https://api.github.com/repos/ollama/ollama,https://api.github.com/repos/ollama/ollama/issues/13151/labels{/name},https://api.github.com/repos/ollama/ollama/issues/13151/comments,https://api.github.com/repos/ollama/ollama/issues/13151/events,https://github.com/ollama/ollama/pull/13151,3641173943,PR_kwDOJ0Z1Ps60RYi9,13151,models: enable deepseek2 (deepseek v3.1 w/ MLA) on the new engine,"{'login': 'pdevine', 'id': 75239, 'node_id': 'MDQ6VXNlcjc1MjM5', 'avatar_url': 'https://avatars.githubusercontent.com/u/75239?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/pdevine', 'html_url': 'https://github.com/pdevine', 'followers_url': 'https://api.github.com/users/pdevine/followers', 'following_url': 'https://api.github.com/users/pdevine/following{/other_user}', 'gists_url': 'https://api.github.com/users/pdevine/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/pdevine/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/pdevine/subscriptions', 'organizations_url': 'https://api.github.com/users/pdevine/orgs', 'repos_url': 'https://api.github.com/users/pdevine/repos', 'events_url': 'https://api.github.com/users/pdevine/events{/privacy}', 'received_events_url': 'https://api.github.com/users/pdevine/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}",[],closed,False,,[],,0,2025-11-19T05:52:56Z,2025-11-19T06:03:52Z,2025-11-19T06:03:50Z,CONTRIBUTOR,,,,,,"{'login': 'pdevine', 'id': 75239, 'node_id': 'MDQ6VXNlcjc1MjM5', 'avatar_url': 'https://avatars.githubusercontent.com/u/75239?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/pdevine', 'html_url': 'https://github.com/pdevine', 'followers_url': 'https://api.github.com/users/pdevine/followers', 'following_url': 'https://api.github.com/users/pdevine/following{/other_user}', 'gists_url': 'https://api.github.com/users/pdevine/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/pdevine/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/pdevine/subscriptions', 'organizations_url': 'https://api.github.com/users/pdevine/orgs', 'repos_url': 'https://api.github.com/users/pdevine/repos', 'events_url': 'https://api.github.com/users/pdevine/events{/privacy}', 'received_events_url': 'https://api.github.com/users/pdevine/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}","{'url': 'https://api.github.com/repos/ollama/ollama/issues/13151/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}",https://api.github.com/repos/ollama/ollama/issues/13151/timeline,,,False,"{'url': 'https://api.github.com/repos/ollama/ollama/pulls/13151', 'html_url': 'https://github.com/ollama/ollama/pull/13151', 'diff_url': 'https://github.com/ollama/ollama/pull/13151.diff', 'patch_url': 'https://github.com/ollama/ollama/pull/13151.patch', 'merged_at': '2025-11-19T06:03:50Z'}"
https://api.github.com/repos/ollama/ollama/issues/13150,https://api.github.com/repos/ollama/ollama,https://api.github.com/repos/ollama/ollama/issues/13150/labels{/name},https://api.github.com/repos/ollama/ollama/issues/13150/comments,https://api.github.com/repos/ollama/ollama/issues/13150/events,https://github.com/ollama/ollama/issues/13150,3641076430,I_kwDOJ0Z1Ps7ZBmrO,13150,"""runtime error: invalid memory address or nil pointer dereference"" with Qwen3 VL","{'login': 'fappaz', 'id': 4013668, 'node_id': 'MDQ6VXNlcjQwMTM2Njg=', 'avatar_url': 'https://avatars.githubusercontent.com/u/4013668?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/fappaz', 'html_url': 'https://github.com/fappaz', 'followers_url': 'https://api.github.com/users/fappaz/followers', 'following_url': 'https://api.github.com/users/fappaz/following{/other_user}', 'gists_url': 'https://api.github.com/users/fappaz/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/fappaz/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/fappaz/subscriptions', 'organizations_url': 'https://api.github.com/users/fappaz/orgs', 'repos_url': 'https://api.github.com/users/fappaz/repos', 'events_url': 'https://api.github.com/users/fappaz/events{/privacy}', 'received_events_url': 'https://api.github.com/users/fappaz/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}","[{'id': 5667396184, 'node_id': 'LA_kwDOJ0Z1Ps8AAAABUc2aWA', 'url': 'https://api.github.com/repos/ollama/ollama/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': ""Something isn't working""}]",open,False,,[],,7,2025-11-19T05:01:58Z,2025-11-24T00:12:29Z,,NONE,,,"{'total': 0, 'completed': 0, 'percent_completed': 0}","{'blocked_by': 0, 'total_blocked_by': 0, 'blocking': 0, 'total_blocking': 0}","### What is the issue?

Getting the following error when interacting with [Qwen3 Vl 8b Thinking Heretic](https://huggingface.co/Kizzington/Qwen3-VL-8B-Thinking-heretic-GGUF/blob/main/qwen3-vl-8B-thinking-heretic.Q4_K_M.gguf):

```log
http: panic serving 127.0.0.1:56959: runtime error: invalid memory address or nil pointer dereference
```

The UI shows this error:

```log
500 Internal Server Error: do load request: Post ""http://127.0.0.1:57029/load"": EOF
```

Tried restarting ollama and redownloading the model, but to no avail. 

I've found similar issues reported on github, but they mostly happened with other model architectures, so not sure if they're the same:
- https://github.com/ollama/ollama/issues/11280
- https://github.com/ollama/ollama/issues/12426

### Relevant log output

```shell
`server.log`:


time=2025-11-19T17:40:54.102+13:00 level=INFO source=routes.go:1544 msg=""server config"" env=""map[CUDA_VISIBLE_DEVICES: GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:C:\\Users\\Lab\\.ollama\\models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:true OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false OLLAMA_VULKAN:false ROCR_VISIBLE_DEVICES:]""
time=2025-11-19T17:40:54.105+13:00 level=INFO source=images.go:522 msg=""total blobs: 17""
time=2025-11-19T17:40:54.106+13:00 level=INFO source=images.go:529 msg=""total unused blobs removed: 0""
time=2025-11-19T17:40:54.108+13:00 level=INFO source=routes.go:1597 msg=""Listening on 127.0.0.1:11434 (version 0.12.11)""
time=2025-11-19T17:40:54.109+13:00 level=INFO source=runner.go:67 msg=""discovering available GPUs...""
time=2025-11-19T17:40:54.109+13:00 level=INFO source=runner.go:98 msg=""experimental Vulkan support disabled.  To enable, set OLLAMA_VULKAN=1""
time=2025-11-19T17:40:54.119+13:00 level=INFO source=server.go:392 msg=""starting runner"" cmd=""C:\\Users\\Lab\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 56916""
time=2025-11-19T17:40:54.316+13:00 level=INFO source=server.go:392 msg=""starting runner"" cmd=""C:\\Users\\Lab\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 56922""
time=2025-11-19T17:40:54.463+13:00 level=INFO source=server.go:392 msg=""starting runner"" cmd=""C:\\Users\\Lab\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 56928""
time=2025-11-19T17:40:54.561+13:00 level=INFO source=types.go:42 msg=""inference compute"" id=GPU-37b1c641-5754-e474-ca7d-ad35f2dac825 filter_id="""" library=CUDA compute=8.6 name=CUDA0 description=""NVIDIA GeForce RTX 3080 Laptop GPU"" libdirs=ollama,cuda_v12 driver=12.7 pci_id=0000:01:00.0 type=discrete total=""8.0 GiB"" available=""7.0 GiB""
time=2025-11-19T17:40:54.561+13:00 level=INFO source=routes.go:1638 msg=""entering low vram mode"" ""total vram""=""8.0 GiB"" threshold=""20.0 GiB""
[GIN] 2025/11/19 - 17:40:54 | 200 |       525.8µs |       127.0.0.1 | GET      ""/api/version""
[GIN] 2025/11/19 - 17:40:54 | 200 |       2.653ms |       127.0.0.1 | GET      ""/api/tags""
[GIN] 2025/11/19 - 17:40:55 | 200 |      53.143ms |       127.0.0.1 | POST     ""/api/show""
time=2025-11-19T17:41:04.842+13:00 level=INFO source=server.go:392 msg=""starting runner"" cmd=""C:\\Users\\Lab\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 56948""
time=2025-11-19T17:41:05.045+13:00 level=INFO source=cpu_windows.go:148 msg=packages count=1
time=2025-11-19T17:41:05.045+13:00 level=INFO source=cpu_windows.go:195 msg="""" package=0 cores=8 efficiency=0 threads=16
time=2025-11-19T17:41:05.103+13:00 level=INFO source=server.go:209 msg=""enabling flash attention""
time=2025-11-19T17:41:05.107+13:00 level=INFO source=server.go:392 msg=""starting runner"" cmd=""C:\\Users\\Lab\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --model C:\\Users\\Lab\\.ollama\\models\\blobs\\sha256-61eba9f6307e5ea0f81608aae6725c9cb69b719efd60b04e479027485311d06f --port 56953""
time=2025-11-19T17:41:05.109+13:00 level=INFO source=sched.go:443 msg=""system memory"" total=""31.4 GiB"" free=""17.7 GiB"" free_swap=""42.4 GiB""
time=2025-11-19T17:41:05.109+13:00 level=INFO source=sched.go:450 msg=""gpu memory"" id=GPU-37b1c641-5754-e474-ca7d-ad35f2dac825 library=CUDA available=""6.6 GiB"" free=""7.0 GiB"" minimum=""457.0 MiB"" overhead=""0 B""
time=2025-11-19T17:41:05.109+13:00 level=INFO source=server.go:702 msg=""loading model"" ""model layers""=37 requested=-1
time=2025-11-19T17:41:05.151+13:00 level=INFO source=runner.go:1398 msg=""starting ollama engine""
time=2025-11-19T17:41:05.152+13:00 level=INFO source=runner.go:1433 msg=""Server listening on 127.0.0.1:56953""
time=2025-11-19T17:41:05.162+13:00 level=INFO source=runner.go:1271 msg=load request=""{Operation:fit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:true KvSize:4096 KvCacheType: NumThreads:8 GPULayers:37[ID:GPU-37b1c641-5754-e474-ca7d-ad35f2dac825 Layers:37(0..36)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}""
time=2025-11-19T17:41:05.186+13:00 level=INFO source=ggml.go:136 msg="""" architecture=qwen3vl file_type=Q4_K_M name=""Qwen3 Vl 8b Thinking Heretic"" description="""" num_tensors=399 num_key_values=31
load_backend: loaded CPU backend from C:\Users\Lab\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-haswell.dll
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3080 Laptop GPU, compute capability 8.6, VMM: yes, ID: GPU-37b1c641-5754-e474-ca7d-ad35f2dac825
load_backend: loaded CUDA backend from C:\Users\Lab\AppData\Local\Programs\Ollama\lib\ollama\cuda_v12\ggml-cuda.dll
time=2025-11-19T17:41:05.311+13:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,520,600,610,700,750,800,860,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2025-11-19T17:41:05.620+13:00 level=INFO source=server.go:3634 msg=""http: panic serving 127.0.0.1:56959: runtime error: invalid memory address or nil pointer dereference\ngoroutine 15 [running]:\nnet/http.(*conn).serve.func1()\n\tnet/http/server.go:1947 +0xbe\npanic({0x7ff72ad66220?, 0x7ff72b9f83d0?})\n\truntime/panic.go:792 +0x132\ngithub.com/ollama/ollama/runner/ollamarunner.(*Server).allocModel.func1()\n\tgithub.com/ollama/ollama/runner/ollamarunner/runner.go:1186 +0x11a\npanic({0x7ff72ad66220?, 0x7ff72b9f83d0?})\n\truntime/panic.go:792 +0x132\ngithub.com/ollama/ollama/ml/nn.(*Conv3D).Forward(0x0, {0x7ff72b0ad680, 0xc000e1a580}, {0x7ff72b0b9ee8?, 0xc000e0e048?}, 0x10101f72ba4e000?, 0x1f978441200?, 0x1f972eb0108?, 0x10?, 0x0, ...)\n\tgithub.com/ollama/ollama/ml/nn/convolution.go:25 +0x3a\ngithub.com/ollama/ollama/model/models/qwen3vl.(*VisionModel).Forward(0xc0004680c0, {0x7ff72b0ad680, 0xc000e1a580}, {0x7ff72b0b9ee8, 0xc000e0e030}, 0xc000cbd080)\n\tgithub.com/ollama/ollama/model/models/qwen3vl/model_vision.go:223 +0x118\ngithub.com/ollama/ollama/model/models/qwen3vl.(*Model).EncodeMultimodal(0xc000138680, {0x7ff72b0ad680, 0xc000e1a580}, {0xc001ac2000, 0x400436, 0x700000})\n\tgithub.com/ollama/ollama/model/models/qwen3vl/model.go:43 +0x14e\ngithub.com/ollama/ollama/runner/ollamarunner.(*Server).reserveWorstCaseGraph(0xc0002050e0, 0x1)\n\tgithub.com/ollama/ollama/runner/ollamarunner/runner.go:1097 +0x34e\ngithub.com/ollama/ollama/runner/ollamarunner.(*Server).allocModel(0xc0002050e0, {0xc0000980e0?, 0x7ff729eca3ba?}, {0x0, 0x8, {0xc00011c080, 0x1, 0x1}, 0x1}, {0x0, ...}, ...)\n\tgithub.com/ollama/ollama/runner/ollamarunner/runner.go:1219 +0x2b1\ngithub.com/ollama/ollama/runner/ollamarunner.(*Server).load(0xc0002050e0, {0x7ff72b09ff90, 0xc000122000}, 0xc0000c2000)\n\tgithub.com/ollama/ollama/runner/ollamarunner/runner.go:1298 +0x54d\nnet/http.HandlerFunc.ServeHTTP(0xc000469440?, {0x7ff72b09ff90?, 0xc000122000?}, 0xc000047b60?)\n\tnet/http/server.go:2294 +0x29\nnet/http.(*ServeMux).ServeHTTP(0x7ff729b6b785?, {0x7ff72b09ff90, 0xc000122000}, 0xc0000c2000)\n\tnet/http/server.go:2822 +0x1c4\nnet/http.serverHandler.ServeHTTP({0x7ff72b09c510?}, {0x7ff72b09ff90?, 0xc000122000?}, 0x1?)\n\tnet/http/server.go:3301 +0x8e\nnet/http.(*conn).serve(0xc0006aa3f0, {0x7ff72b0a2348, 0xc0006b05d0})\n\tnet/http/server.go:2102 +0x625\ncreated by net/http.(*Server).Serve in goroutine 1\n\tnet/http/server.go:3454 +0x485""
time=2025-11-19T17:41:05.621+13:00 level=INFO source=runner.go:1271 msg=load request=""{Operation:close LoraPath:[] Parallel:0 BatchSize:0 FlashAttention:false KvSize:0 KvCacheType: NumThreads:0 GPULayers:[] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}""
time=2025-11-19T17:41:05.621+13:00 level=INFO source=sched.go:470 msg=""Load failed"" model=C:\Users\Lab\.ollama\models\blobs\sha256-61eba9f6307e5ea0f81608aae6725c9cb69b719efd60b04e479027485311d06f error=""do load request: Post \""http://127.0.0.1:56953/load\"": EOF""
time=2025-11-19T17:41:05.659+13:00 level=ERROR source=server.go:265 msg=""llama runner terminated"" error=""exit status 1""
[GIN] 2025/11/19 - 17:41:05 | 500 |    904.2454ms |       127.0.0.1 | POST     ""/api/chat""
[GIN] 2025/11/19 - 17:41:09 | 200 |      2.0956ms |       127.0.0.1 | GET      ""/api/tags""
[GIN] 2025/11/19 - 17:41:09 | 200 |            0s |       127.0.0.1 | GET      ""/api/ps""
```

### OS

Windows

### GPU

Nvidia

### CPU

AMD

### Ollama version

0.12.11",,"{'url': 'https://api.github.com/repos/ollama/ollama/issues/13150/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}",https://api.github.com/repos/ollama/ollama/issues/13150/timeline,,,,
https://api.github.com/repos/ollama/ollama/issues/13149,https://api.github.com/repos/ollama/ollama,https://api.github.com/repos/ollama/ollama/issues/13149/labels{/name},https://api.github.com/repos/ollama/ollama/issues/13149/comments,https://api.github.com/repos/ollama/ollama/issues/13149/events,https://github.com/ollama/ollama/pull/13149,3640966463,PR_kwDOJ0Z1Ps60QsGp,13149,chore: mark vulkan shaders as vendored files,"{'login': 'mxyng', 'id': 2372640, 'node_id': 'MDQ6VXNlcjIzNzI2NDA=', 'avatar_url': 'https://avatars.githubusercontent.com/u/2372640?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/mxyng', 'html_url': 'https://github.com/mxyng', 'followers_url': 'https://api.github.com/users/mxyng/followers', 'following_url': 'https://api.github.com/users/mxyng/following{/other_user}', 'gists_url': 'https://api.github.com/users/mxyng/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/mxyng/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/mxyng/subscriptions', 'organizations_url': 'https://api.github.com/users/mxyng/orgs', 'repos_url': 'https://api.github.com/users/mxyng/repos', 'events_url': 'https://api.github.com/users/mxyng/events{/privacy}', 'received_events_url': 'https://api.github.com/users/mxyng/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}",[],closed,False,,[],,0,2025-11-19T04:00:01Z,2025-11-19T20:01:25Z,2025-11-19T20:01:23Z,CONTRIBUTOR,,,,,,"{'login': 'mxyng', 'id': 2372640, 'node_id': 'MDQ6VXNlcjIzNzI2NDA=', 'avatar_url': 'https://avatars.githubusercontent.com/u/2372640?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/mxyng', 'html_url': 'https://github.com/mxyng', 'followers_url': 'https://api.github.com/users/mxyng/followers', 'following_url': 'https://api.github.com/users/mxyng/following{/other_user}', 'gists_url': 'https://api.github.com/users/mxyng/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/mxyng/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/mxyng/subscriptions', 'organizations_url': 'https://api.github.com/users/mxyng/orgs', 'repos_url': 'https://api.github.com/users/mxyng/repos', 'events_url': 'https://api.github.com/users/mxyng/events{/privacy}', 'received_events_url': 'https://api.github.com/users/mxyng/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}","{'url': 'https://api.github.com/repos/ollama/ollama/issues/13149/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}",https://api.github.com/repos/ollama/ollama/issues/13149/timeline,,,False,"{'url': 'https://api.github.com/repos/ollama/ollama/pulls/13149', 'html_url': 'https://github.com/ollama/ollama/pull/13149', 'diff_url': 'https://github.com/ollama/ollama/pull/13149.diff', 'patch_url': 'https://github.com/ollama/ollama/pull/13149.patch', 'merged_at': '2025-11-19T20:01:23Z'}"
https://api.github.com/repos/ollama/ollama/issues/13148,https://api.github.com/repos/ollama/ollama,https://api.github.com/repos/ollama/ollama/issues/13148/labels{/name},https://api.github.com/repos/ollama/ollama/issues/13148/comments,https://api.github.com/repos/ollama/ollama/issues/13148/events,https://github.com/ollama/ollama/issues/13148,3640935010,I_kwDOJ0Z1Ps7ZBEJi,13148,Add ToolCalls to ChatResponse,"{'login': 'seven1240', 'id': 31364, 'node_id': 'MDQ6VXNlcjMxMzY0', 'avatar_url': 'https://avatars.githubusercontent.com/u/31364?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/seven1240', 'html_url': 'https://github.com/seven1240', 'followers_url': 'https://api.github.com/users/seven1240/followers', 'following_url': 'https://api.github.com/users/seven1240/following{/other_user}', 'gists_url': 'https://api.github.com/users/seven1240/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/seven1240/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/seven1240/subscriptions', 'organizations_url': 'https://api.github.com/users/seven1240/orgs', 'repos_url': 'https://api.github.com/users/seven1240/repos', 'events_url': 'https://api.github.com/users/seven1240/events{/privacy}', 'received_events_url': 'https://api.github.com/users/seven1240/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}","[{'id': 5667396200, 'node_id': 'LA_kwDOJ0Z1Ps8AAAABUc2aaA', 'url': 'https://api.github.com/repos/ollama/ollama/labels/feature%20request', 'name': 'feature request', 'color': 'a2eeef', 'default': False, 'description': 'New feature or request'}]",closed,False,,[],,6,2025-11-19T03:40:59Z,2025-11-23T06:55:56Z,2025-11-23T06:55:56Z,NONE,,,"{'total': 0, 'completed': 0, 'percent_completed': 0}","{'blocked_by': 0, 'total_blocked_by': 0, 'blocking': 0, 'total_blocking': 0}","Hi,

This is great but I found a problem:

ChatResponse has no ToolCalls, while ChatRequest has Tools

GenerateResponse has ToolCalls, but GenerateRequest has no Tools

How to call Generate or Chat with ToolCalls? Thanks.
","{'login': 'seven1240', 'id': 31364, 'node_id': 'MDQ6VXNlcjMxMzY0', 'avatar_url': 'https://avatars.githubusercontent.com/u/31364?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/seven1240', 'html_url': 'https://github.com/seven1240', 'followers_url': 'https://api.github.com/users/seven1240/followers', 'following_url': 'https://api.github.com/users/seven1240/following{/other_user}', 'gists_url': 'https://api.github.com/users/seven1240/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/seven1240/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/seven1240/subscriptions', 'organizations_url': 'https://api.github.com/users/seven1240/orgs', 'repos_url': 'https://api.github.com/users/seven1240/repos', 'events_url': 'https://api.github.com/users/seven1240/events{/privacy}', 'received_events_url': 'https://api.github.com/users/seven1240/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}","{'url': 'https://api.github.com/repos/ollama/ollama/issues/13148/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}",https://api.github.com/repos/ollama/ollama/issues/13148/timeline,,completed,,
https://api.github.com/repos/ollama/ollama/issues/13147,https://api.github.com/repos/ollama/ollama,https://api.github.com/repos/ollama/ollama/issues/13147/labels{/name},https://api.github.com/repos/ollama/ollama/issues/13147/comments,https://api.github.com/repos/ollama/ollama/issues/13147/events,https://github.com/ollama/ollama/pull/13147,3640928672,PR_kwDOJ0Z1Ps60Qkez,13147,Ollama add remote server support,"{'login': 'sonofagl1tch', 'id': 20155474, 'node_id': 'MDQ6VXNlcjIwMTU1NDc0', 'avatar_url': 'https://avatars.githubusercontent.com/u/20155474?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/sonofagl1tch', 'html_url': 'https://github.com/sonofagl1tch', 'followers_url': 'https://api.github.com/users/sonofagl1tch/followers', 'following_url': 'https://api.github.com/users/sonofagl1tch/following{/other_user}', 'gists_url': 'https://api.github.com/users/sonofagl1tch/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/sonofagl1tch/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/sonofagl1tch/subscriptions', 'organizations_url': 'https://api.github.com/users/sonofagl1tch/orgs', 'repos_url': 'https://api.github.com/users/sonofagl1tch/repos', 'events_url': 'https://api.github.com/users/sonofagl1tch/events{/privacy}', 'received_events_url': 'https://api.github.com/users/sonofagl1tch/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}",[],open,False,,[],,3,2025-11-19T03:36:53Z,2025-11-20T16:43:09Z,,NONE,,,,,"# Add Remote Server Configuration Support

## Summary
This PR adds the ability to configure and connect to remote Ollama server instances, enabling users to run the Ollama client while connecting to a server hosted elsewhere.

## Changes
- **Configuration Management**: Added `server_url` configuration option to specify remote Ollama server endpoints
- **CLI Enhancement**: Extended `ollama config` command to support setting and viewing server URL configuration
- **GUI Integration**: Updated desktop application settings to include server URL configuration with data validation
- **Database Schema**: Enhanced store database to persist server configuration settings
- **Documentation**: Added comprehensive documentation for configuration capabilities and CLI usage

## Key Features
- Set server URL via CLI: `ollama config server_url http://remote-host:11434`
- View current configuration: `ollama config`
- GUI settings panel for easy server URL management
- Input validation and sanitization for server URLs
- Backward compatibility with existing local installations

## Files Modified
- `cmd/config.go` - New configuration management functionality
- `app/store/` - Database schema and store enhancements
- `app/ui/app/src/components/Settings.tsx` - GUI configuration interface
- `config/config.go` - Core configuration handling
- `docs/` - Updated documentation for new capabilities

## Testing
- Added unit tests for configuration validation
- Tested GUI input validation and error handling
- Verified backward compatibility with existing setups
",,"{'url': 'https://api.github.com/repos/ollama/ollama/issues/13147/reactions', 'total_count': 2, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 1, 'confused': 0, 'heart': 1, 'rocket': 0, 'eyes': 0}",https://api.github.com/repos/ollama/ollama/issues/13147/timeline,,,False,"{'url': 'https://api.github.com/repos/ollama/ollama/pulls/13147', 'html_url': 'https://github.com/ollama/ollama/pull/13147', 'diff_url': 'https://github.com/ollama/ollama/pull/13147.diff', 'patch_url': 'https://github.com/ollama/ollama/pull/13147.patch', 'merged_at': None}"
https://api.github.com/repos/ollama/ollama/issues/13146,https://api.github.com/repos/ollama/ollama,https://api.github.com/repos/ollama/ollama/issues/13146/labels{/name},https://api.github.com/repos/ollama/ollama/issues/13146/comments,https://api.github.com/repos/ollama/ollama/issues/13146/events,https://github.com/ollama/ollama/issues/13146,3640872019,I_kwDOJ0Z1Ps7ZA0xT,13146,The qwen2.5-vl:30b model encounters an error when running under Vulkan.,"{'login': 'adamyang1980', 'id': 211759365, 'node_id': 'U_kgDODJ8xBQ', 'avatar_url': 'https://avatars.githubusercontent.com/u/211759365?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/adamyang1980', 'html_url': 'https://github.com/adamyang1980', 'followers_url': 'https://api.github.com/users/adamyang1980/followers', 'following_url': 'https://api.github.com/users/adamyang1980/following{/other_user}', 'gists_url': 'https://api.github.com/users/adamyang1980/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/adamyang1980/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/adamyang1980/subscriptions', 'organizations_url': 'https://api.github.com/users/adamyang1980/orgs', 'repos_url': 'https://api.github.com/users/adamyang1980/repos', 'events_url': 'https://api.github.com/users/adamyang1980/events{/privacy}', 'received_events_url': 'https://api.github.com/users/adamyang1980/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}","[{'id': 5667396184, 'node_id': 'LA_kwDOJ0Z1Ps8AAAABUc2aWA', 'url': 'https://api.github.com/repos/ollama/ollama/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': ""Something isn't working""}, {'id': 9653160222, 'node_id': 'LA_kwDOJ0Z1Ps8AAAACP1-JHg', 'url': 'https://api.github.com/repos/ollama/ollama/labels/vulkan', 'name': 'vulkan', 'color': 'A41E22', 'default': False, 'description': ''}]",open,False,"{'login': 'dhiltgen', 'id': 4033016, 'node_id': 'MDQ6VXNlcjQwMzMwMTY=', 'avatar_url': 'https://avatars.githubusercontent.com/u/4033016?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/dhiltgen', 'html_url': 'https://github.com/dhiltgen', 'followers_url': 'https://api.github.com/users/dhiltgen/followers', 'following_url': 'https://api.github.com/users/dhiltgen/following{/other_user}', 'gists_url': 'https://api.github.com/users/dhiltgen/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/dhiltgen/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/dhiltgen/subscriptions', 'organizations_url': 'https://api.github.com/users/dhiltgen/orgs', 'repos_url': 'https://api.github.com/users/dhiltgen/repos', 'events_url': 'https://api.github.com/users/dhiltgen/events{/privacy}', 'received_events_url': 'https://api.github.com/users/dhiltgen/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}","[{'login': 'dhiltgen', 'id': 4033016, 'node_id': 'MDQ6VXNlcjQwMzMwMTY=', 'avatar_url': 'https://avatars.githubusercontent.com/u/4033016?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/dhiltgen', 'html_url': 'https://github.com/dhiltgen', 'followers_url': 'https://api.github.com/users/dhiltgen/followers', 'following_url': 'https://api.github.com/users/dhiltgen/following{/other_user}', 'gists_url': 'https://api.github.com/users/dhiltgen/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/dhiltgen/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/dhiltgen/subscriptions', 'organizations_url': 'https://api.github.com/users/dhiltgen/orgs', 'repos_url': 'https://api.github.com/users/dhiltgen/repos', 'events_url': 'https://api.github.com/users/dhiltgen/events{/privacy}', 'received_events_url': 'https://api.github.com/users/dhiltgen/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}]",,0,2025-11-19T03:12:31Z,2025-11-21T23:42:27Z,,NONE,,,"{'total': 0, 'completed': 0, 'percent_completed': 0}","{'blocked_by': 0, 'total_blocked_by': 0, 'blocking': 0, 'total_blocking': 0}","### What is the issue?

After setting OLLAMA_VULKAN=1 in the ollama 0.12.11 version and running the qwen2.5-vl:30b model, when the client uploads an image for OCR recognition inference, an error occurs: SIGSEGV in ggml_backend_sched_graph_compute_async. But qwen3vl series models can process it normally.

### Relevant log output

```shell
time=2025-11-19T02:03:38.473Z level=DEBUG source=runner.go:246 msg=""refreshing free memory""
ggml_backend_vk_get_device_memory called: uuid 00000000-c500-0000-0000-000000000000
ggml_backend_vk_get_device_memory called: luid 0x0000000000000000
ggml_backend_vk_get_device_memory called: uuid 00000000-c500-0000-0000-000000000000
ggml_backend_vk_get_device_memory called: luid 0x0000000000000000
time=2025-11-19T02:03:38.485Z level=DEBUG source=runner.go:1378 msg=""gathering device infos took"" duration=667.32µs
time=2025-11-19T02:03:38.485Z level=DEBUG source=runner.go:294 msg=""existing runner discovery took"" duration=12.177197ms
time=2025-11-19T02:03:38.485Z level=DEBUG source=runner.go:40 msg=""overall device VRAM discovery took"" duration=12.256865ms
time=2025-11-19T02:03:38.499Z level=DEBUG source=ggml.go:276 msg=""key with type not found"" key=general.alignment default=32
time=2025-11-19T02:03:38.500Z level=DEBUG source=sched.go:572 msg=""gpu reported"" gpu=00000000-c500-0000-0000-000000000000 library=Vulkan available=""128.8 GiB""
time=2025-11-19T02:03:38.500Z level=INFO source=sched.go:583 msg=""updated VRAM based on existing loaded models"" gpu=00000000-c500-0000-0000-000000000000 library=Vulkan total=""129.0 GiB"" available=""104.0 GiB""
time=2025-11-19T02:03:38.544Z level=DEBUG source=ggml.go:276 msg=""key with type not found"" key=general.alignment default=32
time=2025-11-19T02:03:38.545Z level=DEBUG source=ggml.go:276 msg=""key with type not found"" key=qwen25vl.pooling_type default=0
time=2025-11-19T02:03:38.545Z level=DEBUG source=ggml.go:276 msg=""key with type not found"" key=tokenizer.ggml.add_bos_token default=true
time=2025-11-19T02:03:38.545Z level=DEBUG source=ggml.go:276 msg=""key with type not found"" key=tokenizer.ggml.bos_token_id default=0
time=2025-11-19T02:03:38.545Z level=DEBUG source=ggml.go:276 msg=""key with type not found"" key=tokenizer.ggml.eos_token_ids default=""&{size:0 values:[]}""
time=2025-11-19T02:03:38.545Z level=DEBUG source=ggml.go:276 msg=""key with type not found"" key=qwen25vl.rope.dimension_count default=128
time=2025-11-19T02:03:38.545Z level=DEBUG source=ggml.go:276 msg=""key with type not found"" key=qwen25vl.rope.scaling.factor default=1
time=2025-11-19T02:03:38.545Z level=DEBUG source=ggml.go:276 msg=""key with type not found"" key=qwen25vl.qwen25vl.vision.fullatt_block_indexes default=""&{size:0 values:[7 15 23 31]}""
time=2025-11-19T02:03:38.545Z level=DEBUG source=ggml.go:276 msg=""key with type not found"" key=qwen25vl.vision.max_pixels default=1003520
time=2025-11-19T02:03:38.545Z level=DEBUG source=ggml.go:276 msg=""key with type not found"" key=qwen25vl.attention.key_length default=128
time=2025-11-19T02:03:38.545Z level=DEBUG source=ggml.go:276 msg=""key with type not found"" key=qwen25vl.attention.value_length default=128
time=2025-11-19T02:03:38.545Z level=INFO source=server.go:209 msg=""enabling flash attention""
time=2025-11-19T02:03:38.546Z level=INFO source=server.go:392 msg=""starting runner"" cmd=""/usr/bin/ollama runner --ollama-engine --model /root/.ollama/models/blobs/sha256-043a363c6ca35e3b1a29b8a5b0bbd28474820239bbc5ad943c9be18f0dc77b66 --port 37865""
time=2025-11-19T02:03:38.546Z level=DEBUG source=server.go:393 msg=subprocess PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin OLLAMA_FLASH_ATTENTION=true OLLAMA_DEBUG=1 OLLAMA_MAX_LOADED_MODELS=4 OLLAMA_KEEP_ALIVE=1h HSA_ENABLE_SDMA=0 OLLAMA_HOST=0.0.0.0:11434 OLLAMA_NUM_PARALLEL=2 OLLAMA_CONTEXT_LENGTH=65536 HSA_OVERRIDE_GFX_VERSION=11.5.1 OLLAMA_VULKAN=1 LD_LIBRARY_PATH=/usr/lib/ollama:/usr/lib/ollama/vulkan:/usr/local/nvidia/lib:/usr/local/nvidia/lib64 OLLAMA_LIBRARY_PATH=/usr/lib/ollama:/usr/lib/ollama/vulkan
time=2025-11-19T02:03:38.546Z level=INFO source=sched.go:443 msg=""system memory"" total=""124.5 GiB"" free=""92.0 GiB"" free_swap=""8.0 GiB""
time=2025-11-19T02:03:38.546Z level=INFO source=sched.go:450 msg=""gpu memory"" id=00000000-c500-0000-0000-000000000000 library=Vulkan available=""103.6 GiB"" free=""104.0 GiB"" minimum=""457.0 MiB"" overhead=""0 B""
time=2025-11-19T02:03:38.546Z level=INFO source=server.go:702 msg=""loading model"" ""model layers""=65 requested=-1
time=2025-11-19T02:03:38.555Z level=INFO source=runner.go:1398 msg=""starting ollama engine""
time=2025-11-19T02:03:38.555Z level=INFO source=runner.go:1433 msg=""Server listening on 127.0.0.1:37865""
time=2025-11-19T02:03:38.558Z level=INFO source=runner.go:1271 msg=load request=""{Operation:fit LoraPath:[] Parallel:2 BatchSize:512 FlashAttention:true KvSize:131072 KvCacheType: NumThreads:16 GPULayers:65[ID:00000000-c500-0000-0000-000000000000 Layers:65(0..64)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}""
time=2025-11-19T02:03:38.581Z level=DEBUG source=ggml.go:276 msg=""key with type not found"" key=general.alignment default=32
time=2025-11-19T02:03:38.582Z level=DEBUG source=ggml.go:276 msg=""key with type not found"" key=general.name default=""""
time=2025-11-19T02:03:38.582Z level=DEBUG source=ggml.go:276 msg=""key with type not found"" key=general.description default=""""
time=2025-11-19T02:03:38.582Z level=INFO source=ggml.go:136 msg="""" architecture=qwen25vl file_type=Q4_K_M name="""" description="""" num_tensors=1290 num_key_values=36
time=2025-11-19T02:03:38.582Z level=DEBUG source=ggml.go:94 msg=""ggml backend load all from path"" path=/usr/lib/ollama
load_backend: loaded CPU backend from /usr/lib/ollama/libggml-cpu-icelake.so
time=2025-11-19T02:03:38.586Z level=DEBUG source=ggml.go:94 msg=""ggml backend load all from path"" path=/usr/lib/ollama/vulkan
ggml_vulkan: Found 1 Vulkan devices:
ggml_vulkan: 0 = AMD Radeon Graphics (RADV GFX1151) (radv) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 64 | shared memory: 65536 | int dot: 1 | matrix cores: KHR_coopmat
load_backend: loaded Vulkan backend from /usr/lib/ollama/vulkan/libggml-vulkan.so
time=2025-11-19T02:03:38.610Z level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 compiler=cgo(gcc)
ggml_backend_vk_get_device_memory called: uuid 00000000-c500-0000-0000-000000000000
ggml_backend_vk_get_device_memory called: luid 0x0000000000000000
time=2025-11-19T02:03:38.617Z level=DEBUG source=ggml.go:276 msg=""key with type not found"" key=qwen25vl.pooling_type default=0
time=2025-11-19T02:03:38.617Z level=DEBUG source=ggml.go:276 msg=""key with type not found"" key=tokenizer.ggml.add_bos_token default=true
time=2025-11-19T02:03:38.617Z level=DEBUG source=ggml.go:276 msg=""key with type not found"" key=tokenizer.ggml.bos_token_id default=0
time=2025-11-19T02:03:38.617Z level=DEBUG source=ggml.go:276 msg=""key with type not found"" key=tokenizer.ggml.eos_token_ids default=""&{size:0 values:[]}""
time=2025-11-19T02:03:38.617Z level=DEBUG source=ggml.go:276 msg=""key with type not found"" key=qwen25vl.rope.dimension_count default=128
time=2025-11-19T02:03:38.617Z level=DEBUG source=ggml.go:276 msg=""key with type not found"" key=qwen25vl.rope.scaling.factor default=1
time=2025-11-19T02:03:38.617Z level=DEBUG source=ggml.go:276 msg=""key with type not found"" key=qwen25vl.qwen25vl.vision.fullatt_block_indexes default=""&{size:0 values:[7 15 23 31]}""
time=2025-11-19T02:03:38.617Z level=DEBUG source=ggml.go:276 msg=""key with type not found"" key=qwen25vl.vision.max_pixels default=1003520
time=2025-11-19T02:03:38.769Z level=DEBUG source=vocabulary.go:52 msg=""adding bos token to prompt"" id=0
time=2025-11-19T02:03:38.770Z level=DEBUG source=ggml.go:853 msg=""compute graph"" nodes=1716 splits=133
time=2025-11-19T02:03:39.032Z level=DEBUG source=ggml.go:853 msg=""compute graph"" nodes=2122 splits=2
time=2025-11-19T02:03:39.036Z level=DEBUG source=ggml.go:853 msg=""compute graph"" nodes=2120 splits=2
time=2025-11-19T02:03:39.037Z level=DEBUG source=device.go:240 msg=""model weights"" device=Vulkan0 size=""19.3 GiB""
time=2025-11-19T02:03:39.037Z level=DEBUG source=device.go:245 msg=""model weights"" device=CPU size=""417.7 MiB""
time=2025-11-19T02:03:39.037Z level=DEBUG source=device.go:251 msg=""kv cache"" device=Vulkan0 size=""32.0 GiB""
time=2025-11-19T02:03:39.037Z level=DEBUG source=device.go:262 msg=""compute graph"" device=Vulkan0 size=""1.1 GiB""
time=2025-11-19T02:03:39.037Z level=DEBUG source=device.go:267 msg=""compute graph"" device=CPU size=""23.9 MiB""
time=2025-11-19T02:03:39.037Z level=DEBUG source=device.go:272 msg=""total memory"" size=""52.8 GiB""
time=2025-11-19T02:03:39.037Z level=DEBUG source=server.go:727 msg=memory success=true required.InputWeights=437944320 required.CPU.Graph=25108480 required.Vulkan0.ID=00000000-c500-0000-0000-000000000000 required.Vulkan0.Weights=""[312184832 312184832 312184832 312184832 312184832 312184832 312184832 312184832 275689472 275689472 312184832 275689472 274337792 310833152 275689472 274337792 310833152 275689472 274337792 310833152 275689472 274337792 310833152 275689472 274337792 310833152 275689472 274337792 310833152 275689472 274337792 310833152 275689472 274337792 310833152 275689472 274337792 310833152 275689472 274337792 310833152 275689472 274337792 310833152 275689472 274337792 310833152 275689472 274337792 310833152 275689472 274337792 310833152 275689472 274337792 310833152 312184832 310833152 310833152 312184832 310833152 310833152 312184832 310833152 1950071296]"" required.Vulkan0.Cache=""[536870912 536870912 536870912 536870912 536870912 536870912 536870912 536870912 536870912 536870912 536870912 536870912 536870912 536870912 536870912 536870912 536870912 536870912 536870912 536870912 536870912 536870912 536870912 536870912 536870912 536870912 536870912 536870912 536870912 536870912 536870912 536870912 536870912 536870912 536870912 536870912 536870912 536870912 536870912 536870912 536870912 536870912 536870912 536870912 536870912 536870912 536870912 536870912 536870912 536870912 536870912 536870912 536870912 536870912 536870912 536870912 536870912 536870912 536870912 536870912 536870912 536870912 536870912 536870912 0]"" required.Vulkan0.Graph=1132159792
time=2025-11-19T02:03:39.037Z level=DEBUG source=server.go:921 msg=""available gpu"" id=00000000-c500-0000-0000-000000000000 library=Vulkan ""available layer vram""=""102.5 GiB"" backoff=0.00 minimum=""457.0 MiB"" overhead=""0 B"" graph=""1.1 GiB""
time=2025-11-19T02:03:39.037Z level=DEBUG source=server.go:738 msg=""new layout created"" layers=""65[ID:00000000-c500-0000-0000-000000000000 Layers:65(0..64)]""
time=2025-11-19T02:03:39.037Z level=INFO source=runner.go:1271 msg=load request=""{Operation:alloc LoraPath:[] Parallel:2 BatchSize:512 FlashAttention:true KvSize:131072 KvCacheType: NumThreads:16 GPULayers:65[ID:00000000-c500-0000-0000-000000000000 Layers:65(0..64)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}""
time=2025-11-19T02:03:39.051Z level=DEBUG source=ggml.go:276 msg=""key with type not found"" key=general.alignment default=32
ggml_backend_vk_get_device_memory called: uuid 00000000-c500-0000-0000-000000000000
ggml_backend_vk_get_device_memory called: luid 0x0000000000000000
time=2025-11-19T02:03:39.907Z level=DEBUG source=ggml.go:276 msg=""key with type not found"" key=qwen25vl.pooling_type default=0
time=2025-11-19T02:03:39.907Z level=DEBUG source=ggml.go:276 msg=""key with type not found"" key=tokenizer.ggml.add_bos_token default=true
time=2025-11-19T02:03:39.907Z level=DEBUG source=ggml.go:276 msg=""key with type not found"" key=tokenizer.ggml.bos_token_id default=0
time=2025-11-19T02:03:39.907Z level=DEBUG source=ggml.go:276 msg=""key with type not found"" key=tokenizer.ggml.eos_token_ids default=""&{size:0 values:[]}""
time=2025-11-19T02:03:39.907Z level=DEBUG source=ggml.go:276 msg=""key with type not found"" key=qwen25vl.rope.dimension_count default=128
time=2025-11-19T02:03:39.907Z level=DEBUG source=ggml.go:276 msg=""key with type not found"" key=qwen25vl.rope.scaling.factor default=1
time=2025-11-19T02:03:39.907Z level=DEBUG source=ggml.go:276 msg=""key with type not found"" key=qwen25vl.qwen25vl.vision.fullatt_block_indexes default=""&{size:0 values:[7 15 23 31]}""
time=2025-11-19T02:03:39.907Z level=DEBUG source=ggml.go:276 msg=""key with type not found"" key=qwen25vl.vision.max_pixels default=1003520
time=2025-11-19T02:03:40.060Z level=DEBUG source=vocabulary.go:52 msg=""adding bos token to prompt"" id=0
time=2025-11-19T02:03:40.122Z level=DEBUG source=ggml.go:853 msg=""compute graph"" nodes=1716 splits=133
time=2025-11-19T02:03:42.613Z level=DEBUG source=ggml.go:853 msg=""compute graph"" nodes=2122 splits=2
time=2025-11-19T02:03:42.639Z level=DEBUG source=ggml.go:853 msg=""compute graph"" nodes=2120 splits=2
time=2025-11-19T02:03:42.641Z level=DEBUG source=device.go:240 msg=""model weights"" device=Vulkan0 size=""19.3 GiB""
time=2025-11-19T02:03:42.641Z level=DEBUG source=device.go:245 msg=""model weights"" device=CPU size=""417.7 MiB""
time=2025-11-19T02:03:42.641Z level=DEBUG source=device.go:251 msg=""kv cache"" device=Vulkan0 size=""32.0 GiB""
time=2025-11-19T02:03:42.641Z level=DEBUG source=device.go:262 msg=""compute graph"" device=Vulkan0 size=""1.1 GiB""
time=2025-11-19T02:03:42.641Z level=DEBUG source=device.go:267 msg=""compute graph"" device=CPU size=""23.9 MiB""
time=2025-11-19T02:03:42.641Z level=DEBUG source=device.go:272 msg=""total memory"" size=""52.8 GiB""
time=2025-11-19T02:03:42.641Z level=DEBUG source=server.go:727 msg=memory success=true required.InputWeights=437944320 required.CPU.Graph=25108480 required.Vulkan0.ID=00000000-c500-0000-0000-000000000000 required.Vulkan0.Weights=""[312184832 312184832 312184832 312184832 312184832 312184832 312184832 312184832 275689472 275689472 312184832 275689472 274337792 310833152 275689472 274337792 310833152 275689472 274337792 310833152 275689472 274337792 310833152 275689472 274337792 310833152 275689472 274337792 310833152 275689472 274337792 310833152 275689472 274337792 310833152 275689472 274337792 310833152 275689472 274337792 310833152 275689472 274337792 310833152 275689472 274337792 310833152 275689472 274337792 310833152 275689472 274337792 310833152 275689472 274337792 310833152 312184832 310833152 310833152 312184832 310833152 310833152 312184832 310833152 1950071296]"" required.Vulkan0.Cache=""[536870912 536870912 536870912 536870912 536870912 536870912 536870912 536870912 536870912 536870912 536870912 536870912 536870912 536870912 536870912 536870912 536870912 536870912 536870912 536870912 536870912 536870912 536870912 536870912 536870912 536870912 536870912 536870912 536870912 536870912 536870912 536870912 536870912 536870912 536870912 536870912 536870912 536870912 536870912 536870912 536870912 536870912 536870912 536870912 536870912 536870912 536870912 536870912 536870912 536870912 536870912 536870912 536870912 536870912 536870912 536870912 536870912 536870912 536870912 536870912 536870912 536870912 536870912 536870912 0]"" required.Vulkan0.Graph=1132159792
time=2025-11-19T02:03:42.641Z level=DEBUG source=server.go:921 msg=""available gpu"" id=00000000-c500-0000-0000-000000000000 library=Vulkan ""available layer vram""=""102.5 GiB"" backoff=0.00 minimum=""457.0 MiB"" overhead=""0 B"" graph=""1.1 GiB""
time=2025-11-19T02:03:42.641Z level=DEBUG source=server.go:738 msg=""new layout created"" layers=""65[ID:00000000-c500-0000-0000-000000000000 Layers:65(0..64)]""
time=2025-11-19T02:03:42.642Z level=INFO source=runner.go:1271 msg=load request=""{Operation:commit LoraPath:[] Parallel:2 BatchSize:512 FlashAttention:true KvSize:131072 KvCacheType: NumThreads:16 GPULayers:65[ID:00000000-c500-0000-0000-000000000000 Layers:65(0..64)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}""
time=2025-11-19T02:03:42.642Z level=INFO source=ggml.go:482 msg=""offloading 64 repeating layers to GPU""
time=2025-11-19T02:03:42.642Z level=INFO source=ggml.go:489 msg=""offloading output layer to GPU""
time=2025-11-19T02:03:42.642Z level=INFO source=ggml.go:494 msg=""offloaded 65/65 layers to GPU""
time=2025-11-19T02:03:42.642Z level=INFO source=device.go:240 msg=""model weights"" device=Vulkan0 size=""19.3 GiB""
time=2025-11-19T02:03:42.642Z level=INFO source=device.go:245 msg=""model weights"" device=CPU size=""417.7 MiB""
time=2025-11-19T02:03:42.642Z level=INFO source=device.go:251 msg=""kv cache"" device=Vulkan0 size=""32.0 GiB""
time=2025-11-19T02:03:42.642Z level=INFO source=device.go:262 msg=""compute graph"" device=Vulkan0 size=""1.1 GiB""
time=2025-11-19T02:03:42.642Z level=INFO source=device.go:267 msg=""compute graph"" device=CPU size=""23.9 MiB""
time=2025-11-19T02:03:42.642Z level=INFO source=device.go:272 msg=""total memory"" size=""52.8 GiB""
time=2025-11-19T02:03:42.642Z level=INFO source=sched.go:517 msg=""loaded runners"" count=2
time=2025-11-19T02:03:42.642Z level=DEBUG source=sched.go:221 msg=""new model fits with existing models, loading""
time=2025-11-19T02:03:42.642Z level=INFO source=server.go:1294 msg=""waiting for llama runner to start responding""
time=2025-11-19T02:03:42.642Z level=INFO source=server.go:1328 msg=""waiting for server to become available"" status=""llm server loading model""
time=2025-11-19T02:03:42.893Z level=DEBUG source=server.go:1338 msg=""model load progress 0.03""
time=2025-11-19T02:03:43.144Z level=DEBUG source=server.go:1338 msg=""model load progress 0.07""
time=2025-11-19T02:03:43.394Z level=DEBUG source=server.go:1338 msg=""model load progress 0.11""
time=2025-11-19T02:03:43.645Z level=DEBUG source=server.go:1338 msg=""model load progress 0.15""
time=2025-11-19T02:03:43.896Z level=DEBUG source=server.go:1338 msg=""model load progress 0.19""
time=2025-11-19T02:03:44.147Z level=DEBUG source=server.go:1338 msg=""model load progress 0.23""
time=2025-11-19T02:03:44.397Z level=DEBUG source=server.go:1338 msg=""model load progress 0.27""
time=2025-11-19T02:03:44.648Z level=DEBUG source=server.go:1338 msg=""model load progress 0.31""
time=2025-11-19T02:03:44.899Z level=DEBUG source=server.go:1338 msg=""model load progress 0.35""
time=2025-11-19T02:03:45.149Z level=DEBUG source=server.go:1338 msg=""model load progress 0.39""
time=2025-11-19T02:03:45.400Z level=DEBUG source=server.go:1338 msg=""model load progress 0.43""
time=2025-11-19T02:03:45.651Z level=DEBUG source=server.go:1338 msg=""model load progress 0.47""
time=2025-11-19T02:03:45.901Z level=DEBUG source=server.go:1338 msg=""model load progress 0.51""
time=2025-11-19T02:03:46.152Z level=DEBUG source=server.go:1338 msg=""model load progress 0.55""
time=2025-11-19T02:03:46.403Z level=DEBUG source=server.go:1338 msg=""model load progress 0.59""
time=2025-11-19T02:03:46.653Z level=DEBUG source=server.go:1338 msg=""model load progress 0.63""
time=2025-11-19T02:03:46.904Z level=DEBUG source=server.go:1338 msg=""model load progress 0.67""
time=2025-11-19T02:03:47.155Z level=DEBUG source=server.go:1338 msg=""model load progress 0.71""
time=2025-11-19T02:03:47.405Z level=DEBUG source=server.go:1338 msg=""model load progress 0.75""
time=2025-11-19T02:03:47.656Z level=DEBUG source=server.go:1338 msg=""model load progress 0.79""
time=2025-11-19T02:03:47.907Z level=DEBUG source=server.go:1338 msg=""model load progress 0.83""
time=2025-11-19T02:03:48.157Z level=DEBUG source=server.go:1338 msg=""model load progress 0.87""
time=2025-11-19T02:03:48.408Z level=DEBUG source=server.go:1338 msg=""model load progress 0.91""
time=2025-11-19T02:03:48.659Z level=DEBUG source=server.go:1338 msg=""model load progress 0.95""
time=2025-11-19T02:03:48.910Z level=DEBUG source=server.go:1338 msg=""model load progress 0.98""
time=2025-11-19T02:03:49.161Z level=DEBUG source=server.go:1338 msg=""model load progress 0.99""
time=2025-11-19T02:03:49.283Z level=DEBUG source=ggml.go:276 msg=""key with type not found"" key=qwen25vl.pooling_type default=0
time=2025-11-19T02:03:49.412Z level=INFO source=server.go:1332 msg=""llama runner started in 10.87 seconds""
time=2025-11-19T02:03:49.412Z level=DEBUG source=sched.go:529 msg=""finished setting up"" runner.name=registry.ollama.ai/library/qwen2.5vl:32b runner.inference=""[{ID:00000000-c500-0000-0000-000000000000 Library:Vulkan}]"" runner.size=""52.8 GiB"" runner.vram=""52.8 GiB"" runner.parallel=2 runner.pid=216 runner.model=/root/.ollama/models/blobs/sha256-043a363c6ca35e3b1a29b8a5b0bbd28474820239bbc5ad943c9be18f0dc77b66 runner.num_ctx=65536
time=2025-11-19T02:03:49.424Z level=DEBUG source=server.go:1465 msg=""completion request"" images=2 prompt=445 format=""""
time=2025-11-19T02:03:49.425Z level=DEBUG source=vocabulary.go:52 msg=""adding bos token to prompt"" id=0
time=2025-11-19T02:03:49.431Z level=DEBUG source=vocabulary.go:52 msg=""adding bos token to prompt"" id=0
time=2025-11-19T02:03:49.431Z level=DEBUG source=vocabulary.go:52 msg=""adding bos token to prompt"" id=0
time=2025-11-19T02:03:49.431Z level=DEBUG source=cache.go:142 msg=""loading cache slot"" id=0 cache=0 prompt=135 used=0 remaining=135
SIGSEGV: segmentation violation
PC=0x723155313e3c m=30 sigcode=1 addr=0x52
signal arrived during cgo execution

goroutine 13 gp=0xc000702fc0 m=30 mp=0xc000f88008 [syscall]:
runtime.cgocall(0x5a31c524e760, 0xc000de1568)
	runtime/cgocall.go:167 +0x4b fp=0xc000de1540 sp=0xc000de1508 pc=0x5a31c453f62b
github.com/ollama/ollama/ml/backend/ggml._Cfunc_ggml_backend_sched_graph_compute_async(0x723178100860, 0x7230d4cbdd60)
	_cgo_gotypes.go:961 +0x4a fp=0xc000de1568 sp=0xc000de1540 pc=0x5a31c497482a
github.com/ollama/ollama/ml/backend/ggml.(*Context).ComputeWithNotify.func2(...)
	github.com/ollama/ollama/ml/backend/ggml/ggml.go:826
github.com/ollama/ollama/ml/backend/ggml.(*Context).ComputeWithNotify(0xc0011a12c0, 0x5a31c45450a9?, {0xc000044c10, 0x1, 0xc000044c01?})
	github.com/ollama/ollama/ml/backend/ggml/ggml.go:826 +0x1b2 fp=0xc000de1640 sp=0xc000de1568 pc=0x5a31c4982232
github.com/ollama/ollama/ml/backend/ggml.(*Context).Compute(0xc0011a12c0?, {0xc000044c10?, 0x1?, 0x5a31c4520c89?})
	github.com/ollama/ollama/ml/backend/ggml/ggml.go:812 +0x25 fp=0xc000de1678 sp=0xc000de1640 pc=0x5a31c4982045
github.com/ollama/ollama/runner/ollamarunner.multimodalStore.getTensor(0x5a31c5890be0?, {0x5a31c5a5ee58, 0xc000151760}, {0x5a31c5a638b0, 0xc0011a1200}, {0x5a31c5a6dd20, 0xc0041b7e60}, 0x0)
	github.com/ollama/ollama/runner/ollamarunner/multimodal.go:93 +0x2f4 fp=0xc000de1788 sp=0xc000de1678 pc=0x5a31c4a49e94
github.com/ollama/ollama/runner/ollamarunner.multimodalStore.getMultimodal(0xc00119f200, {0x5a31c5a5ee58, 0xc000151760}, {0x5a31c5a638b0, 0xc0011a1200}, {0xc0003ca3c0, 0x1, 0xfffffffffffffffc?}, 0x0)
	github.com/ollama/ollama/runner/ollamarunner/multimodal.go:56 +0xe5 fp=0xc000de17f0 sp=0xc000de1788 pc=0x5a31c4a49a85
github.com/ollama/ollama/runner/ollamarunner.(*Server).forwardBatch(_, {0x0, {0x0, 0x0}, {0x0, 0x0}, {0x0, 0x0, 0x0}, {{0x0, ...}, ...}, ...})
	github.com/ollama/ollama/runner/ollamarunner/runner.go:584 +0x1217 fp=0xc000de1b58 sp=0xc000de17f0 pc=0x5a31c4a4d0f7
github.com/ollama/ollama/runner/ollamarunner.(*Server).run(0xc0002230e0, {0x5a31c5a58f00, 0xc00051f770})
	github.com/ollama/ollama/runner/ollamarunner/runner.go:452 +0x18c fp=0xc000de1fb8 sp=0xc000de1b58 pc=0x5a31c4a4bc8c
github.com/ollama/ollama/runner/ollamarunner.Execute.gowrap1()
	github.com/ollama/ollama/runner/ollamarunner/runner.go:1411 +0x28 fp=0xc000de1fe0 sp=0xc000de1fb8 pc=0x5a31c4a551e8
runtime.goexit({})
	runtime/asm_amd64.s:1700 +0x1 fp=0xc000de1fe8 sp=0xc000de1fe0 pc=0x5a31c454a941
created by github.com/ollama/ollama/runner/ollamarunner.Execute in goroutine 1
	github.com/ollama/ollama/runner/ollamarunner/runner.go:1411 +0x4c9

goroutine 1 gp=0xc000002380 m=nil [IO wait]:
runtime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)
	runtime/proc.go:435 +0xce fp=0xc000ddf790 sp=0xc000ddf770 pc=0x5a31c4542aae
runtime.netpollblock(0xc0004ad7e0?, 0xc44dc1e6?, 0x31?)
	runtime/netpoll.go:575 +0xf7 fp=0xc000ddf7c8 sp=0xc000ddf790 pc=0x5a31c4507dd7
internal/poll.runtime_pollWait(0x7231e5a1ade0, 0x72)
	runtime/netpoll.go:351 +0x85 fp=0xc000ddf7e8 sp=0xc000ddf7c8 pc=0x5a31c4541cc5
internal/poll.(*pollDesc).wait(0xc0006a2100?, 0x900000036?, 0x0)
	internal/poll/fd_poll_runtime.go:84 +0x27 fp=0xc000ddf810 sp=0xc000ddf7e8 pc=0x5a31c45c9c07
internal/poll.(*pollDesc).waitRead(...)
	internal/poll/fd_poll_runtime.go:89
internal/poll.(*FD).Accept(0xc0006a2100)
	internal/poll/fd_unix.go:620 +0x295 fp=0xc000ddf8b8 sp=0xc000ddf810 pc=0x5a31c45cefd5
net.(*netFD).accept(0xc0006a2100)
	net/fd_unix.go:172 +0x29 fp=0xc000ddf970 sp=0xc000ddf8b8 pc=0x5a31c4641ea9
net.(*TCPListener).accept(0xc000519800)
	net/tcpsock_posix.go:159 +0x1b fp=0xc000ddf9c0 sp=0xc000ddf970 pc=0x5a31c465785b
net.(*TCPListener).Accept(0xc000519800)
	net/tcpsock.go:380 +0x30 fp=0xc000ddf9f0 sp=0xc000ddf9c0 pc=0x5a31c4656710
net/http.(*onceCloseListener).Accept(0xc0002e43f0?)
	<autogenerated>:1 +0x24 fp=0xc000ddfa08 sp=0xc000ddf9f0 pc=0x5a31c486dee4
net/http.(*Server).Serve(0xc000051100, {0x5a31c5a56928, 0xc000519800})
	net/http/server.go:3424 +0x30c fp=0xc000ddfb38 sp=0xc000ddfa08 pc=0x5a31c48457ac
github.com/ollama/ollama/runner/ollamarunner.Execute({0xc00012a030, 0x4, 0x4})
	github.com/ollama/ollama/runner/ollamarunner/runner.go:1434 +0x94e fp=0xc000ddfd08 sp=0xc000ddfb38 pc=0x5a31c4a54f6e
github.com/ollama/ollama/runner.Execute({0xc00012a010?, 0x0?, 0x0?})
	github.com/ollama/ollama/runner/runner.go:20 +0xc9 fp=0xc000ddfd30 sp=0xc000ddfd08 pc=0x5a31c4a55869
github.com/ollama/ollama/cmd.NewCLI.func2(0xc000050f00?, {0x5a31c555e0ab?, 0x4?, 0x5a31c555e0af?})
	github.com/ollama/ollama/cmd/cmd.go:1841 +0x45 fp=0xc000ddfd58 sp=0xc000ddfd30 pc=0x5a31c51e05c5
github.com/spf13/cobra.(*Command).execute(0xc0002e9508, {0xc00051f6d0, 0x5, 0x5})
	github.com/spf13/cobra@v1.7.0/command.go:940 +0x85c fp=0xc000ddfe78 sp=0xc000ddfd58 pc=0x5a31c46bb4fc
github.com/spf13/cobra.(*Command).ExecuteC(0xc0000f2908)
	github.com/spf13/cobra@v1.7.0/command.go:1068 +0x3a5 fp=0xc000ddff30 sp=0xc000ddfe78 pc=0x5a31c46bbd45
github.com/spf13/cobra.(*Command).Execute(...)
	github.com/spf13/cobra@v1.7.0/command.go:992
github.com/spf13/cobra.(*Command).ExecuteContext(...)
	github.com/spf13/cobra@v1.7.0/command.go:985
main.main()
	github.com/ollama/ollama/main.go:12 +0x4d fp=0xc000ddff50 sp=0xc000ddff30 pc=0x5a31c51e10ad
runtime.main()
	runtime/proc.go:283 +0x29d fp=0xc000ddffe0 sp=0xc000ddff50 pc=0x5a31c450f45d
runtime.goexit({})
	runtime/asm_amd64.s:1700 +0x1 fp=0xc000ddffe8 sp=0xc000ddffe0 pc=0x5a31c454a941

goroutine 2 gp=0xc000002e00 m=nil [force gc (idle)]:
runtime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)
	runtime/proc.go:435 +0xce fp=0xc0000aafa8 sp=0xc0000aaf88 pc=0x5a31c4542aae
runtime.goparkunlock(...)
	runtime/proc.go:441
runtime.forcegchelper()
	runtime/proc.go:348 +0xb8 fp=0xc0000aafe0 sp=0xc0000aafa8 pc=0x5a31c450f798
runtime.goexit({})
	runtime/asm_amd64.s:1700 +0x1 fp=0xc0000aafe8 sp=0xc0000aafe0 pc=0x5a31c454a941
created by runtime.init.7 in goroutine 1
	runtime/proc.go:336 +0x1a

goroutine 3 gp=0xc000003340 m=nil [GC sweep wait]:
runtime.gopark(0x1?, 0x0?, 0x0?, 0x0?, 0x0?)
	runtime/proc.go:435 +0xce fp=0xc0000ab780 sp=0xc0000ab760 pc=0x5a31c4542aae
runtime.goparkunlock(...)
	runtime/proc.go:441
runtime.bgsweep(0xc0000d6000)
	runtime/mgcsweep.go:316 +0xdf fp=0xc0000ab7c8 sp=0xc0000ab780 pc=0x5a31c44f9f3f
runtime.gcenable.gowrap1()
	runtime/mgc.go:204 +0x25 fp=0xc0000ab7e0 sp=0xc0000ab7c8 pc=0x5a31c44ee325
runtime.goexit({})
	runtime/asm_amd64.s:1700 +0x1 fp=0xc0000ab7e8 sp=0xc0000ab7e0 pc=0x5a31c454a941
created by runtime.gcenable in goroutine 1
	runtime/mgc.go:204 +0x66

goroutine 4 gp=0xc000003500 m=nil [GC scavenge wait]:
runtime.gopark(0x10000?, 0x5a31c57268a0?, 0x0?, 0x0?, 0x0?)
	runtime/proc.go:435 +0xce fp=0xc0000abf78 sp=0xc0000abf58 pc=0x5a31c4542aae
runtime.goparkunlock(...)
	runtime/proc.go:441
runtime.(*scavengerState).park(0x5a31c63120a0)
	runtime/mgcscavenge.go:425 +0x49 fp=0xc0000abfa8 sp=0xc0000abf78 pc=0x5a31c44f7989
runtime.bgscavenge(0xc0000d6000)
	runtime/mgcscavenge.go:658 +0x59 fp=0xc0000abfc8 sp=0xc0000abfa8 pc=0x5a31c44f7f19
runtime.gcenable.gowrap2()
	runtime/mgc.go:205 +0x25 fp=0xc0000abfe0 sp=0xc0000abfc8 pc=0x5a31c44ee2c5
runtime.goexit({})
	runtime/asm_amd64.s:1700 +0x1 fp=0xc0000abfe8 sp=0xc0000abfe0 pc=0x5a31c454a941
created by runtime.gcenable in goroutine 1
	runtime/mgc.go:205 +0xa5

goroutine 18 gp=0xc000104380 m=nil [finalizer wait]:
runtime.gopark(0x1b8?, 0xc000002380?, 0x1?, 0x23?, 0xc0000aa688?)
	runtime/proc.go:435 +0xce fp=0xc0000aa630 sp=0xc0000aa610 pc=0x5a31c4542aae
runtime.runfinq()
	runtime/mfinal.go:196 +0x107 fp=0xc0000aa7e0 sp=0xc0000aa630 pc=0x5a31c44ed2e7
runtime.goexit({})
	runtime/asm_amd64.s:1700 +0x1 fp=0xc0000aa7e8 sp=0xc0000aa7e0 pc=0x5a31c454a941
created by runtime.createfing in goroutine 1
	runtime/mfinal.go:166 +0x3d

goroutine 19 gp=0xc000104e00 m=nil [chan receive]:
runtime.gopark(0xc000221860?, 0xc068300018?, 0x60?, 0x67?, 0x5a31c4628ae8?)
	runtime/proc.go:435 +0xce fp=0xc0000a6718 sp=0xc0000a66f8 pc=0x5a31c4542aae
runtime.chanrecv(0xc000100310, 0x0, 0x1)
	runtime/chan.go:664 +0x445 fp=0xc0000a6790 sp=0xc0000a6718 pc=0x5a31c44dedc5
runtime.chanrecv1(0x0?, 0x0?)
	runtime/chan.go:506 +0x12 fp=0xc0000a67b8 sp=0xc0000a6790 pc=0x5a31c44de952
runtime.unique_runtime_registerUniqueMapCleanup.func2(...)
	runtime/mgc.go:1796
runtime.unique_runtime_registerUniqueMapCleanup.gowrap1()
	runtime/mgc.go:1799 +0x2f fp=0xc0000a67e0 sp=0xc0000a67b8 pc=0x5a31c44f14cf
runtime.goexit({})
	runtime/asm_amd64.s:1700 +0x1 fp=0xc0000a67e8 sp=0xc0000a67e0 pc=0x5a31c454a941
created by unique.runtime_registerUniqueMapCleanup in goroutine 1
	runtime/mgc.go:1794 +0x85

goroutine 20 gp=0xc000105180 m=nil [GC worker (idle)]:
runtime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)
	runtime/proc.go:435 +0xce fp=0xc0000a6f38 sp=0xc0000a6f18 pc=0x5a31c4542aae
runtime.gcBgMarkWorker(0xc000101570)
	runtime/mgc.go:1423 +0xe9 fp=0xc0000a6fc8 sp=0xc0000a6f38 pc=0x5a31c44f07e9
runtime.gcBgMarkStartWorkers.gowrap1()
	runtime/mgc.go:1339 +0x25 fp=0xc0000a6fe0 sp=0xc0000a6fc8 pc=0x5a31c44f06c5
runtime.goexit({})
	runtime/asm_amd64.s:1700 +0x1 fp=0xc0000a6fe8 sp=0xc0000a6fe0 pc=0x5a31c454a941
created by runtime.gcBgMarkStartWorkers in goroutine 1
	runtime/mgc.go:1339 +0x105

goroutine 21 gp=0xc000105340 m=nil [GC worker (idle)]:
runtime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)
	runtime/proc.go:435 +0xce fp=0xc0000a7738 sp=0xc0000a7718 pc=0x5a31c4542aae
runtime.gcBgMarkWorker(0xc000101570)
	runtime/mgc.go:1423 +0xe9 fp=0xc0000a77c8 sp=0xc0000a7738 pc=0x5a31c44f07e9
runtime.gcBgMarkStartWorkers.gowrap1()
	runtime/mgc.go:1339 +0x25 fp=0xc0000a77e0 sp=0xc0000a77c8 pc=0x5a31c44f06c5
runtime.goexit({})
	runtime/asm_amd64.s:1700 +0x1 fp=0xc0000a77e8 sp=0xc0000a77e0 pc=0x5a31c454a941
created by runtime.gcBgMarkStartWorkers in goroutine 1
	runtime/mgc.go:1339 +0x105

goroutine 34 gp=0xc0002bc000 m=nil [GC worker (idle)]:
runtime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)
	runtime/proc.go:435 +0xce fp=0xc0002c2738 sp=0xc0002c2718 pc=0x5a31c4542aae
runtime.gcBgMarkWorker(0xc000101570)
	runtime/mgc.go:1423 +0xe9 fp=0xc0002c27c8 sp=0xc0002c2738 pc=0x5a31c44f07e9
runtime.gcBgMarkStartWorkers.gowrap1()
	runtime/mgc.go:1339 +0x25 fp=0xc0002c27e0 sp=0xc0002c27c8 pc=0x5a31c44f06c5
runtime.goexit({})
	runtime/asm_amd64.s:1700 +0x1 fp=0xc0002c27e8 sp=0xc0002c27e0 pc=0x5a31c454a941
created by runtime.gcBgMarkStartWorkers in goroutine 1
	runtime/mgc.go:1339 +0x105

goroutine 35 gp=0xc0002bc1c0 m=nil [GC worker (idle)]:
runtime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)
	runtime/proc.go:435 +0xce fp=0xc0002c2f38 sp=0xc0002c2f18 pc=0x5a31c4542aae
runtime.gcBgMarkWorker(0xc000101570)
	runtime/mgc.go:1423 +0xe9 fp=0xc0002c2fc8 sp=0xc0002c2f38 pc=0x5a31c44f07e9
runtime.gcBgMarkStartWorkers.gowrap1()
	runtime/mgc.go:1339 +0x25 fp=0xc0002c2fe0 sp=0xc0002c2fc8 pc=0x5a31c44f06c5
runtime.goexit({})
	runtime/asm_amd64.s:1700 +0x1 fp=0xc0002c2fe8 sp=0xc0002c2fe0 pc=0x5a31c454a941
created by runtime.gcBgMarkStartWorkers in goroutine 1
	runtime/mgc.go:1339 +0x105

goroutine 36 gp=0xc0002bc380 m=nil [GC worker (idle)]:
runtime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)
	runtime/proc.go:435 +0xce fp=0xc0002c3738 sp=0xc0002c3718 pc=0x5a31c4542aae
runtime.gcBgMarkWorker(0xc000101570)
	runtime/mgc.go:1423 +0xe9 fp=0xc0002c37c8 sp=0xc0002c3738 pc=0x5a31c44f07e9
runtime.gcBgMarkStartWorkers.gowrap1()
	runtime/mgc.go:1339 +0x25 fp=0xc0002c37e0 sp=0xc0002c37c8 pc=0x5a31c44f06c5
runtime.goexit({})
	runtime/asm_amd64.s:1700 +0x1 fp=0xc0002c37e8 sp=0xc0002c37e0 pc=0x5a31c454a941
created by runtime.gcBgMarkStartWorkers in goroutine 1
	runtime/mgc.go:1339 +0x105

goroutine 37 gp=0xc0002bc540 m=nil [GC worker (idle)]:
runtime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)
	runtime/proc.go:435 +0xce fp=0xc0002c3f38 sp=0xc0002c3f18 pc=0x5a31c4542aae
runtime.gcBgMarkWorker(0xc000101570)
	runtime/mgc.go:1423 +0xe9 fp=0xc0002c3fc8 sp=0xc0002c3f38 pc=0x5a31c44f07e9
runtime.gcBgMarkStartWorkers.gowrap1()
	runtime/mgc.go:1339 +0x25 fp=0xc0002c3fe0 sp=0xc0002c3fc8 pc=0x5a31c44f06c5
runtime.goexit({})
	runtime/asm_amd64.s:1700 +0x1 fp=0xc0002c3fe8 sp=0xc0002c3fe0 pc=0x5a31c454a941
created by runtime.gcBgMarkStartWorkers in goroutine 1
	runtime/mgc.go:1339 +0x105

goroutine 38 gp=0xc0002bc700 m=nil [GC worker (idle)]:
runtime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)
	runtime/proc.go:435 +0xce fp=0xc0002c4738 sp=0xc0002c4718 pc=0x5a31c4542aae
runtime.gcBgMarkWorker(0xc000101570)
	runtime/mgc.go:1423 +0xe9 fp=0xc0002c47c8 sp=0xc0002c4738 pc=0x5a31c44f07e9
runtime.gcBgMarkStartWorkers.gowrap1()
	runtime/mgc.go:1339 +0x25 fp=0xc0002c47e0 sp=0xc0002c47c8 pc=0x5a31c44f06c5
runtime.goexit({})
	runtime/asm_amd64.s:1700 +0x1 fp=0xc0002c47e8 sp=0xc0002c47e0 pc=0x5a31c454a941
created by runtime.gcBgMarkStartWorkers in goroutine 1
	runtime/mgc.go:1339 +0x105

goroutine 39 gp=0xc0002bc8c0 m=nil [GC worker (idle)]:
runtime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)
	runtime/proc.go:435 +0xce fp=0xc0002c4f38 sp=0xc0002c4f18 pc=0x5a31c4542aae
runtime.gcBgMarkWorker(0xc000101570)
	runtime/mgc.go:1423 +0xe9 fp=0xc0002c4fc8 sp=0xc0002c4f38 pc=0x5a31c44f07e9
runtime.gcBgMarkStartWorkers.gowrap1()
	runtime/mgc.go:1339 +0x25 fp=0xc0002c4fe0 sp=0xc0002c4fc8 pc=0x5a31c44f06c5
runtime.goexit({})
	runtime/asm_amd64.s:1700 +0x1 fp=0xc0002c4fe8 sp=0xc0002c4fe0 pc=0x5a31c454a941
created by runtime.gcBgMarkStartWorkers in goroutine 1
	runtime/mgc.go:1339 +0x105

goroutine 5 gp=0xc000003dc0 m=nil [GC worker (idle)]:
runtime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)
	runtime/proc.go:435 +0xce fp=0xc0000ac738 sp=0xc0000ac718 pc=0x5a31c4542aae
runtime.gcBgMarkWorker(0xc000101570)
	runtime/mgc.go:1423 +0xe9 fp=0xc0000ac7c8 sp=0xc0000ac738 pc=0x5a31c44f07e9
runtime.gcBgMarkStartWorkers.gowrap1()
	runtime/mgc.go:1339 +0x25 fp=0xc0000ac7e0 sp=0xc0000ac7c8 pc=0x5a31c44f06c5
runtime.goexit({})
	runtime/asm_amd64.s:1700 +0x1 fp=0xc0000ac7e8 sp=0xc0000ac7e0 pc=0x5a31c454a941
created by runtime.gcBgMarkStartWorkers in goroutine 1
	runtime/mgc.go:1339 +0x105

goroutine 6 gp=0xc0000ee000 m=nil [GC worker (idle)]:
runtime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)
	runtime/proc.go:435 +0xce fp=0xc0000acf38 sp=0xc0000acf18 pc=0x5a31c4542aae
runtime.gcBgMarkWorker(0xc000101570)
	runtime/mgc.go:1423 +0xe9 fp=0xc0000acfc8 sp=0xc0000acf38 pc=0x5a31c44f07e9
runtime.gcBgMarkStartWorkers.gowrap1()
	runtime/mgc.go:1339 +0x25 fp=0xc0000acfe0 sp=0xc0000acfc8 pc=0x5a31c44f06c5
runtime.goexit({})
	runtime/asm_amd64.s:1700 +0x1 fp=0xc0000acfe8 sp=0xc0000acfe0 pc=0x5a31c454a941
created by runtime.gcBgMarkStartWorkers in goroutine 1
	runtime/mgc.go:1339 +0x105

goroutine 7 gp=0xc0000ee1c0 m=nil [GC worker (idle)]:
runtime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)
	runtime/proc.go:435 +0xce fp=0xc0000ad738 sp=0xc0000ad718 pc=0x5a31c4542aae
runtime.gcBgMarkWorker(0xc000101570)
	runtime/mgc.go:1423 +0xe9 fp=0xc0000ad7c8 sp=0xc0000ad738 pc=0x5a31c44f07e9
runtime.gcBgMarkStartWorkers.gowrap1()
	runtime/mgc.go:1339 +0x25 fp=0xc0000ad7e0 sp=0xc0000ad7c8 pc=0x5a31c44f06c5
runtime.goexit({})
	runtime/asm_amd64.s:1700 +0x1 fp=0xc0000ad7e8 sp=0xc0000ad7e0 pc=0x5a31c454a941
created by runtime.gcBgMarkStartWorkers in goroutine 1
	runtime/mgc.go:1339 +0x105

goroutine 8 gp=0xc0000ee380 m=nil [GC worker (idle)]:
runtime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)
	runtime/proc.go:435 +0xce fp=0xc0000adf38 sp=0xc0000adf18 pc=0x5a31c4542aae
runtime.gcBgMarkWorker(0xc000101570)
	runtime/mgc.go:1423 +0xe9 fp=0xc0000adfc8 sp=0xc0000adf38 pc=0x5a31c44f07e9
runtime.gcBgMarkStartWorkers.gowrap1()
	runtime/mgc.go:1339 +0x25 fp=0xc0000adfe0 sp=0xc0000adfc8 pc=0x5a31c44f06c5
runtime.goexit({})
	runtime/asm_amd64.s:1700 +0x1 fp=0xc0000adfe8 sp=0xc0000adfe0 pc=0x5a31c454a941
created by runtime.gcBgMarkStartWorkers in goroutine 1
	runtime/mgc.go:1339 +0x105

goroutine 9 gp=0xc0000ee540 m=nil [GC worker (idle)]:
runtime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)
	runtime/proc.go:435 +0xce fp=0xc0002be738 sp=0xc0002be718 pc=0x5a31c4542aae
runtime.gcBgMarkWorker(0xc000101570)
	runtime/mgc.go:1423 +0xe9 fp=0xc0002be7c8 sp=0xc0002be738 pc=0x5a31c44f07e9
runtime.gcBgMarkStartWorkers.gowrap1()
	runtime/mgc.go:1339 +0x25 fp=0xc0002be7e0 sp=0xc0002be7c8 pc=0x5a31c44f06c5
runtime.goexit({})
	runtime/asm_amd64.s:1700 +0x1 fp=0xc0002be7e8 sp=0xc0002be7e0 pc=0x5a31c454a941
created by runtime.gcBgMarkStartWorkers in goroutine 1
	runtime/mgc.go:1339 +0x105

goroutine 22 gp=0xc000105500 m=nil [GC worker (idle)]:
runtime.gopark(0x1c5a33a6d7814?, 0x0?, 0x0?, 0x0?, 0x0?)
	runtime/proc.go:435 +0xce fp=0xc0000a7f38 sp=0xc0000a7f18 pc=0x5a31c4542aae
runtime.gcBgMarkWorker(0xc000101570)
	runtime/mgc.go:1423 +0xe9 fp=0xc0000a7fc8 sp=0xc0000a7f38 pc=0x5a31c44f07e9
runtime.gcBgMarkStartWorkers.gowrap1()
	runtime/mgc.go:1339 +0x25 fp=0xc0000a7fe0 sp=0xc0000a7fc8 pc=0x5a31c44f06c5
runtime.goexit({})
	runtime/asm_amd64.s:1700 +0x1 fp=0xc0000a7fe8 sp=0xc0000a7fe0 pc=0x5a31c454a941
created by runtime.gcBgMarkStartWorkers in goroutine 1
	runtime/mgc.go:1339 +0x105

goroutine 40 gp=0xc0002bca80 m=nil [GC worker (idle)]:
runtime.gopark(0x1c5a33a81d3ea?, 0x0?, 0x0?, 0x0?, 0x0?)
	runtime/proc.go:435 +0xce fp=0xc0002c5738 sp=0xc0002c5718 pc=0x5a31c4542aae
runtime.gcBgMarkWorker(0xc000101570)
	runtime/mgc.go:1423 +0xe9 fp=0xc0002c57c8 sp=0xc0002c5738 pc=0x5a31c44f07e9
runtime.gcBgMarkStartWorkers.gowrap1()
	runtime/mgc.go:1339 +0x25 fp=0xc0002c57e0 sp=0xc0002c57c8 pc=0x5a31c44f06c5
runtime.goexit({})
	runtime/asm_amd64.s:1700 +0x1 fp=0xc0002c57e8 sp=0xc0002c57e0 pc=0x5a31c454a941
created by runtime.gcBgMarkStartWorkers in goroutine 1
	runtime/mgc.go:1339 +0x105

goroutine 41 gp=0xc0002bcc40 m=nil [GC worker (idle)]:
runtime.gopark(0x1c5a33a6da92e?, 0x0?, 0x0?, 0x0?, 0x0?)
	runtime/proc.go:435 +0xce fp=0xc0002c5f38 sp=0xc0002c5f18 pc=0x5a31c4542aae
runtime.gcBgMarkWorker(0xc000101570)
	runtime/mgc.go:1423 +0xe9 fp=0xc0002c5fc8 sp=0xc0002c5f38 pc=0x5a31c44f07e9
runtime.gcBgMarkStartWorkers.gowrap1()
	runtime/mgc.go:1339 +0x25 fp=0xc0002c5fe0 sp=0xc0002c5fc8 pc=0x5a31c44f06c5
runtime.goexit({})
	runtime/asm_amd64.s:1700 +0x1 fp=0xc0002c5fe8 sp=0xc0002c5fe0 pc=0x5a31c454a941
created by runtime.gcBgMarkStartWorkers in goroutine 1
	runtime/mgc.go:1339 +0x105

goroutine 42 gp=0xc0002bce00 m=nil [GC worker (idle)]:
runtime.gopark(0x5a31c63c1e40?, 0x1?, 0xdf?, 0x7d?, 0x0?)
	runtime/proc.go:435 +0xce fp=0xc0002ca738 sp=0xc0002ca718 pc=0x5a31c4542aae
runtime.gcBgMarkWorker(0xc000101570)
	runtime/mgc.go:1423 +0xe9 fp=0xc0002ca7c8 sp=0xc0002ca738 pc=0x5a31c44f07e9
runtime.gcBgMarkStartWorkers.gowrap1()
	runtime/mgc.go:1339 +0x25 fp=0xc0002ca7e0 sp=0xc0002ca7c8 pc=0x5a31c44f06c5
runtime.goexit({})
	runtime/asm_amd64.s:1700 +0x1 fp=0xc0002ca7e8 sp=0xc0002ca7e0 pc=0x5a31c454a941
created by runtime.gcBgMarkStartWorkers in goroutine 1
	runtime/mgc.go:1339 +0x105

goroutine 43 gp=0xc0002bcfc0 m=nil [GC worker (idle)]:
runtime.gopark(0x1c5a33a6e4d79?, 0x1?, 0xaa?, 0x13?, 0x0?)
	runtime/proc.go:435 +0xce fp=0xc0002caf38 sp=0xc0002caf18 pc=0x5a31c4542aae
runtime.gcBgMarkWorker(0xc000101570)
	runtime/mgc.go:1423 +0xe9 fp=0xc0002cafc8 sp=0xc0002caf38 pc=0x5a31c44f07e9
runtime.gcBgMarkStartWorkers.gowrap1()
	runtime/mgc.go:1339 +0x25 fp=0xc0002cafe0 sp=0xc0002cafc8 pc=0x5a31c44f06c5
runtime.goexit({})
	runtime/asm_amd64.s:1700 +0x1 fp=0xc0002cafe8 sp=0xc0002cafe0 pc=0x5a31c454a941
created by runtime.gcBgMarkStartWorkers in goroutine 1
	runtime/mgc.go:1339 +0x105

goroutine 44 gp=0xc0002bd180 m=nil [GC worker (idle)]:
runtime.gopark(0x1c5a33a6dbb4e?, 0x1?, 0xfa?, 0xd1?, 0x0?)
	runtime/proc.go:435 +0xce fp=0xc0002cb738 sp=0xc0002cb718 pc=0x5a31c4542aae
runtime.gcBgMarkWorker(0xc000101570)
	runtime/mgc.go:1423 +0xe9 fp=0xc0002cb7c8 sp=0xc0002cb738 pc=0x5a31c44f07e9
runtime.gcBgMarkStartWorkers.gowrap1()
	runtime/mgc.go:1339 +0x25 fp=0xc0002cb7e0 sp=0xc0002cb7c8 pc=0x5a31c44f06c5
runtime.goexit({})
	runtime/asm_amd64.s:1700 +0x1 fp=0xc0002cb7e8 sp=0xc0002cb7e0 pc=0x5a31c454a941
created by runtime.gcBgMarkStartWorkers in goroutine 1
	runtime/mgc.go:1339 +0x105

goroutine 45 gp=0xc0002bd340 m=nil [GC worker (idle)]:
runtime.gopark(0x1c5a33a6da212?, 0x1?, 0x81?, 0x44?, 0x0?)
	runtime/proc.go:435 +0xce fp=0xc0002cbf38 sp=0xc0002cbf18 pc=0x5a31c4542aae
runtime.gcBgMarkWorker(0xc000101570)
	runtime/mgc.go:1423 +0xe9 fp=0xc0002cbfc8 sp=0xc0002cbf38 pc=0x5a31c44f07e9
runtime.gcBgMarkStartWorkers.gowrap1()
	runtime/mgc.go:1339 +0x25 fp=0xc0002cbfe0 sp=0xc0002cbfc8 pc=0x5a31c44f06c5
runtime.goexit({})
	runtime/asm_amd64.s:1700 +0x1 fp=0xc0002cbfe8 sp=0xc0002cbfe0 pc=0x5a31c454a941
created by runtime.gcBgMarkStartWorkers in goroutine 1
	runtime/mgc.go:1339 +0x105

goroutine 46 gp=0xc0002bd500 m=nil [GC worker (idle)]:
runtime.gopark(0x1c5a33a822cbe?, 0x3?, 0x8e?, 0xa5?, 0x0?)
	runtime/proc.go:435 +0xce fp=0xc0002cc738 sp=0xc0002cc718 pc=0x5a31c4542aae
runtime.gcBgMarkWorker(0xc000101570)
	runtime/mgc.go:1423 +0xe9 fp=0xc0002cc7c8 sp=0xc0002cc738 pc=0x5a31c44f07e9
runtime.gcBgMarkStartWorkers.gowrap1()
	runtime/mgc.go:1339 +0x25 fp=0xc0002cc7e0 sp=0xc0002cc7c8 pc=0x5a31c44f06c5
runtime.goexit({})
	runtime/asm_amd64.s:1700 +0x1 fp=0xc0002cc7e8 sp=0xc0002cc7e0 pc=0x5a31c454a941
created by runtime.gcBgMarkStartWorkers in goroutine 1
	runtime/mgc.go:1339 +0x105

goroutine 47 gp=0xc0002bd6c0 m=nil [GC worker (idle)]:
runtime.gopark(0x5a31c63c1e40?, 0x1?, 0xc2?, 0xb7?, 0x0?)
	runtime/proc.go:435 +0xce fp=0xc0002ccf38 sp=0xc0002ccf18 pc=0x5a31c4542aae
runtime.gcBgMarkWorker(0xc000101570)
	runtime/mgc.go:1423 +0xe9 fp=0xc0002ccfc8 sp=0xc0002ccf38 pc=0x5a31c44f07e9
runtime.gcBgMarkStartWorkers.gowrap1()
	runtime/mgc.go:1339 +0x25 fp=0xc0002ccfe0 sp=0xc0002ccfc8 pc=0x5a31c44f06c5
runtime.goexit({})
	runtime/asm_amd64.s:1700 +0x1 fp=0xc0002ccfe8 sp=0xc0002ccfe0 pc=0x5a31c454a941
created by runtime.gcBgMarkStartWorkers in goroutine 1
	runtime/mgc.go:1339 +0x105

goroutine 48 gp=0xc0002bd880 m=nil [GC worker (idle)]:
runtime.gopark(0x1c5a33a6e2c14?, 0x1?, 0x61?, 0xad?, 0x0?)
	runtime/proc.go:435 +0xce fp=0xc0002cd738 sp=0xc0002cd718 pc=0x5a31c4542aae
runtime.gcBgMarkWorker(0xc000101570)
	runtime/mgc.go:1423 +0xe9 fp=0xc0002cd7c8 sp=0xc0002cd738 pc=0x5a31c44f07e9
runtime.gcBgMarkStartWorkers.gowrap1()
	runtime/mgc.go:1339 +0x25 fp=0xc0002cd7e0 sp=0xc0002cd7c8 pc=0x5a31c44f06c5
runtime.goexit({})
	runtime/asm_amd64.s:1700 +0x1 fp=0xc0002cd7e8 sp=0xc0002cd7e0 pc=0x5a31c454a941
created by runtime.gcBgMarkStartWorkers in goroutine 1
	runtime/mgc.go:1339 +0x105

goroutine 10 gp=0xc0000ee700 m=nil [GC worker (idle)]:
runtime.gopark(0x5a31c63c1e40?, 0x1?, 0xa8?, 0xc2?, 0x0?)
	runtime/proc.go:435 +0xce fp=0xc0002bef38 sp=0xc0002bef18 pc=0x5a31c4542aae
runtime.gcBgMarkWorker(0xc000101570)
	runtime/mgc.go:1423 +0xe9 fp=0xc0002befc8 sp=0xc0002bef38 pc=0x5a31c44f07e9
runtime.gcBgMarkStartWorkers.gowrap1()
	runtime/mgc.go:1339 +0x25 fp=0xc0002befe0 sp=0xc0002befc8 pc=0x5a31c44f06c5
runtime.goexit({})
	runtime/asm_amd64.s:1700 +0x1 fp=0xc0002befe8 sp=0xc0002befe0 pc=0x5a31c454a941
created by runtime.gcBgMarkStartWorkers in goroutine 1
	runtime/mgc.go:1339 +0x105

goroutine 23 gp=0xc000105a40 m=nil [GC worker (idle)]:
runtime.gopark(0x1c5a33a6bce44?, 0x3?, 0xe4?, 0x2?, 0x0?)
	runtime/proc.go:435 +0xce fp=0xc0000a8738 sp=0xc0000a8718 pc=0x5a31c4542aae
runtime.gcBgMarkWorker(0xc000101570)
	runtime/mgc.go:1423 +0xe9 fp=0xc0000a87c8 sp=0xc0000a8738 pc=0x5a31c44f07e9
runtime.gcBgMarkStartWorkers.gowrap1()
	runtime/mgc.go:1339 +0x25 fp=0xc0000a87e0 sp=0xc0000a87c8 pc=0x5a31c44f06c5
runtime.goexit({})
	runtime/asm_amd64.s:1700 +0x1 fp=0xc0000a87e8 sp=0xc0000a87e0 pc=0x5a31c454a941
created by runtime.gcBgMarkStartWorkers in goroutine 1
	runtime/mgc.go:1339 +0x105

goroutine 24 gp=0xc000105c00 m=nil [GC worker (idle)]:
runtime.gopark(0x5a31c63c1e40?, 0x3?, 0xd7?, 0x19?, 0x0?)
	runtime/proc.go:435 +0xce fp=0xc0000a8f38 sp=0xc0000a8f18 pc=0x5a31c4542aae
runtime.gcBgMarkWorker(0xc000101570)
	runtime/mgc.go:1423 +0xe9 fp=0xc0000a8fc8 sp=0xc0000a8f38 pc=0x5a31c44f07e9
runtime.gcBgMarkStartWorkers.gowrap1()
	runtime/mgc.go:1339 +0x25 fp=0xc0000a8fe0 sp=0xc0000a8fc8 pc=0x5a31c44f06c5
runtime.goexit({})
	runtime/asm_amd64.s:1700 +0x1 fp=0xc0000a8fe8 sp=0xc0000a8fe0 pc=0x5a31c454a941
created by runtime.gcBgMarkStartWorkers in goroutine 1
	runtime/mgc.go:1339 +0x105

goroutine 25 gp=0xc000105dc0 m=nil [GC worker (idle)]:
runtime.gopark(0x5a31c63c1e40?, 0x1?, 0xd0?, 0x9f?, 0x0?)
	runtime/proc.go:435 +0xce fp=0xc0000a9738 sp=0xc0000a9718 pc=0x5a31c4542aae
runtime.gcBgMarkWorker(0xc000101570)
	runtime/mgc.go:1423 +0xe9 fp=0xc0000a97c8 sp=0xc0000a9738 pc=0x5a31c44f07e9
runtime.gcBgMarkStartWorkers.gowrap1()
	runtime/mgc.go:1339 +0x25 fp=0xc0000a97e0 sp=0xc0000a97c8 pc=0x5a31c44f06c5
runtime.goexit({})
	runtime/asm_amd64.s:1700 +0x1 fp=0xc0000a97e8 sp=0xc0000a97e0 pc=0x5a31c454a941
created by runtime.gcBgMarkStartWorkers in goroutine 1
	runtime/mgc.go:1339 +0x105

goroutine 26 gp=0xc00049c000 m=nil [GC worker (idle)]:
runtime.gopark(0x1c5a33a8244a6?, 0x3?, 0xd6?, 0x18?, 0x0?)
	runtime/proc.go:435 +0xce fp=0xc0000a9f38 sp=0xc0000a9f18 pc=0x5a31c4542aae
runtime.gcBgMarkWorker(0xc000101570)
	runtime/mgc.go:1423 +0xe9 fp=0xc0000a9fc8 sp=0xc0000a9f38 pc=0x5a31c44f07e9
runtime.gcBgMarkStartWorkers.gowrap1()
	runtime/mgc.go:1339 +0x25 fp=0xc0000a9fe0 sp=0xc0000a9fc8 pc=0x5a31c44f06c5
runtime.goexit({})
	runtime/asm_amd64.s:1700 +0x1 fp=0xc0000a9fe8 sp=0xc0000a9fe0 pc=0x5a31c454a941
created by runtime.gcBgMarkStartWorkers in goroutine 1
	runtime/mgc.go:1339 +0x105

goroutine 50 gp=0xc000504000 m=nil [GC worker (idle)]:
runtime.gopark(0x1c5a33a823236?, 0x3?, 0x8f?, 0xf?, 0x0?)
	runtime/proc.go:435 +0xce fp=0xc0002c6738 sp=0xc0002c6718 pc=0x5a31c4542aae
runtime.gcBgMarkWorker(0xc000101570)
	runtime/mgc.go:1423 +0xe9 fp=0xc0002c67c8 sp=0xc0002c6738 pc=0x5a31c44f07e9
runtime.gcBgMarkStartWorkers.gowrap1()
	runtime/mgc.go:1339 +0x25 fp=0xc0002c67e0 sp=0xc0002c67c8 pc=0x5a31c44f06c5
runtime.goexit({})
	runtime/asm_amd64.s:1700 +0x1 fp=0xc0002c67e8 sp=0xc0002c67e0 pc=0x5a31c454a941
created by runtime.gcBgMarkStartWorkers in goroutine 1
	runtime/mgc.go:1339 +0x105

goroutine 27 gp=0xc00049c1c0 m=nil [GC worker (idle)]:
runtime.gopark(0x1c5a33a81e218?, 0x1?, 0x9a?, 0x36?, 0x0?)
	runtime/proc.go:435 +0xce fp=0xc0004a2738 sp=0xc0004a2718 pc=0x5a31c4542aae
runtime.gcBgMarkWorker(0xc000101570)
	runtime/mgc.go:1423 +0xe9 fp=0xc0004a27c8 sp=0xc0004a2738 pc=0x5a31c44f07e9
runtime.gcBgMarkStartWorkers.gowrap1()
	runtime/mgc.go:1339 +0x25 fp=0xc0004a27e0 sp=0xc0004a27c8 pc=0x5a31c44f06c5
runtime.goexit({})
	runtime/asm_amd64.s:1700 +0x1 fp=0xc0004a27e8 sp=0xc0004a27e0 pc=0x5a31c454a941
created by runtime.gcBgMarkStartWorkers in goroutine 1
	runtime/mgc.go:1339 +0x105

goroutine 11 gp=0xc0000ee8c0 m=nil [GC worker (idle)]:
runtime.gopark(0x1c5a33a81e16e?, 0x1?, 0x9?, 0x5c?, 0x0?)
	runtime/proc.go:435 +0xce fp=0xc0002bf738 sp=0xc0002bf718 pc=0x5a31c4542aae
runtime.gcBgMarkWorker(0xc000101570)
	runtime/mgc.go:1423 +0xe9 fp=0xc0002bf7c8 sp=0xc0002bf738 pc=0x5a31c44f07e9
runtime.gcBgMarkStartWorkers.gowrap1()
	runtime/mgc.go:1339 +0x25 fp=0xc0002bf7e0 sp=0xc0002bf7c8 pc=0x5a31c44f06c5
runtime.goexit({})
	runtime/asm_amd64.s:1700 +0x1 fp=0xc0002bf7e8 sp=0xc0002bf7e0 pc=0x5a31c454a941
created by runtime.gcBgMarkStartWorkers in goroutine 1
	runtime/mgc.go:1339 +0x105

goroutine 12 gp=0xc0000eea80 m=nil [GC worker (idle)]:
runtime.gopark(0x1c5a33a820982?, 0x1?, 0x92?, 0x38?, 0x0?)
	runtime/proc.go:435 +0xce fp=0xc0002bff38 sp=0xc0002bff18 pc=0x5a31c4542aae
runtime.gcBgMarkWorker(0xc000101570)
	runtime/mgc.go:1423 +0xe9 fp=0xc0002bffc8 sp=0xc0002bff38 pc=0x5a31c44f07e9
runtime.gcBgMarkStartWorkers.gowrap1()
	runtime/mgc.go:1339 +0x25 fp=0xc0002bffe0 sp=0xc0002bffc8 pc=0x5a31c44f06c5
runtime.goexit({})
	runtime/asm_amd64.s:1700 +0x1 fp=0xc0002bffe8 sp=0xc0002bffe0 pc=0x5a31c454a941
created by runtime.gcBgMarkStartWorkers in goroutine 1
	runtime/mgc.go:1339 +0x105

goroutine 14 gp=0xc000703180 m=nil [select]:
runtime.gopark(0xc000049a08?, 0x2?, 0x0?, 0x0?, 0xc00004986c?)
	runtime/proc.go:435 +0xce fp=0xc000049698 sp=0xc000049678 pc=0x5a31c4542aae
runtime.selectgo(0xc000049a08, 0xc000049868, 0x87?, 0x0, 0x1?, 0x1)
	runtime/select.go:351 +0x837 fp=0xc0000497d0 sp=0xc000049698 pc=0x5a31c4521957
github.com/ollama/ollama/runner/ollamarunner.(*Server).completion(0xc0002230e0, {0x5a31c5a56b08, 0xc0011aa540}, 0xc0005208c0)
	github.com/ollama/ollama/runner/ollamarunner/runner.go:950 +0xc4e fp=0xc000049ac0 sp=0xc0000497d0 pc=0x5a31c4a503ae
github.com/ollama/ollama/runner/ollamarunner.(*Server).completion-fm({0x5a31c5a56b08?, 0xc0011aa540?}, 0xc000049b40?)
	<autogenerated>:1 +0x36 fp=0xc000049af0 sp=0xc000049ac0 pc=0x5a31c4a556d6
net/http.HandlerFunc.ServeHTTP(0xc00051a780?, {0x5a31c5a56b08?, 0xc0011aa540?}, 0xc000049b60?)
	net/http/server.go:2294 +0x29 fp=0xc000049b18 sp=0xc000049af0 pc=0x5a31c4841de9
net/http.(*ServeMux).ServeHTTP(0x5a31c44e7805?, {0x5a31c5a56b08, 0xc0011aa540}, 0xc0005208c0)
	net/http/server.go:2822 +0x1c4 fp=0xc000049b68 sp=0xc000049b18 pc=0x5a31c4843ce4
net/http.serverHandler.ServeHTTP({0x5a31c5a53110?}, {0x5a31c5a56b08?, 0xc0011aa540?}, 0x1?)
	net/http/server.go:3301 +0x8e fp=0xc000049b98 sp=0xc000049b68 pc=0x5a31c486176e
net/http.(*conn).serve(0xc0002e43f0, {0x5a31c5a58ec8, 0xc0002e0e10})
	net/http/server.go:2102 +0x625 fp=0xc000049fb8 sp=0xc000049b98 pc=0x5a31c48402e5
net/http.(*Server).Serve.gowrap3()
	net/http/server.go:3454 +0x28 fp=0xc000049fe0 sp=0xc000049fb8 pc=0x5a31c4845ba8
runtime.goexit({})
	runtime/asm_amd64.s:1700 +0x1 fp=0xc000049fe8 sp=0xc000049fe0 pc=0x5a31c454a941
created by net/http.(*Server).Serve in goroutine 1
	net/http/server.go:3454 +0x485

goroutine 1098 gp=0xc000504fc0 m=nil [IO wait]:
runtime.gopark(0x4786b5804786b500?, 0x4786b6804786b600?, 0x0?, 0xb7?, 0xb?)
	runtime/proc.go:435 +0xce fp=0xc0039795d8 sp=0xc0039795b8 pc=0x5a31c4542aae
runtime.netpollblock(0x5a31c4566158?, 0xc44dc1e6?, 0x31?)
	runtime/netpoll.go:575 +0xf7 fp=0xc003979610 sp=0xc0039795d8 pc=0x5a31c4507dd7
internal/poll.runtime_pollWait(0x7231e5a1acc8, 0x72)
	runtime/netpoll.go:351 +0x85 fp=0xc003979630 sp=0xc003979610 pc=0x5a31c4541cc5
internal/poll.(*pollDesc).wait(0xc0006a2180?, 0xc0002e0f11?, 0x0)
	internal/poll/fd_poll_runtime.go:84 +0x27 fp=0xc003979658 sp=0xc003979630 pc=0x5a31c45c9c07
internal/poll.(*pollDesc).waitRead(...)
	internal/poll/fd_poll_runtime.go:89
internal/poll.(*FD).Read(0xc0006a2180, {0xc0002e0f11, 0x1, 0x1})
	internal/poll/fd_unix.go:165 +0x27a fp=0xc0039796f0 sp=0xc003979658 pc=0x5a31c45caefa
net.(*netFD).Read(0xc0006a2180, {0xc0002e0f11?, 0xc0005198d8?, 0xc003979770?})
	net/fd_posix.go:55 +0x25 fp=0xc003979738 sp=0xc0039796f0 pc=0x5a31c463ff05
net.(*conn).Read(0xc0000ae6e0, {0xc0002e0f11?, 0xc000127000?, 0x5a31c48ab200?})
	net/net.go:194 +0x45 fp=0xc003979780 sp=0xc003979738 pc=0x5a31c464e2c5
net/http.(*connReader).backgroundRead(0xc0002e0f00)
	net/http/server.go:690 +0x37 fp=0xc0039797c8 sp=0xc003979780 pc=0x5a31c483a1b7
net/http.(*connReader).startBackgroundRead.gowrap2()
	net/http/server.go:686 +0x25 fp=0xc0039797e0 sp=0xc0039797c8 pc=0x5a31c483a0e5
runtime.goexit({})
	runtime/asm_amd64.s:1700 +0x1 fp=0xc0039797e8 sp=0xc0039797e0 pc=0x5a31c454a941
created by net/http.(*connReader).startBackgroundRead in goroutine 14
	net/http/server.go:686 +0xb6

rax    0x0
rbx    0x72315c0171c0
rcx    0x0
rdx    0x723155352a40
rdi    0x7231553134cd
rsi    0x43
rbp    0x72312a9089c0
rsp    0x72312a907ce0
r8     0x0
r9     0x7230d409b930
r10    0x7230d4cc1248
r11    0x22
r12    0x20
r13    0x72312a908aa0
r14    0x7231797a3e48
r15    0x7230d409b930
rip    0x723155313e3c
rflags 0x10206
cs     0x33
fs     0x0
gs     0x0
time=2025-11-19T02:03:49.568Z level=ERROR source=server.go:1539 msg=""post predict"" error=""Post \""http://127.0.0.1:37865/completion\"": EOF""
[GIN] 2025/11/19 - 02:03:49 | 500 | 11.199601469s |       10.1.3.13 | POST     ""/api/generate""
time=2025-11-19T02:03:49.568Z level=DEBUG source=sched.go:537 msg=""context for request finished""
time=2025-11-19T02:03:49.568Z level=DEBUG source=sched.go:290 msg=""runner with non-zero duration has gone idle, adding timer"" runner.name=registry.ollama.ai/library/qwen2.5vl:32b runner.inference=""[{ID:00000000-c500-0000-0000-000000000000 Library:Vulkan}]"" runner.size=""52.8 GiB"" runner.vram=""52.8 GiB"" runner.parallel=2 runner.pid=216 runner.model=/root/.ollama/models/blobs/sha256-043a363c6ca35e3b1a29b8a5b0bbd28474820239bbc5ad943c9be18f0dc77b66 runner.num_ctx=65536 duration=1h0m0s
time=2025-11-19T02:03:49.568Z level=DEBUG source=sched.go:308 msg=""after processing request finished event"" runner.name=registry.ollama.ai/library/qwen2.5vl:32b runner.inference=""[{ID:00000000-c500-0000-0000-000000000000 Library:Vulkan}]"" runner.size=""52.8 GiB"" runner.vram=""52.8 GiB"" runner.parallel=2 runner.pid=216
```

### OS

Linux

### GPU

AMD

### CPU

AMD

### Ollama version

0.12.11",,"{'url': 'https://api.github.com/repos/ollama/ollama/issues/13146/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}",https://api.github.com/repos/ollama/ollama/issues/13146/timeline,,,,
https://api.github.com/repos/ollama/ollama/issues/13145,https://api.github.com/repos/ollama/ollama,https://api.github.com/repos/ollama/ollama/issues/13145/labels{/name},https://api.github.com/repos/ollama/ollama/issues/13145/comments,https://api.github.com/repos/ollama/ollama/issues/13145/events,https://github.com/ollama/ollama/pull/13145,3640834700,PR_kwDOJ0Z1Ps60QRAA,13145,Parser for Cogito v2,"{'login': 'gr4ceG', 'id': 88872231, 'node_id': 'MDQ6VXNlcjg4ODcyMjMx', 'avatar_url': 'https://avatars.githubusercontent.com/u/88872231?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/gr4ceG', 'html_url': 'https://github.com/gr4ceG', 'followers_url': 'https://api.github.com/users/gr4ceG/followers', 'following_url': 'https://api.github.com/users/gr4ceG/following{/other_user}', 'gists_url': 'https://api.github.com/users/gr4ceG/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/gr4ceG/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/gr4ceG/subscriptions', 'organizations_url': 'https://api.github.com/users/gr4ceG/orgs', 'repos_url': 'https://api.github.com/users/gr4ceG/repos', 'events_url': 'https://api.github.com/users/gr4ceG/events{/privacy}', 'received_events_url': 'https://api.github.com/users/gr4ceG/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}",[],closed,False,,[],,0,2025-11-19T02:59:08Z,2025-11-20T01:21:09Z,2025-11-20T01:21:07Z,CONTRIBUTOR,,,,,,"{'login': 'gr4ceG', 'id': 88872231, 'node_id': 'MDQ6VXNlcjg4ODcyMjMx', 'avatar_url': 'https://avatars.githubusercontent.com/u/88872231?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/gr4ceG', 'html_url': 'https://github.com/gr4ceG', 'followers_url': 'https://api.github.com/users/gr4ceG/followers', 'following_url': 'https://api.github.com/users/gr4ceG/following{/other_user}', 'gists_url': 'https://api.github.com/users/gr4ceG/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/gr4ceG/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/gr4ceG/subscriptions', 'organizations_url': 'https://api.github.com/users/gr4ceG/orgs', 'repos_url': 'https://api.github.com/users/gr4ceG/repos', 'events_url': 'https://api.github.com/users/gr4ceG/events{/privacy}', 'received_events_url': 'https://api.github.com/users/gr4ceG/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}","{'url': 'https://api.github.com/repos/ollama/ollama/issues/13145/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}",https://api.github.com/repos/ollama/ollama/issues/13145/timeline,,,False,"{'url': 'https://api.github.com/repos/ollama/ollama/pulls/13145', 'html_url': 'https://github.com/ollama/ollama/pull/13145', 'diff_url': 'https://github.com/ollama/ollama/pull/13145.diff', 'patch_url': 'https://github.com/ollama/ollama/pull/13145.patch', 'merged_at': '2025-11-20T01:21:07Z'}"
https://api.github.com/repos/ollama/ollama/issues/13144,https://api.github.com/repos/ollama/ollama,https://api.github.com/repos/ollama/ollama/issues/13144/labels{/name},https://api.github.com/repos/ollama/ollama/issues/13144/comments,https://api.github.com/repos/ollama/ollama/issues/13144/events,https://github.com/ollama/ollama/pull/13144,3640802508,PR_kwDOJ0Z1Ps60QKaB,13144,nomic-embed: nomic-embed-text defaulted to ollama runner ,"{'login': 'npardal', 'id': 109545900, 'node_id': 'U_kgDOBoeJrA', 'avatar_url': 'https://avatars.githubusercontent.com/u/109545900?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/npardal', 'html_url': 'https://github.com/npardal', 'followers_url': 'https://api.github.com/users/npardal/followers', 'following_url': 'https://api.github.com/users/npardal/following{/other_user}', 'gists_url': 'https://api.github.com/users/npardal/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/npardal/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/npardal/subscriptions', 'organizations_url': 'https://api.github.com/users/npardal/orgs', 'repos_url': 'https://api.github.com/users/npardal/repos', 'events_url': 'https://api.github.com/users/npardal/events{/privacy}', 'received_events_url': 'https://api.github.com/users/npardal/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}",[],closed,False,,[],,0,2025-11-19T02:46:03Z,2025-11-19T21:03:46Z,2025-11-19T21:03:45Z,CONTRIBUTOR,,,,,This PR makes all models with nomic bert architecture run on the Ollama engine,"{'login': 'npardal', 'id': 109545900, 'node_id': 'U_kgDOBoeJrA', 'avatar_url': 'https://avatars.githubusercontent.com/u/109545900?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/npardal', 'html_url': 'https://github.com/npardal', 'followers_url': 'https://api.github.com/users/npardal/followers', 'following_url': 'https://api.github.com/users/npardal/following{/other_user}', 'gists_url': 'https://api.github.com/users/npardal/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/npardal/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/npardal/subscriptions', 'organizations_url': 'https://api.github.com/users/npardal/orgs', 'repos_url': 'https://api.github.com/users/npardal/repos', 'events_url': 'https://api.github.com/users/npardal/events{/privacy}', 'received_events_url': 'https://api.github.com/users/npardal/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}","{'url': 'https://api.github.com/repos/ollama/ollama/issues/13144/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}",https://api.github.com/repos/ollama/ollama/issues/13144/timeline,,,False,"{'url': 'https://api.github.com/repos/ollama/ollama/pulls/13144', 'html_url': 'https://github.com/ollama/ollama/pull/13144', 'diff_url': 'https://github.com/ollama/ollama/pull/13144.diff', 'patch_url': 'https://github.com/ollama/ollama/pull/13144.patch', 'merged_at': '2025-11-19T21:03:45Z'}"
https://api.github.com/repos/ollama/ollama/issues/13143,https://api.github.com/repos/ollama/ollama,https://api.github.com/repos/ollama/ollama/issues/13143/labels{/name},https://api.github.com/repos/ollama/ollama/issues/13143/comments,https://api.github.com/repos/ollama/ollama/issues/13143/events,https://github.com/ollama/ollama/pull/13143,3640752813,PR_kwDOJ0Z1Ps60P_xB,13143,Deepseekv2 models default to new engine,"{'login': 'gr4ceG', 'id': 88872231, 'node_id': 'MDQ6VXNlcjg4ODcyMjMx', 'avatar_url': 'https://avatars.githubusercontent.com/u/88872231?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/gr4ceG', 'html_url': 'https://github.com/gr4ceG', 'followers_url': 'https://api.github.com/users/gr4ceG/followers', 'following_url': 'https://api.github.com/users/gr4ceG/following{/other_user}', 'gists_url': 'https://api.github.com/users/gr4ceG/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/gr4ceG/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/gr4ceG/subscriptions', 'organizations_url': 'https://api.github.com/users/gr4ceG/orgs', 'repos_url': 'https://api.github.com/users/gr4ceG/repos', 'events_url': 'https://api.github.com/users/gr4ceG/events{/privacy}', 'received_events_url': 'https://api.github.com/users/gr4ceG/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}",[],closed,False,,[],,0,2025-11-19T02:22:24Z,2025-11-19T19:19:39Z,2025-11-19T19:19:38Z,CONTRIBUTOR,,,,,,"{'login': 'gr4ceG', 'id': 88872231, 'node_id': 'MDQ6VXNlcjg4ODcyMjMx', 'avatar_url': 'https://avatars.githubusercontent.com/u/88872231?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/gr4ceG', 'html_url': 'https://github.com/gr4ceG', 'followers_url': 'https://api.github.com/users/gr4ceG/followers', 'following_url': 'https://api.github.com/users/gr4ceG/following{/other_user}', 'gists_url': 'https://api.github.com/users/gr4ceG/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/gr4ceG/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/gr4ceG/subscriptions', 'organizations_url': 'https://api.github.com/users/gr4ceG/orgs', 'repos_url': 'https://api.github.com/users/gr4ceG/repos', 'events_url': 'https://api.github.com/users/gr4ceG/events{/privacy}', 'received_events_url': 'https://api.github.com/users/gr4ceG/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}","{'url': 'https://api.github.com/repos/ollama/ollama/issues/13143/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}",https://api.github.com/repos/ollama/ollama/issues/13143/timeline,,,True,"{'url': 'https://api.github.com/repos/ollama/ollama/pulls/13143', 'html_url': 'https://github.com/ollama/ollama/pull/13143', 'diff_url': 'https://github.com/ollama/ollama/pull/13143.diff', 'patch_url': 'https://github.com/ollama/ollama/pull/13143.patch', 'merged_at': None}"
https://api.github.com/repos/ollama/ollama/issues/13142,https://api.github.com/repos/ollama/ollama,https://api.github.com/repos/ollama/ollama/issues/13142/labels{/name},https://api.github.com/repos/ollama/ollama/issues/13142/comments,https://api.github.com/repos/ollama/ollama/issues/13142/events,https://github.com/ollama/ollama/issues/13142,3640752224,I_kwDOJ0Z1Ps7ZAXhg,13142,feat: add API endpoint to cancel/abort ongoing model downloads,"{'login': 'akhileshthite', 'id': 68826419, 'node_id': 'MDQ6VXNlcjY4ODI2NDE5', 'avatar_url': 'https://avatars.githubusercontent.com/u/68826419?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/akhileshthite', 'html_url': 'https://github.com/akhileshthite', 'followers_url': 'https://api.github.com/users/akhileshthite/followers', 'following_url': 'https://api.github.com/users/akhileshthite/following{/other_user}', 'gists_url': 'https://api.github.com/users/akhileshthite/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/akhileshthite/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/akhileshthite/subscriptions', 'organizations_url': 'https://api.github.com/users/akhileshthite/orgs', 'repos_url': 'https://api.github.com/users/akhileshthite/repos', 'events_url': 'https://api.github.com/users/akhileshthite/events{/privacy}', 'received_events_url': 'https://api.github.com/users/akhileshthite/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}","[{'id': 5667396200, 'node_id': 'LA_kwDOJ0Z1Ps8AAAABUc2aaA', 'url': 'https://api.github.com/repos/ollama/ollama/labels/feature%20request', 'name': 'feature request', 'color': 'a2eeef', 'default': False, 'description': 'New feature or request'}]",open,False,,[],,3,2025-11-19T02:22:04Z,2025-11-19T03:11:10Z,,NONE,,,"{'total': 0, 'completed': 0, 'percent_completed': 0}","{'blocked_by': 0, 'total_blocked_by': 0, 'blocking': 0, 'total_blocking': 0}","### Use Case: 
[PeerSky Browser](https://github.com/p2plabsxyz/peersky-browser) is integrating Ollama as the local LLM provider for P2P web applications. Users can download models through our UI, but currently there's no way to cancel downloads once started. This creates poor UX when users accidentally start downloading large models (70GB+) or change their mind mid-download.

<img width=""800"" height=""625"" alt=""Image"" src=""https://github.com/user-attachments/assets/a2e2ba32-ceb9-4374-9e04-a8e280d9aee3"" />

### Current Behavior: 
The `/api/pull` endpoint starts a model download but provides no mechanism to stop it. Even when clients abort the HTTP connection, the download continues server-side until completion. The only workaround is to kill the entire Ollama process.

### Proposed Solution: 

Add a new API endpoint to cancel ongoing downloads:

```bash
# Cancel current download
DELETE /api/pull

# Or cancel specific model download
DELETE /api/pull/{model_name}
```

#### Alternative Approaches:
1. Add a `cancel` field to the existing `/api/pull` endpoint
2. Include a download ID in pull responses that can be used for cancellation
3. Add a generic `/api/downloads/{id}/cancel` endpoint

This would enable browser-based UIs like PeerSky to provide proper download management with cancel buttons, improving user experience for applications that embed Ollama.

Related: https://github.com/p2plabsxyz/peersky-browser/pull/96",,"{'url': 'https://api.github.com/repos/ollama/ollama/issues/13142/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}",https://api.github.com/repos/ollama/ollama/issues/13142/timeline,,,,
https://api.github.com/repos/ollama/ollama/issues/13141,https://api.github.com/repos/ollama/ollama,https://api.github.com/repos/ollama/ollama/issues/13141/labels{/name},https://api.github.com/repos/ollama/ollama/issues/13141/comments,https://api.github.com/repos/ollama/ollama/issues/13141/events,https://github.com/ollama/ollama/pull/13141,3640497144,PR_kwDOJ0Z1Ps60PI3S,13141,kvcache: Use SetRows to store cache data,"{'login': 'jessegross', 'id': 6468499, 'node_id': 'MDQ6VXNlcjY0Njg0OTk=', 'avatar_url': 'https://avatars.githubusercontent.com/u/6468499?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/jessegross', 'html_url': 'https://github.com/jessegross', 'followers_url': 'https://api.github.com/users/jessegross/followers', 'following_url': 'https://api.github.com/users/jessegross/following{/other_user}', 'gists_url': 'https://api.github.com/users/jessegross/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/jessegross/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/jessegross/subscriptions', 'organizations_url': 'https://api.github.com/users/jessegross/orgs', 'repos_url': 'https://api.github.com/users/jessegross/repos', 'events_url': 'https://api.github.com/users/jessegross/events{/privacy}', 'received_events_url': 'https://api.github.com/users/jessegross/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}",[],closed,False,,[],,0,2025-11-19T00:21:00Z,2025-11-19T04:42:30Z,2025-11-19T04:42:28Z,CONTRIBUTOR,,,,,"We currently copy data into the KV cache in contiguous buffers using ggml_cpy(). ggml_set_rows() was introduced to allow scatter operation so that contiguous buffers are no longer required. The direct primary benefit of this is that we no longer need to perform defragmentation.

However, GGML recently removed an optimization for ggml_cpy() and we picked it up in 544b673 ""ggml update to b6840 (#12791)"". This caused a roughly 40% drop in token generation performance on CUDA due to CUDA graphs no longer being used. By switching to ggml_set_rows(), the original optimization is no longer necessary and CUDA performance is restored.

Fixes #13112","{'login': 'jessegross', 'id': 6468499, 'node_id': 'MDQ6VXNlcjY0Njg0OTk=', 'avatar_url': 'https://avatars.githubusercontent.com/u/6468499?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/jessegross', 'html_url': 'https://github.com/jessegross', 'followers_url': 'https://api.github.com/users/jessegross/followers', 'following_url': 'https://api.github.com/users/jessegross/following{/other_user}', 'gists_url': 'https://api.github.com/users/jessegross/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/jessegross/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/jessegross/subscriptions', 'organizations_url': 'https://api.github.com/users/jessegross/orgs', 'repos_url': 'https://api.github.com/users/jessegross/repos', 'events_url': 'https://api.github.com/users/jessegross/events{/privacy}', 'received_events_url': 'https://api.github.com/users/jessegross/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}","{'url': 'https://api.github.com/repos/ollama/ollama/issues/13141/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}",https://api.github.com/repos/ollama/ollama/issues/13141/timeline,,,False,"{'url': 'https://api.github.com/repos/ollama/ollama/pulls/13141', 'html_url': 'https://github.com/ollama/ollama/pull/13141', 'diff_url': 'https://github.com/ollama/ollama/pull/13141.diff', 'patch_url': 'https://github.com/ollama/ollama/pull/13141.patch', 'merged_at': '2025-11-19T04:42:28Z'}"
https://api.github.com/repos/ollama/ollama/issues/13140,https://api.github.com/repos/ollama/ollama,https://api.github.com/repos/ollama/ollama/issues/13140/labels{/name},https://api.github.com/repos/ollama/ollama/issues/13140/comments,https://api.github.com/repos/ollama/ollama/issues/13140/events,https://github.com/ollama/ollama/pull/13140,3640488931,PR_kwDOJ0Z1Ps60PHD-,13140,refactor rope,"{'login': 'mxyng', 'id': 2372640, 'node_id': 'MDQ6VXNlcjIzNzI2NDA=', 'avatar_url': 'https://avatars.githubusercontent.com/u/2372640?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/mxyng', 'html_url': 'https://github.com/mxyng', 'followers_url': 'https://api.github.com/users/mxyng/followers', 'following_url': 'https://api.github.com/users/mxyng/following{/other_user}', 'gists_url': 'https://api.github.com/users/mxyng/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/mxyng/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/mxyng/subscriptions', 'organizations_url': 'https://api.github.com/users/mxyng/orgs', 'repos_url': 'https://api.github.com/users/mxyng/repos', 'events_url': 'https://api.github.com/users/mxyng/events{/privacy}', 'received_events_url': 'https://api.github.com/users/mxyng/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}",[],open,False,,[],,0,2025-11-19T00:17:26Z,2025-11-20T01:27:43Z,,CONTRIBUTOR,,,,,"change to a flatter directory structure and group the options with the function

update models to call rope in one place",,"{'url': 'https://api.github.com/repos/ollama/ollama/issues/13140/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}",https://api.github.com/repos/ollama/ollama/issues/13140/timeline,,,False,"{'url': 'https://api.github.com/repos/ollama/ollama/pulls/13140', 'html_url': 'https://github.com/ollama/ollama/pull/13140', 'diff_url': 'https://github.com/ollama/ollama/pull/13140.diff', 'patch_url': 'https://github.com/ollama/ollama/pull/13140.patch', 'merged_at': None}"
https://api.github.com/repos/ollama/ollama/issues/13139,https://api.github.com/repos/ollama/ollama,https://api.github.com/repos/ollama/ollama/issues/13139/labels{/name},https://api.github.com/repos/ollama/ollama/issues/13139/comments,https://api.github.com/repos/ollama/ollama/issues/13139/events,https://github.com/ollama/ollama/pull/13139,3640382101,PR_kwDOJ0Z1Ps60Ov87,13139,Renderer for Cogito,"{'login': 'gr4ceG', 'id': 88872231, 'node_id': 'MDQ6VXNlcjg4ODcyMjMx', 'avatar_url': 'https://avatars.githubusercontent.com/u/88872231?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/gr4ceG', 'html_url': 'https://github.com/gr4ceG', 'followers_url': 'https://api.github.com/users/gr4ceG/followers', 'following_url': 'https://api.github.com/users/gr4ceG/following{/other_user}', 'gists_url': 'https://api.github.com/users/gr4ceG/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/gr4ceG/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/gr4ceG/subscriptions', 'organizations_url': 'https://api.github.com/users/gr4ceG/orgs', 'repos_url': 'https://api.github.com/users/gr4ceG/repos', 'events_url': 'https://api.github.com/users/gr4ceG/events{/privacy}', 'received_events_url': 'https://api.github.com/users/gr4ceG/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}",[],closed,False,,[],,1,2025-11-18T23:28:11Z,2025-11-22T10:51:57Z,2025-11-19T03:06:34Z,CONTRIBUTOR,,,,,,"{'login': 'gr4ceG', 'id': 88872231, 'node_id': 'MDQ6VXNlcjg4ODcyMjMx', 'avatar_url': 'https://avatars.githubusercontent.com/u/88872231?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/gr4ceG', 'html_url': 'https://github.com/gr4ceG', 'followers_url': 'https://api.github.com/users/gr4ceG/followers', 'following_url': 'https://api.github.com/users/gr4ceG/following{/other_user}', 'gists_url': 'https://api.github.com/users/gr4ceG/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/gr4ceG/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/gr4ceG/subscriptions', 'organizations_url': 'https://api.github.com/users/gr4ceG/orgs', 'repos_url': 'https://api.github.com/users/gr4ceG/repos', 'events_url': 'https://api.github.com/users/gr4ceG/events{/privacy}', 'received_events_url': 'https://api.github.com/users/gr4ceG/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}","{'url': 'https://api.github.com/repos/ollama/ollama/issues/13139/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}",https://api.github.com/repos/ollama/ollama/issues/13139/timeline,,,False,"{'url': 'https://api.github.com/repos/ollama/ollama/pulls/13139', 'html_url': 'https://github.com/ollama/ollama/pull/13139', 'diff_url': 'https://github.com/ollama/ollama/pull/13139.diff', 'patch_url': 'https://github.com/ollama/ollama/pull/13139.patch', 'merged_at': '2025-11-19T03:06:34Z'}"
https://api.github.com/repos/ollama/ollama/issues/13138,https://api.github.com/repos/ollama/ollama,https://api.github.com/repos/ollama/ollama/issues/13138/labels{/name},https://api.github.com/repos/ollama/ollama/issues/13138/comments,https://api.github.com/repos/ollama/ollama/issues/13138/events,https://github.com/ollama/ollama/pull/13138,3640143128,PR_kwDOJ0Z1Ps60N7fk,13138,win: exit instead of abort,"{'login': 'dhiltgen', 'id': 4033016, 'node_id': 'MDQ6VXNlcjQwMzMwMTY=', 'avatar_url': 'https://avatars.githubusercontent.com/u/4033016?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/dhiltgen', 'html_url': 'https://github.com/dhiltgen', 'followers_url': 'https://api.github.com/users/dhiltgen/followers', 'following_url': 'https://api.github.com/users/dhiltgen/following{/other_user}', 'gists_url': 'https://api.github.com/users/dhiltgen/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/dhiltgen/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/dhiltgen/subscriptions', 'organizations_url': 'https://api.github.com/users/dhiltgen/orgs', 'repos_url': 'https://api.github.com/users/dhiltgen/repos', 'events_url': 'https://api.github.com/users/dhiltgen/events{/privacy}', 'received_events_url': 'https://api.github.com/users/dhiltgen/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}",[],closed,False,,[],,0,2025-11-18T22:00:33Z,2025-11-19T00:33:36Z,2025-11-19T00:33:33Z,COLLABORATOR,,,,,"Calling abort on windows may trigger the C++ runtime to attempt a debugger attach, which causes the crashed runners to hang instead of exit, leading to a timeout instead of a fast failure during discovery.

Fixes #12264 

Ref: https://learn.microsoft.com/en-us/cpp/c-runtime-library/reference/abort?view=msvc-170
","{'login': 'dhiltgen', 'id': 4033016, 'node_id': 'MDQ6VXNlcjQwMzMwMTY=', 'avatar_url': 'https://avatars.githubusercontent.com/u/4033016?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/dhiltgen', 'html_url': 'https://github.com/dhiltgen', 'followers_url': 'https://api.github.com/users/dhiltgen/followers', 'following_url': 'https://api.github.com/users/dhiltgen/following{/other_user}', 'gists_url': 'https://api.github.com/users/dhiltgen/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/dhiltgen/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/dhiltgen/subscriptions', 'organizations_url': 'https://api.github.com/users/dhiltgen/orgs', 'repos_url': 'https://api.github.com/users/dhiltgen/repos', 'events_url': 'https://api.github.com/users/dhiltgen/events{/privacy}', 'received_events_url': 'https://api.github.com/users/dhiltgen/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}","{'url': 'https://api.github.com/repos/ollama/ollama/issues/13138/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}",https://api.github.com/repos/ollama/ollama/issues/13138/timeline,,,False,"{'url': 'https://api.github.com/repos/ollama/ollama/pulls/13138', 'html_url': 'https://github.com/ollama/ollama/pull/13138', 'diff_url': 'https://github.com/ollama/ollama/pull/13138.diff', 'patch_url': 'https://github.com/ollama/ollama/pull/13138.patch', 'merged_at': '2025-11-19T00:33:33Z'}"
https://api.github.com/repos/ollama/ollama/issues/13137,https://api.github.com/repos/ollama/ollama,https://api.github.com/repos/ollama/ollama/issues/13137/labels{/name},https://api.github.com/repos/ollama/ollama/issues/13137/comments,https://api.github.com/repos/ollama/ollama/issues/13137/events,https://github.com/ollama/ollama/pull/13137,3640129249,PR_kwDOJ0Z1Ps60N4gI,13137,app/ui: use requestAnimationFrame to prevent bottom line cutoff in streaming thinking display,"{'login': 'hoyyeva', 'id': 63033505, 'node_id': 'MDQ6VXNlcjYzMDMzNTA1', 'avatar_url': 'https://avatars.githubusercontent.com/u/63033505?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/hoyyeva', 'html_url': 'https://github.com/hoyyeva', 'followers_url': 'https://api.github.com/users/hoyyeva/followers', 'following_url': 'https://api.github.com/users/hoyyeva/following{/other_user}', 'gists_url': 'https://api.github.com/users/hoyyeva/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/hoyyeva/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/hoyyeva/subscriptions', 'organizations_url': 'https://api.github.com/users/hoyyeva/orgs', 'repos_url': 'https://api.github.com/users/hoyyeva/repos', 'events_url': 'https://api.github.com/users/hoyyeva/events{/privacy}', 'received_events_url': 'https://api.github.com/users/hoyyeva/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}",[],open,False,,[],,0,2025-11-18T21:55:32Z,2025-11-18T21:55:32Z,,CONTRIBUTOR,,,,,"When models with thinking capability stream their reasoning in real-time, the bottomline of thinking content is cut off and not visible to users in the collapsed thinking panel.

The reason why is because the thinking component uses CSS `translateY()` transform to position content and show the bottom portion in collapsed mode. During active streaming, a race condition occurs where the transform calculation happens before the browser completes layout of newly added content, resulting in positioning based on stale dimensions.

This PR modified 2 things: 
1. Wrapped transform calculations in `requestAnimationFrame`: Ensures DOM layout is complete before calculating content dimensions and positioning (to avoid race condition)
2. Added auto-scroll for expanded mode: When the thinking panel is expanded during active streaming, automatically scrolls to show the latest content

This PR is resolving #13094 ",,"{'url': 'https://api.github.com/repos/ollama/ollama/issues/13137/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}",https://api.github.com/repos/ollama/ollama/issues/13137/timeline,,,False,"{'url': 'https://api.github.com/repos/ollama/ollama/pulls/13137', 'html_url': 'https://github.com/ollama/ollama/pull/13137', 'diff_url': 'https://github.com/ollama/ollama/pull/13137.diff', 'patch_url': 'https://github.com/ollama/ollama/pull/13137.patch', 'merged_at': None}"
https://api.github.com/repos/ollama/ollama/issues/13136,https://api.github.com/repos/ollama/ollama,https://api.github.com/repos/ollama/ollama/issues/13136/labels{/name},https://api.github.com/repos/ollama/ollama/issues/13136/comments,https://api.github.com/repos/ollama/ollama/issues/13136/events,https://github.com/ollama/ollama/issues/13136,3639986804,I_kwDOJ0Z1Ps7Y9cp0,13136,python not detecting ollama,"{'login': 'CStone6', 'id': 209313289, 'node_id': 'U_kgDODHneCQ', 'avatar_url': 'https://avatars.githubusercontent.com/u/209313289?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/CStone6', 'html_url': 'https://github.com/CStone6', 'followers_url': 'https://api.github.com/users/CStone6/followers', 'following_url': 'https://api.github.com/users/CStone6/following{/other_user}', 'gists_url': 'https://api.github.com/users/CStone6/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/CStone6/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/CStone6/subscriptions', 'organizations_url': 'https://api.github.com/users/CStone6/orgs', 'repos_url': 'https://api.github.com/users/CStone6/repos', 'events_url': 'https://api.github.com/users/CStone6/events{/privacy}', 'received_events_url': 'https://api.github.com/users/CStone6/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}","[{'id': 5667396184, 'node_id': 'LA_kwDOJ0Z1Ps8AAAABUc2aWA', 'url': 'https://api.github.com/repos/ollama/ollama/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': ""Something isn't working""}]",closed,False,,[],,20,2025-11-18T20:28:12Z,2025-11-21T01:04:06Z,2025-11-20T21:06:42Z,NONE,,,"{'total': 0, 'completed': 0, 'percent_completed': 0}","{'blocked_by': 0, 'total_blocked_by': 0, 'blocking': 0, 'total_blocking': 0}","### What is the issue?

im trying to use https://ollama.com/blog/web-search, but I get this error. I have a workaround but it is really annoying to do and my ollama host is set to 0.0.0.0

### Relevant log output

```shell
Traceback (most recent call last):
  File ""b:\neo\tools.py"", line 8, in <module>
    response = chat(
               ^^^^^
  File ""B:\neo\.venv\Lib\site-packages\ollama\_client.py"", line 365, in chat
    return self._request(
           ^^^^^^^^^^^^^^
  File ""B:\neo\.venv\Lib\site-packages\ollama\_client.py"", line 189, in _request        
    return cls(**self._request_raw(*args, **kwargs).json())
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""B:\neo\.venv\Lib\site-packages\ollama\_client.py"", line 135, in _request_raw    
    raise ConnectionError(CONNECTION_ERROR_MESSAGE) from None
ConnectionError: Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download
```

### OS

Windows

### GPU

AMD

### CPU

AMD

### Ollama version

0.12.11","{'login': 'CStone6', 'id': 209313289, 'node_id': 'U_kgDODHneCQ', 'avatar_url': 'https://avatars.githubusercontent.com/u/209313289?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/CStone6', 'html_url': 'https://github.com/CStone6', 'followers_url': 'https://api.github.com/users/CStone6/followers', 'following_url': 'https://api.github.com/users/CStone6/following{/other_user}', 'gists_url': 'https://api.github.com/users/CStone6/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/CStone6/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/CStone6/subscriptions', 'organizations_url': 'https://api.github.com/users/CStone6/orgs', 'repos_url': 'https://api.github.com/users/CStone6/repos', 'events_url': 'https://api.github.com/users/CStone6/events{/privacy}', 'received_events_url': 'https://api.github.com/users/CStone6/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}","{'url': 'https://api.github.com/repos/ollama/ollama/issues/13136/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}",https://api.github.com/repos/ollama/ollama/issues/13136/timeline,,completed,,
https://api.github.com/repos/ollama/ollama/issues/13135,https://api.github.com/repos/ollama/ollama,https://api.github.com/repos/ollama/ollama/issues/13135/labels{/name},https://api.github.com/repos/ollama/ollama/issues/13135/comments,https://api.github.com/repos/ollama/ollama/issues/13135/events,https://github.com/ollama/ollama/pull/13135,3639722731,PR_kwDOJ0Z1Ps60MgFy,13135,Ready for team review,"{'login': 'yuhongsun96', 'id': 32520769, 'node_id': 'MDQ6VXNlcjMyNTIwNzY5', 'avatar_url': 'https://avatars.githubusercontent.com/u/32520769?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/yuhongsun96', 'html_url': 'https://github.com/yuhongsun96', 'followers_url': 'https://api.github.com/users/yuhongsun96/followers', 'following_url': 'https://api.github.com/users/yuhongsun96/following{/other_user}', 'gists_url': 'https://api.github.com/users/yuhongsun96/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/yuhongsun96/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/yuhongsun96/subscriptions', 'organizations_url': 'https://api.github.com/users/yuhongsun96/orgs', 'repos_url': 'https://api.github.com/users/yuhongsun96/repos', 'events_url': 'https://api.github.com/users/yuhongsun96/events{/privacy}', 'received_events_url': 'https://api.github.com/users/yuhongsun96/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}",[],open,False,"{'login': 'jmorganca', 'id': 251292, 'node_id': 'MDQ6VXNlcjI1MTI5Mg==', 'avatar_url': 'https://avatars.githubusercontent.com/u/251292?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/jmorganca', 'html_url': 'https://github.com/jmorganca', 'followers_url': 'https://api.github.com/users/jmorganca/followers', 'following_url': 'https://api.github.com/users/jmorganca/following{/other_user}', 'gists_url': 'https://api.github.com/users/jmorganca/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/jmorganca/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/jmorganca/subscriptions', 'organizations_url': 'https://api.github.com/users/jmorganca/orgs', 'repos_url': 'https://api.github.com/users/jmorganca/repos', 'events_url': 'https://api.github.com/users/jmorganca/events{/privacy}', 'received_events_url': 'https://api.github.com/users/jmorganca/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}","[{'login': 'jmorganca', 'id': 251292, 'node_id': 'MDQ6VXNlcjI1MTI5Mg==', 'avatar_url': 'https://avatars.githubusercontent.com/u/251292?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/jmorganca', 'html_url': 'https://github.com/jmorganca', 'followers_url': 'https://api.github.com/users/jmorganca/followers', 'following_url': 'https://api.github.com/users/jmorganca/following{/other_user}', 'gists_url': 'https://api.github.com/users/jmorganca/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/jmorganca/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/jmorganca/subscriptions', 'organizations_url': 'https://api.github.com/users/jmorganca/orgs', 'repos_url': 'https://api.github.com/users/jmorganca/repos', 'events_url': 'https://api.github.com/users/jmorganca/events{/privacy}', 'received_events_url': 'https://api.github.com/users/jmorganca/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}]",,0,2025-11-18T19:24:50Z,2025-11-20T00:48:58Z,,NONE,,,,,,,"{'url': 'https://api.github.com/repos/ollama/ollama/issues/13135/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}",https://api.github.com/repos/ollama/ollama/issues/13135/timeline,,,False,"{'url': 'https://api.github.com/repos/ollama/ollama/pulls/13135', 'html_url': 'https://github.com/ollama/ollama/pull/13135', 'diff_url': 'https://github.com/ollama/ollama/pull/13135.diff', 'patch_url': 'https://github.com/ollama/ollama/pull/13135.patch', 'merged_at': None}"
https://api.github.com/repos/ollama/ollama/issues/13134,https://api.github.com/repos/ollama/ollama,https://api.github.com/repos/ollama/ollama/issues/13134/labels{/name},https://api.github.com/repos/ollama/ollama/issues/13134/comments,https://api.github.com/repos/ollama/ollama/issues/13134/events,https://github.com/ollama/ollama/pull/13134,3639034814,PR_kwDOJ0Z1Ps60KHvE,13134,test:  fix data race in unit tests,"{'login': 'dhiltgen', 'id': 4033016, 'node_id': 'MDQ6VXNlcjQwMzMwMTY=', 'avatar_url': 'https://avatars.githubusercontent.com/u/4033016?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/dhiltgen', 'html_url': 'https://github.com/dhiltgen', 'followers_url': 'https://api.github.com/users/dhiltgen/followers', 'following_url': 'https://api.github.com/users/dhiltgen/following{/other_user}', 'gists_url': 'https://api.github.com/users/dhiltgen/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/dhiltgen/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/dhiltgen/subscriptions', 'organizations_url': 'https://api.github.com/users/dhiltgen/orgs', 'repos_url': 'https://api.github.com/users/dhiltgen/repos', 'events_url': 'https://api.github.com/users/dhiltgen/events{/privacy}', 'received_events_url': 'https://api.github.com/users/dhiltgen/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}",[],open,False,,[],,0,2025-11-18T16:33:23Z,2025-11-18T16:33:23Z,,COLLABORATOR,,,,,"'go test -race ./server' now passes

Fixes #12963",,"{'url': 'https://api.github.com/repos/ollama/ollama/issues/13134/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}",https://api.github.com/repos/ollama/ollama/issues/13134/timeline,,,False,"{'url': 'https://api.github.com/repos/ollama/ollama/pulls/13134', 'html_url': 'https://github.com/ollama/ollama/pull/13134', 'diff_url': 'https://github.com/ollama/ollama/pull/13134.diff', 'patch_url': 'https://github.com/ollama/ollama/pull/13134.patch', 'merged_at': None}"
https://api.github.com/repos/ollama/ollama/issues/13133,https://api.github.com/repos/ollama/ollama,https://api.github.com/repos/ollama/ollama/issues/13133/labels{/name},https://api.github.com/repos/ollama/ollama/issues/13133/comments,https://api.github.com/repos/ollama/ollama/issues/13133/events,https://github.com/ollama/ollama/pull/13133,3638655289,PR_kwDOJ0Z1Ps60I0Zy,13133,KUHUL OS AI,"{'login': 'cannaseedus-bot', 'id': 238004612, 'node_id': 'U_kgDODi-phA', 'avatar_url': 'https://avatars.githubusercontent.com/u/238004612?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/cannaseedus-bot', 'html_url': 'https://github.com/cannaseedus-bot', 'followers_url': 'https://api.github.com/users/cannaseedus-bot/followers', 'following_url': 'https://api.github.com/users/cannaseedus-bot/following{/other_user}', 'gists_url': 'https://api.github.com/users/cannaseedus-bot/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/cannaseedus-bot/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/cannaseedus-bot/subscriptions', 'organizations_url': 'https://api.github.com/users/cannaseedus-bot/orgs', 'repos_url': 'https://api.github.com/users/cannaseedus-bot/repos', 'events_url': 'https://api.github.com/users/cannaseedus-bot/events{/privacy}', 'received_events_url': 'https://api.github.com/users/cannaseedus-bot/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}",[],closed,False,,[],,0,2025-11-18T15:05:48Z,2025-11-19T20:16:21Z,2025-11-19T20:16:21Z,NONE,,,,,"# 🛸 K'uhul Multi Hive OS - Ollama-Powered Multi-Agent AI System

## 🌟 Overview

This PR introduces **K'uhul Multi Hive OS**, a sophisticated multi-agent orchestration system powered by  ASX Language Framework integration.

## 🎯 What's New

### Core Features
- ✅ **Multi-Agent Hive Architecture**: 5 specialized agents (Queen, Coder, Analyst, Creative, Memory)
- ✅ **Ollama Integration**: Local, private LLM execution
- ✅ **FastAPI Backend**: RESTful API server on port 8000
- ✅ **Beautiful Web Interface**: Cyberpunk-themed UI with real-time monitoring
- ✅ **Knowledge Base**: File ingestion with automatic summarization
- ✅ **ASX Framework Integration**: XJSON, KLH, SCXQ2, Tape Runtime support

### Technical Implementation
- **Multi-Agent Orchestration**: Queen-led coordination with parallel specialist queries
- **Quantum Torrent**: Distributed data sharding with SHA3-512 verification
- **XJSON Engine**: Execute workflows as executable JSON
- **KLH Orchestrator**: Multi-hive coordination patterns
- **Cross-Platform**: Linux, macOS, and Windows startup scripts

","{'login': 'pdevine', 'id': 75239, 'node_id': 'MDQ6VXNlcjc1MjM5', 'avatar_url': 'https://avatars.githubusercontent.com/u/75239?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/pdevine', 'html_url': 'https://github.com/pdevine', 'followers_url': 'https://api.github.com/users/pdevine/followers', 'following_url': 'https://api.github.com/users/pdevine/following{/other_user}', 'gists_url': 'https://api.github.com/users/pdevine/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/pdevine/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/pdevine/subscriptions', 'organizations_url': 'https://api.github.com/users/pdevine/orgs', 'repos_url': 'https://api.github.com/users/pdevine/repos', 'events_url': 'https://api.github.com/users/pdevine/events{/privacy}', 'received_events_url': 'https://api.github.com/users/pdevine/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}","{'url': 'https://api.github.com/repos/ollama/ollama/issues/13133/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}",https://api.github.com/repos/ollama/ollama/issues/13133/timeline,,,False,"{'url': 'https://api.github.com/repos/ollama/ollama/pulls/13133', 'html_url': 'https://github.com/ollama/ollama/pull/13133', 'diff_url': 'https://github.com/ollama/ollama/pull/13133.diff', 'patch_url': 'https://github.com/ollama/ollama/pull/13133.patch', 'merged_at': None}"
https://api.github.com/repos/ollama/ollama/issues/13132,https://api.github.com/repos/ollama/ollama,https://api.github.com/repos/ollama/ollama/issues/13132/labels{/name},https://api.github.com/repos/ollama/ollama/issues/13132/comments,https://api.github.com/repos/ollama/ollama/issues/13132/events,https://github.com/ollama/ollama/issues/13132,3638463395,I_kwDOJ0Z1Ps7Y3ouj,13132,pull model manifest: 500,"{'login': 'PixelSymbols', 'id': 81364140, 'node_id': 'MDQ6VXNlcjgxMzY0MTQw', 'avatar_url': 'https://avatars.githubusercontent.com/u/81364140?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/PixelSymbols', 'html_url': 'https://github.com/PixelSymbols', 'followers_url': 'https://api.github.com/users/PixelSymbols/followers', 'following_url': 'https://api.github.com/users/PixelSymbols/following{/other_user}', 'gists_url': 'https://api.github.com/users/PixelSymbols/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/PixelSymbols/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/PixelSymbols/subscriptions', 'organizations_url': 'https://api.github.com/users/PixelSymbols/orgs', 'repos_url': 'https://api.github.com/users/PixelSymbols/repos', 'events_url': 'https://api.github.com/users/PixelSymbols/events{/privacy}', 'received_events_url': 'https://api.github.com/users/PixelSymbols/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}",[],closed,False,,[],,0,2025-11-18T14:24:08Z,2025-11-18T14:27:34Z,2025-11-18T14:27:34Z,NONE,,,"{'total': 0, 'completed': 0, 'percent_completed': 0}","{'blocked_by': 0, 'total_blocked_by': 0, 'blocking': 0, 'total_blocking': 0}","issue needs to be reopened:
https://github.com/ollama/ollama/issues/8873#issuecomment-3547829395

<img width=""2482"" height=""1651"" alt=""Image"" src=""https://github.com/user-attachments/assets/4bedec95-3725-4b49-8f99-f1737040e575"" />","{'login': 'rick-github', 'id': 14946854, 'node_id': 'MDQ6VXNlcjE0OTQ2ODU0', 'avatar_url': 'https://avatars.githubusercontent.com/u/14946854?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/rick-github', 'html_url': 'https://github.com/rick-github', 'followers_url': 'https://api.github.com/users/rick-github/followers', 'following_url': 'https://api.github.com/users/rick-github/following{/other_user}', 'gists_url': 'https://api.github.com/users/rick-github/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/rick-github/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/rick-github/subscriptions', 'organizations_url': 'https://api.github.com/users/rick-github/orgs', 'repos_url': 'https://api.github.com/users/rick-github/repos', 'events_url': 'https://api.github.com/users/rick-github/events{/privacy}', 'received_events_url': 'https://api.github.com/users/rick-github/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}","{'url': 'https://api.github.com/repos/ollama/ollama/issues/13132/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}",https://api.github.com/repos/ollama/ollama/issues/13132/timeline,,duplicate,,
https://api.github.com/repos/ollama/ollama/issues/13131,https://api.github.com/repos/ollama/ollama,https://api.github.com/repos/ollama/ollama/issues/13131/labels{/name},https://api.github.com/repos/ollama/ollama/issues/13131/comments,https://api.github.com/repos/ollama/ollama/issues/13131/events,https://github.com/ollama/ollama/pull/13131,3638453740,PR_kwDOJ0Z1Ps60IHxV,13131,Add H'uhul Multi Hive OS - Ollama-powered multi-agent system,"{'login': 'cannaseedus-bot', 'id': 238004612, 'node_id': 'U_kgDODi-phA', 'avatar_url': 'https://avatars.githubusercontent.com/u/238004612?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/cannaseedus-bot', 'html_url': 'https://github.com/cannaseedus-bot', 'followers_url': 'https://api.github.com/users/cannaseedus-bot/followers', 'following_url': 'https://api.github.com/users/cannaseedus-bot/following{/other_user}', 'gists_url': 'https://api.github.com/users/cannaseedus-bot/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/cannaseedus-bot/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/cannaseedus-bot/subscriptions', 'organizations_url': 'https://api.github.com/users/cannaseedus-bot/orgs', 'repos_url': 'https://api.github.com/users/cannaseedus-bot/repos', 'events_url': 'https://api.github.com/users/cannaseedus-bot/events{/privacy}', 'received_events_url': 'https://api.github.com/users/cannaseedus-bot/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}",[],closed,False,,[],,0,2025-11-18T14:21:19Z,2025-11-19T20:13:30Z,2025-11-19T20:13:30Z,NONE,,,,,"Implement a comprehensive multi-agent AI operating system that leverages Ollama's local LLM capabilities with ASX Framework integration.

Features:
- Multi-agent hive architecture (Queen + specialized workers)
- Ollama integration for local, private LLM execution
- FastAPI backend with RESTful API (port 8000)
- Beautiful cyberpunk-themed web interface
- File ingestion and knowledge base management
- XJSON runtime engine for executable JSON workflows
- KLH orchestration for multi-hive coordination
- Quantum Torrent distributed data sharding with SHA3-512 verification
- ASX Tape Runtime for modular execution containers
- Cross-platform startup scripts (Linux/macOS/Windows)

Components:
- backend/huhul_server.py: Main FastAPI server with 5 specialized agents
- backend/quantum_torrent.py: Distributed sharding and integrity verification
- frontend/index.html: Interactive web interface with real-time monitoring
- integration/asx_bridge.py: ASX Framework integration (XJSON, KLH, SCXQ2)
- config/: Configuration files and example XJSON tape
- Comprehensive documentation (README, QUICKSTART)

Agents:
- Queen (qwen2.5): Orchestrator and decision maker
- Coder (qwen2.5-coder): Code generation specialist
- Analyst (llama3.2): Data analysis and reasoning
- Creative (mistral): Creative thinking and ideation
- Memory (llama3.2): Knowledge retrieval and storage

Integration:
- ASX Language Framework (XJSON, KLH, SCXQ2, Tape Runtime)
- Quantum-resistant hashing for data integrity
- Multi-hive distributed architecture support","{'login': 'pdevine', 'id': 75239, 'node_id': 'MDQ6VXNlcjc1MjM5', 'avatar_url': 'https://avatars.githubusercontent.com/u/75239?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/pdevine', 'html_url': 'https://github.com/pdevine', 'followers_url': 'https://api.github.com/users/pdevine/followers', 'following_url': 'https://api.github.com/users/pdevine/following{/other_user}', 'gists_url': 'https://api.github.com/users/pdevine/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/pdevine/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/pdevine/subscriptions', 'organizations_url': 'https://api.github.com/users/pdevine/orgs', 'repos_url': 'https://api.github.com/users/pdevine/repos', 'events_url': 'https://api.github.com/users/pdevine/events{/privacy}', 'received_events_url': 'https://api.github.com/users/pdevine/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}","{'url': 'https://api.github.com/repos/ollama/ollama/issues/13131/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}",https://api.github.com/repos/ollama/ollama/issues/13131/timeline,,,False,"{'url': 'https://api.github.com/repos/ollama/ollama/pulls/13131', 'html_url': 'https://github.com/ollama/ollama/pull/13131', 'diff_url': 'https://github.com/ollama/ollama/pull/13131.diff', 'patch_url': 'https://github.com/ollama/ollama/pull/13131.patch', 'merged_at': None}"
